{
  "hash": "73e0d3ed77fa09b6e658ef6faecc9216",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Grouped operations\n---\n\n::: {.callout-tip}\n#### Learning objectives\n\n- Learn how to use grouped operations in data analysis.\n- Be able to distinguish between grouped summaries and other types of grouped operations.\n:::\n\n\n## Context\n\nWe’ve done different types of operations, all on the entire data set. Sometimes there is structure within the data, such as different groups (e.g. genotypes, patient cohorts, geographical areas etc). We might then want information on a group-by-group basis.\n\n## Section setup {#setup_grouped_operations}\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe'll continue this section with the script named `da3-08-grouped-operations.R`. If needed, add the following code to the top of your script and run it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys <- read_csv(\"data/surveys.csv\")\n```\n:::\n\n\n## Python\n\nWe'll continue this section with the script named `da3-08-grouped-operations.py`. If needed, add the following code to the top of your script and run it.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\n```\n:::\n\n\n:::\n:::\n\n## Split-apply-combine\n\nConceptually, this kind of operation can be referred to as split-apply-combine, because we split the data, apply some function and then combine the outcome.\n\nLet's illustrate this with an example. @fig-groupby_table shows a hypothetical data set, where we have temperature and rainfall measurements for different cities.\n\n![An example of a table with groups](images/groupby_table.png){#fig-groupby_table}\n\nLet's assume we were interested in the average temperature for each city. We would have to do the following:\n\n1. Split the data by `city`\n2. Calculate the average `temperature`\n3. Combine the outcome together in a new table\n\nThis is visualised in @fig-groupby_split.\n\n![Split-apply-combine](images/groupby_split.png){#fig-groupby_split}\n\n## Summary operations\n\nLet's put this into practice with our data set.\n\n### Summarising data\n\nA common task in data analysis is to summarise variables to get the mean and the variation around it.\n\nFor example, let’s calculate what the mean and standard deviation are for `weight`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe can achieve this task using the `summarise()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  summarise(weight_mean = mean(weight, na.rm  = TRUE),\n            weight_sd = sd(weight, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  weight_mean weight_sd\n        <dbl>     <dbl>\n1        42.7      36.6\n```\n\n\n:::\n:::\n\n\nA couple of things to notice:\n\nThe output of `summarise` is a new table, where each column is named according to the input to `summarise()`.\n\nWithin `summarise()` we should use functions for which the output is a single value.\nAlso notice that, above, we used the `na.rm` option within the summary functions, so that they ignored missing values when calculating the respective statistics.\n\n## Python\n\nFor these kind of summary statistics we can use `.agg()` - the aggregate function in pandas. You can apply this to a DataFrame or Series. It works on standard summary functions, listed below. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nsurveys[\"weight\"].agg(\n    weight_mean = \"mean\",\n    weight_sd = \"std\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nweight_mean    42.672428\nweight_sd      36.631259\nName: weight, dtype: float64\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.callout-tip}\n## Summary functions\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThere are many functions whose input is a vector (or a column in a table) and the output is a single number. Here are several common ones:\n\n| R             | Example                   | Description                                 |\n|---------------|---------------------------|---------------------------------------------|\n| `mean`        | `mean(x, na.rm = TRUE)`   | Arithmetic mean                             |\n| `median`      | `median(x, na.rm = TRUE)` | Median                                      |\n| `sd`          | `sd(x, na.rm = TRUE)`     | Standard deviation                          |\n| `var`         | `var(x, na.rm = TRUE)`    | Variance                                    |\n| `mad`         | `mad(x, na.rm = TRUE)`    | Median absolute deviation                   |\n| `min`         | `min(x, na.rm = TRUE)`    | Minimum value                               |\n| `max`         | `max(x, na.rm = TRUE)`    | Maximum value                               |\n| `sum`         | `sum(x, na.rm = TRUE)`    | Sum of all values                           |\n| `n_distinct`  | `n_distinct(x)`           | Number of distinct (unique) values          |\n\n\nAll of these have the option `na.rm`, which tells the function remove missing values before doing the calculation.\n\n## Python\n\n| Python (`pandas`) | Example (in `.agg()`)          | Description                        |\n|-------------------|--------------------------------|------------------------------------|\n| `\"mean\"`          | `df[\"x\"].agg(\"mean\")`          | Arithmetic mean                    |\n| `\"median\"`        | `df[\"x\"].agg(\"median\")`        | Median                             |\n| `\"std\"`           | `df[\"x\"].agg(\"std\")`           | Standard deviation                 |\n| `\"var\"`           | `df[\"x\"].agg(\"var\")`           | Variance                           |\n| `\"mad\"`           | `df[\"x\"].agg(\"mad\")`           | Mean absolute deviation            |\n| `\"min\"`           | `df[\"x\"].agg(\"min\")`           | Minimum value                      |\n| `\"max\"`           | `df[\"x\"].agg(\"max\")`           | Maximum value                      |\n| `\"sum\"`           | `df[\"x\"].agg(\"sum\")`           | Sum of all values                  |\n| `\"nunique\"`       | `df[\"x\"].agg(\"nunique\")`       | Number of distinct (unique) values |\n\n:::\n:::\n\n### Grouped summaries\n\nIn most cases we want to calculate summary statistics across groups of our data. \n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe can achieve this by combining `summarise()` with the `group_by()` function. For example, let’s modify the previous example to calculate the summary for each `sex` group:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(sex) |> \n  summarise(weight_mean = mean(weight, na.rm  = TRUE),\n            weight_sd = sd(weight, na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  sex   weight_mean weight_sd\n  <chr>       <dbl>     <dbl>\n1 F            42.2      36.8\n2 M            43.0      36.2\n3 <NA>         64.7      62.2\n```\n\n\n:::\n:::\n\n\nThe table output now includes both the columns we defined within `summarise()` as well as the grouping columns defined within `group_by()`.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsurveys.groupby(\"sex\")[\"weight\"].agg(\n    weight_mean = \"mean\",\n    weight_sd = \"std\"\n).reset_index()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sex  weight_mean  weight_sd\n0   F    42.170555  36.847958\n1   M    42.995379  36.184981\n```\n\n\n:::\n:::\n\n\n:::\n\n## Counting data\n\nCounting or tallying data is an extremely useful way of getting to know your data better.\n\n### Simple counting\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe can use the `count()` function from `dplyr` to count data. It always returns the number of rows it counts.\n\nFor example, this gives us the total number of observations (rows) in our data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount(surveys)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n      n\n  <int>\n1 35549\n```\n\n\n:::\n:::\n\n\n\n## Python\n\nWe can use the `.shape` attribute. In pandas, each DataFrame has a `.shape` attribute that returns a tuple in the format `(rows, columns)`.\n\nSo, `.shape[0]` will return the number of rows, whereas `.shape[1]` returns the number of columns.\n\nFor our `surveys` DataFrame we then get the number of rows by:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsurveys.shape[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n35549\n```\n\n\n:::\n:::\n\n\n:::\n\nWe can also use that in combination with a conditional statement. For example, if we're interested in all the observations from the year `1982`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# count the observations from the year 1982\nsurveys |> \n  filter(year == 1982) |> \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n      n\n  <int>\n1  1978\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsurveys[surveys[\"year\"] == 1982].shape[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1978\n```\n\n\n:::\n:::\n\n\nOr, slightly easier to read, with the `.query()` function:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsurveys.query(\"year == 1982\").shape[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1978\n```\n\n\n:::\n:::\n\n\n:::\n\n### Counting by group\n\nCounting really comes into its own when we're combining this with some grouping. For example, we might be interested in the number of observations for each year.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  count(year)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 2\n    year     n\n   <dbl> <int>\n 1  1977   503\n 2  1978  1048\n 3  1979   719\n 4  1980  1415\n 5  1981  1472\n 6  1982  1978\n 7  1983  1673\n 8  1984   981\n 9  1985  1438\n10  1986   942\n# ℹ 16 more rows\n```\n\n\n:::\n:::\n\n\nWe can also easily visualise this (we can pipe straight into `ggplot()`). We use `geom_col()` to create a bar chart of the number of observations per year. We count by `sex` and use this variable to fill the colour of the bars.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  count(sex, year) |> \n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_col()\n```\n\n::: {.cell-output-display}\n![Number of observations per year, by `sex`.](da3-08-grouped-operations_files/figure-html/fig-counts_year_sex-1.png){#fig-counts_year_sex width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Count number of rows for each year\ncounts = surveys.groupby(\"year\").size().reset_index(name = \"n\")\n\n# Look at the first few rows\ncounts.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   year     n\n0  1977   503\n1  1978  1048\n2  1979   719\n3  1980  1415\n4  1981  1472\n```\n\n\n:::\n:::\n\n\nLet's expand this example a bit, where we count by two variables: `sex` and `year`. We then also plot the results, just to illustrate how useful that can be.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Count observations by sex and year\ncounts = surveys.groupby([\"sex\", \"year\"]).size().reset_index(name = \"n\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (\n    ggplot(counts, aes(x = \"year\", y = \"n\", fill = \"sex\")) +\n    geom_col()\n)\n\np.show()\n```\n\n::: {.cell-output-display}\n![Number of observations per year, by `sex`.](da3-08-grouped-operations_files/figure-html/fig-counts_year_sex_py-1.png){#fig-counts_year_sex_py width=614}\n:::\n:::\n\n\n:::\n\n::: {.callout-important}\n## Counting within a summary pipeline\n\nOften we want to do counting when we're creating summaries. Let's illustrate this with an example where we take the observations from `1981` and `1982`, then calculate the mean `weight` and count the number of observations.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe `count()` function can't be used within `summarise()`, but there is a special helper function called `n()`. Look at the following example, where we group by year, filter the data, create some summary statistic and also count the number of rows within each group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |>                                   # group the data\n  filter(year %in% c(1981, 1982)) |>                  # filter a subset of years\n  summarise(mean_weight = mean(weight, na.rm = TRUE), # calculate mean weight\n            n_obs = n()) |>                           # number of rows\n  ungroup()                                           # drop the grouping\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n   year mean_weight n_obs\n  <dbl>       <dbl> <int>\n1  1981        65.8  1472\n2  1982        53.8  1978\n```\n\n\n:::\n:::\n\n\n## Python\n\nWe need to do this in two steps:\n\n1. Filter out the relevant data\n2. Calculate the summary statistics\n\nHere we're using `(  )` around the pipeline, so we can break up the code into different lines. This aids with readability, but doesn't change how the code works!\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Filter years 1981 and 1982\nfiltered = surveys[surveys[\"year\"].isin([1981, 1982])]\n\n# Group by year and summarise\nsummary = (\n    filtered                               # input DataFrame\n    .groupby(\"year\")                       # group by year\n    .agg(                                  # create summary statistics\n        mean_weight = (\"weight\", \"mean\"),  # calculate mean weights\n        n_obs = (\"weight\", \"count\")        # count of non-NaN weights\n    )\n    .reset_index()                         # converts index into regular column\n)\n\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   year  mean_weight  n_obs\n0  1981    65.843888   1358\n1  1982    53.765888   1841\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n### Counting missing data\n\nOh, missing data! How we've missed you. For something that isn't there, is has quite the presence. But, it is an important consideration in data analysis. We've [already seen](#missingdata-revisited) how we can remove missing data from and also explored ways to visualise them.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe have seen how to use the `summary()` function to find missing values. Here we'll see (even more) ways to tally them.\n\nWe can use the `is.na()` function to great effect, within a `summarise()` pipeline. We can negate with `!is.na()` to find non-missing values. Again, using the `sum()` function then enables us to tally how many missing / non-missing values there are.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  summarise(obs_present = sum(!is.na(species_id)),    # count non-missing data\n            obs_absent = sum(is.na(species_id)),      # count missing data\n            n_obs = n(),                              # total number of rows\n            precentage_absent = \n              (obs_absent / n_obs) * 100) |>          # percentage of missing data\n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  obs_present obs_absent n_obs precentage_absent\n        <int>      <int> <int>             <dbl>\n1       34786        763 35549              2.15\n```\n\n\n:::\n:::\n\n\n## Python\n\nWe can use the `.isna()` and `.notna()` functions to great effect. Again, we're using the `.sum()` function to tally the numbers.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsummary = pd.DataFrame([{\n    \"obs_present\": surveys[\"species_id\"].notna().sum(),\n    \"obs_absent\": surveys[\"species_id\"].isna().sum(),\n    \"n_obs\": len(surveys),\n    \"percentage_absent\": surveys[\"species_id\"].isna().mean() * 100\n}])\n\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   obs_present  obs_absent  n_obs  percentage_absent\n0        34786         763  35549           2.146333\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Grouped operations\n\n### Grouped filters\nSometimes it can be really handy to filter data, by group. In our `surveys` data, for example, you might be interested to find out what the minimum `weight` value is for each `year`. We can do that as follows: \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |> \n  filter(weight == min(weight, na.rm = TRUE)) |> \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 94 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n 1       218     9    13  1977       1 PF         M                  13      4\n 2      1161     8     5  1978       6 PF         F                  16      6\n 3      1230     9     3  1978      19 PF         M                  16      6\n 4      1265     9     4  1978      15 PF         F                  16      6\n 5      1282     9     4  1978       4 PF         F                  16      6\n 6      1343    10     7  1978      11 PF         F                  15      6\n 7      1351    10     8  1978       2 PF         M                  15      6\n 8      1380    10     8  1978       3 PF         F                  16      6\n 9      1400    11     4  1978      19 PF         M                  15      6\n10      1427    11     4  1978      21 PF         F                  15      6\n# ℹ 84 more rows\n```\n\n\n:::\n:::\n\n\nYou can see that this outputs the minimum value, but if there are multiple entries for each year (such as in `1978`), multiple rows returned. If we only wanted to get a single row per minimum value, per year, then we can use `slice(1)`. This slices the first row of each group:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |> \n  filter(weight == min(weight, na.rm = TRUE)) |> \n  slice(1) |> \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       <dbl> <dbl> <dbl> <dbl>   <dbl> <chr>      <chr>           <dbl>  <dbl>\n 1       218     9    13  1977       1 PF         M                  13      4\n 2      1161     8     5  1978       6 PF         F                  16      6\n 3      1923     7    25  1979      18 PF         F                  NA      6\n 4      2506     2    25  1980       3 PF         F                  15      5\n 5      4052     4     5  1981       3 PF         F                  15      4\n 6      5346     2    22  1982      21 PF         F                  14      4\n 7      8736    12     8  1983      19 RM         M                  17      4\n 8      8809     2     4  1984      16 RM         F                  14      7\n 9      9790     1    19  1985      16 RM         F                  16      4\n10     11299     3     9  1986       3 RM         F                  16      7\n# ℹ 16 more rows\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Drop rows where weight is missing\nsurveys_filtered = surveys.dropna(subset = [\"weight\"])  # get non-NA weights\n\n# For each year, get rows with the minimum weight\nmin_weights_by_year = (\n    surveys_filtered.loc[\n        surveys_filtered.groupby(\"year\")[\"weight\"]      # group by year\n        .transform(\"min\") == surveys_filtered[\"weight\"] # get min weight\n    ]\n)\n\n# Look at the first few rows\nmin_weights_by_year.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n217         218      9   13  1977  ...          PF   M            13.0     4.0\n1160       1161      8    5  1978  ...          PF   F            16.0     6.0\n1229       1230      9    3  1978  ...          PF   M            16.0     6.0\n1264       1265      9    4  1978  ...          PF   F            16.0     6.0\n1281       1282      9    4  1978  ...          PF   F            16.0     6.0\n\n[5 rows x 9 columns]\n```\n\n\n:::\n:::\n\n\nYou can see that this outputs the minimum value, but if there are multiple entries for each year (such as in `1978`), multiple rows returned. If we only wanted to get a single row per minimum value, per year, then we can do the following:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Retain only the first occurrence of the minimum value\nfirst_min_per_year = surveys_filtered.loc[\n    surveys_filtered.groupby(\"year\")[\"weight\"].idxmin()\n].reset_index(drop = True)\n\n# Look at the first few rows\nfirst_min_per_year.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   record_id  month  day  year  plot_id species_id sex  hindfoot_length  weight\n0        218      9   13  1977        1         PF   M             13.0     4.0\n1       1161      8    5  1978        6         PF   F             16.0     6.0\n2       1923      7   25  1979       18         PF   F              NaN     6.0\n3       2506      2   25  1980        3         PF   F             15.0     5.0\n4       4052      4    5  1981        3         PF   F             15.0     4.0\n```\n\n\n:::\n:::\n\n\n:::\n\n### Grouped changes\nSometimes you might need to add a new variable to our table, based on different groups. Let's say we want to see how many female and male observations there are in our `surveys` data set for each `year`.\n\nWe're also interested in the percentage of female observations out of the total number of observations where `sex` was recorded.\n\nWe have the following number of female / male observations:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |> \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |> \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 3\n    year n_obs_f n_obs_m\n   <dbl>   <int>   <int>\n 1  1977     204     214\n 2  1978     503     433\n 3  1979     327     324\n 4  1980     605     727\n 5  1981     631     745\n 6  1982     823    1027\n 7  1983     771     797\n 8  1984     445     443\n 9  1985     636     716\n10  1986     414     455\n# ℹ 16 more rows\n```\n\n\n:::\n:::\n\n\nNow, let's say we'd be interested in the percentage of female observations out of the total of observations where it was scored. We'd have to add a new column. Adding new columns is, as we've seen before, a job for `mutate()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |> \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |> \n  ungroup() |> \n  mutate(female_pct = n_obs_f / (n_obs_f + n_obs_m) * 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 4\n    year n_obs_f n_obs_m female_pct\n   <dbl>   <int>   <int>      <dbl>\n 1  1977     204     214       48.8\n 2  1978     503     433       53.7\n 3  1979     327     324       50.2\n 4  1980     605     727       45.4\n 5  1981     631     745       45.9\n 6  1982     823    1027       44.5\n 7  1983     771     797       49.2\n 8  1984     445     443       50.1\n 9  1985     636     716       47.0\n10  1986     414     455       47.6\n# ℹ 16 more rows\n```\n\n\n:::\n:::\n\n\nThe nice thing about chaining all these commands is that we can quickly build up what we want. We could, for example, easily plot the outcome of this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurveys |> \n  group_by(year) |> \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |> \n  ungroup() |> \n  mutate(female_pct = n_obs_f / (n_obs_f + n_obs_m) * 100) |> \n  ggplot(aes(x = year, y = female_pct)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![Percentage of female observations across years.](da3-08-grouped-operations_files/figure-html/fig-femalepct_year-1.png){#fig-femalepct_year width=672}\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsummary = (\n    surveys\n    .groupby(\"year\")\n    .agg(\n        n_obs_f = (\"sex\", lambda x: (x == \"F\").sum()),\n        n_obs_m = (\"sex\", lambda x: (x == \"M\").sum())\n    )\n    .reset_index()\n)\n\nsummary.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   year  n_obs_f  n_obs_m\n0  1977      204      214\n1  1978      503      433\n2  1979      327      324\n3  1980      605      727\n4  1981      631      745\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Add female percentage column\nsummary[\"female_pct\"] = summary[\"n_obs_f\"] / (summary[\"n_obs_f\"] + summary[\"n_obs_m\"]) * 100\n\nsummary.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   year  n_obs_f  n_obs_m  female_pct\n0  1977      204      214   48.803828\n1  1978      503      433   53.739316\n2  1979      327      324   50.230415\n3  1980      605      727   45.420420\n4  1981      631      745   45.857558\n```\n\n\n:::\n:::\n\n\nNow we have the table, we can easily plot it to get a better sense of any trends.\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (\n    ggplot(summary, aes(x = \"year\", y = \"female_pct\")) +\n    geom_line()\n)\n\np.show()\n```\n\n::: {.cell-output-display}\n![Percentage of female observations across years.](da3-08-grouped-operations_files/figure-html/fig-femalepct_year_py-1.png){#fig-femalepct_year_py width=614}\n:::\n:::\n\n\n\n:::\n\n::: {.callout-warning}\n## To ungroup or not ungroup (R-only)\n\nEach time you do a grouped operation, it's good practice to remove the grouping afterwards. If you don't, then you might unintentionally be doing operations within the groups later on.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nLet's illustrate this with an example. We'll take out any missing values, to simplify things.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_count <- surveys |> \n  drop_na() |> \n  group_by(sex, year) |> \n  summarise(n_obs = n())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_count\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 52 × 3\n# Groups:   sex [2]\n   sex    year n_obs\n   <chr> <dbl> <int>\n 1 F      1977   123\n 2 F      1978   430\n 3 F      1979   297\n 4 F      1980   442\n 5 F      1981   482\n 6 F      1982   672\n 7 F      1983   758\n 8 F      1984   436\n 9 F      1985   624\n10 F      1986   400\n# ℹ 42 more rows\n```\n\n\n:::\n:::\n\n\nLet's say we now wanted to transform the `n_obs` variable to a percentage of the total number of observations in the entire data set (which is 30676).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_count <- obs_count |> \n  mutate(n_obs_pct = n_obs / sum(n_obs) * 100)\n```\n:::\n\n\nWe'd expect these values in `n_obs_pct` to add up to 100%.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(obs_count$n_obs_pct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 200\n```\n\n\n:::\n:::\n\n\nHowever, they add up to 200 instead! Why? That's because the table was still grouped by `sex` and as such, the percentages were calculated by each `sex` group. There are two of them (`F`, `M` - we filtered out the missing values), so the percentages add up to 100% *within* each `sex` group.\n\nThe way to avoid this issue is to ensure we remove any groups from our table, which we can do with `ungroup()`. Here’s the full string of commands, with the ungrouping step added:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_count <- surveys |> \n  drop_na() |>                                  # remove all NAs\n  group_by(sex, year) |>                        # group by sex, year\n  summarise(n_obs = n()) |>                     # get number of rows\n  ungroup() |>                                  # ungroup here\n  mutate(n_obs_pct = n_obs / sum(n_obs) * 100)  # calculate percentage\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n```\n\n\n:::\n:::\n\n\nWe can check the percentages again and see that all is well:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(obs_count$n_obs_pct)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 100\n```\n\n\n:::\n:::\n\n\n## Python\n\nIn `pandas`, grouping only affects the aggregation step. Once you run `.groupby().agg()` or `.groupby().sum()`, the grouping is gone — the resulting DataFrame is no longer grouped.\n\nSo, things are easier in Python in this respect. Sometimes it's nice to be smug.\n:::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n- We can split our data into groups and apply operations to each group.\n- We can then combine the outcomes in a new table.\n- We use `summarise()` to calculate summary statistics (e.g. mean, median, maximum, etc).\n- Using pipes with groups (e.g. `group_by() |> summarise()`) we can calculate those summaries across groups.\n- We can also filter (`group_by() |>  filter()`) or create new columns (`group_by() |> mutate()`).\n- It is good practice to remove grouping (with `ungroup()`) from tables after `group_by()` operations, to avoid issues with retained groupings.\n\n## Python\n\n- We can split our data into groups and apply operations to each group.\n- We can then combine the outcomes in a new table.\n- We use `.agg()` to calculate summary statistics (e.g. mean, median, maximum, etc).\n- We can use `.groupby()` to group by variables in our data.\n- Using grouped filters we can find values for each group within our data.\n:::\n\n:::\n",
    "supporting": [
      "da3-08-grouped-operations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}