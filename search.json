[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis in R and Python",
    "section": "",
    "text": "Target audience\nThese sessions provide an introduction to coding in R and Python, focussing on data exploration and analysis. Both languages are leading in the field of data science and widely used for data visualisation, analyses, statistics and machine learning.\nBoth languages are open-source software and all the software we use during the course is free. We frequently run courses based on these materials, focussing on either R or Python (learning two languages at once is not ideal!).\nThese courses have been developed and are organised by the Cambridge Centre for Research Informatics Training (University of Cambridge, UK).\nHave a look at our timetable to see when the next iteration is scheduled. We run in-person and online versions of these courses.\nCourses are open to everyone.\nPlease see our guidelines for more details on eligibility and potential charges.\nThe course is aimed at beginners, so no prior knowledge is required. If you already have some coding experience, but look to refresh your knowledge, this course is also for you. Different exercise levels will help challenge you at the appropriate level.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data analysis in R and Python</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Data analysis in R and Python",
    "section": "Citation & authors",
    "text": "Citation & authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\n\nYou can cite these materials as:\n\nTavares, H., van Rongen, M. (2025). Data analysis in R and Python. https://cambiotraining.github.io/data-analysis-in-r-and-python/\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {Tavares, Hugo and van Rongen, Martin},\n  month = {6},\n  title = {Data analysis in R and Python},\n  url = {https://cambiotraining.github.io/data-analysis-in-r-and-python/},\n  year = {2025}\n}\nAbout the authors:\nHugo Tavares  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - original draft; conceptualisation; software\n\nMartin van Rongen  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - original draft; conceptualisation; software",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data analysis in R and Python</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Data analysis in R and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nSome parts of these materials are loosely based on the original course contents of the “Data Carpentry lesson in Ecology”, as released by Michonneau et al. (2019).\nWe thank Raquel Manzano Garcia, Mark Fernandes and Ian Tsang for valuable feedback on the initial manuscript.\n\n\n\n\nMichonneau, François, Tracy Teal, Auriel Fournier, Brian Seok, Adam Obeng, Aleksandra Natalia Pawlik, Ana Costa Conrado, et al. 2019. “Datacarpentry/r-Ecology-Lesson: Data Carpentry: Data Analysis and Visualization in r for Ecologists, June 2019.” Zenodo. https://doi.org/10.5281/zenodo.3264888.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data analysis in R and Python</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop, then copy it into your working directory to follow along with the materials. We cover how to do this in the Intro to software section.\nDownload",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & Setup",
    "section": "Software",
    "text": "Software\n\nR\n\nFirst install R and RStudio following these instructions.\nThen, open RStudio and run the following commands on the Console (bottom left panel) to install the required packages:\ninstall.packages(c(\"MASS\", \"janitor\", \"naniar\", \n                  \"patchwork\", \"scales\", \"tidyverse\"))\n\n\n\nPython\n\nFirst install Mamba following these instructions.\n\nWindows users: Make sure to first install WSL2 as instructed on that page.\n\nThen, open a terminal and run the following command to install the required packages:\nmamba create -n data-analysis -y -c conda-forge python plotnine pyjanitor matplotlib missingno numpy pandas seaborn textwrap jupyterlab\nOnce installation completes, activate the environment (your prompt should change to indicate you are in the data-analysis environment):\nmamba activate data-analysis\nFinally, launch JupyterLab:\njupyter lab",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "3  References",
    "section": "",
    "text": "Colour Blind Awareness. 2025. “Colour Blindness.” https://www.colourblindawareness.org/colour-blindness/.\n\n\nMichonneau, François, Tracy Teal, Auriel Fournier, Brian Seok, Adam\nObeng, Aleksandra Natalia Pawlik, Ana Costa Conrado, et al. 2019.\n“Datacarpentry/r-Ecology-Lesson: Data Carpentry: Data Analysis and\nVisualization in r for Ecologists, June 2019.” Zenodo. https://doi.org/10.5281/zenodo.3264888.\n\n\nOkabe, Masatake, and Kei Ito. 2008. “Color Universal Design (CUD)\n- How to Make Figures and Presentations That Are Friendly to Colorblind\nPeople -.” https://jfly.uni-koeln.de/color/.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html",
    "href": "materials/da1-01-intro-software.html",
    "title": "4  Intro to software",
    "section": "",
    "text": "4.1 Context",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#context",
    "href": "materials/da1-01-intro-software.html#context",
    "title": "4  Intro to software",
    "section": "",
    "text": "4.1.1 Why learn programming?\nSimply put, programming allows you to move away from point-and-click activities. They might seem great because they’re intuitive and relatively easy - until you want to redo an analysis after collecting more data. You then need to manually go through the whole process again.\nBy writing out the different steps of your analysis you’re making all those ‘clicks’ explicit. The added benefit is that it forces you to think more about what steps you need to go through in your analysis, thereby making errors less likely!\nThis also very much improves the reproducibility of the analysis: after all, running the same code on the same data should also result in the same outcome. This is great when you’re sharing your analysis with colleagues, but also great for your future self. Your brilliant approach might have made sense at the time, but who knows how much you still recall a year later… More and more often funding bodies are (rightfully!) requiring you to include raw data and analysis pipelines when submitting a paper. So, you’ll be ahead of the game.\n\n\n4.1.2 Interacting with the programming language\nIn this course we’re focusing on generic programming concepts, but we’re illustrating them in two different languages: R and Python.\nIn both cases we don’t directly type in commands in some kind of terminal window, but instead use an additional bit of software to make our programming experience a bit more user-friendly.\n\nRPython\n\n\nFor R, the RStudio software is a very popular choice - and one we’ll be using here. It’s free, open-source and well-supported. What more can you ask for?\n\n\n\n\n\n\nFigure 4.1: The welcome screen of RStudio\n\n\n\nIn the image above you can see that the window is divided into three main parts, each with various tabs. In clockwise order we have:\n\nConsole / Terminal / Background jobs\nEnvironment / History / Connections / Tutorial\nFiles / Plots / Packages / Help / Viewer / Presentation\n\nIn bold we’ve highlighted the tabs we’ll be using most.\nIn the Console we can directly run code - more on that soon. The Environment shows any information that is stored in R’s memory (empty in the figure above). The Files tab is a mini-browser; Plots shows you any plots that have been created and Help is where you can go for… well, help.\n\n\nFor Python, the JupyterLab software is a very popular choice - and one we’ll be using here. It’s free, open-source and well-supported. What more can you ask for?\n\n\n\n\n\n\nFigure 4.2: The welcome screen of JupyterLab",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#working-directory",
    "href": "materials/da1-01-intro-software.html#working-directory",
    "title": "4  Intro to software",
    "section": "4.2 Working directory",
    "text": "4.2 Working directory\nA good way of staying organised is to keep all the files related to a given project together. Using that concept when programming is really helpful, because it makes it easier for the computer to find all the data, scripts and other information related to an analysis.\nWe often refer to this as the working directory. This simply is the starting point for the computer to look for stuff.\nBecause you easily accumulate a lot of files when analysing data, it’s good to be organised. During this course we’ll create a project folder called data-analysis, which we’ll make our working directory.\nWithin this folder we’ll have sub folders that allow us to further organise our data. We’ll use the following structure:\n\n\n\n\n\n\nFigure 4.3: The working directory structure of this course\n\n\n\n\n\n\n\n\n\n\nFolder\nDescription\n\n\n\n\ndata\nContains the data files we’ll use in this course, for example surveys.csv. For your own analysis you might want to consider adding another folder within this to contain the raw data. It’s good practice to always keep an untouched copy of your raw data. This helps with transparency and allows you analyse data differently in the future. Aim to keep your data cleaning and analyses programmatically.\n\n\nimages\nThis folder will contain any images you might produce, for example for publications or data exploration.\n\n\nscripts\nHere we can store any scripts we create. Here it’s also good to be structured and organised, something we cover a bit more in Section 4.4.1.\n\n\n…\nThe opportunities are endless. You can add folders for documents, presentations, etc. How you do things matters less than being consistent!\n\n\n\nAll the files in the working directory can be referenced using relative paths. This allows you to move you working directory across your computer - or to other computers - without breaking any of the links within your scripts.\n\n\n\n\n\n\nImportantRelative versus absolute paths\n\n\n\nRelative paths are relative to a certain location on your computer. Absolute paths start from the absolute start of your hard drive. This is easiest illustrated with an example:\n\n\n\n\n\n\nFigure 4.4: Relative vs absolute paths\n\n\n\nThe advantage of using relative paths instead of absolute paths is that they still work if you move your analysis to another computer (or you send your analysis to a collaborator). This means that it greatly improves reproducibility of the code, since your code will still work on other people’s computers!\n\n\n\n4.2.1 Creating a working directory\nBefore we start writing any code we’ll set up our working environment properly. To do this, we’ll create our data-analysis working directory, with all its sub folders.\n\nRPython\n\n\nThe easiest way to set up a working directory in R is to create an R-project. This is simply a folder on your computer with a shortcut in it (ending in .RProj). When you double-click on the shortcut, it opens RStudio and sets the working directory to that particular folder.\nTo create an “R Project”:\n\n\n\n  \n  \n                \n          \n            \n          \n            \n          \n            \n          \n  \n  \n    \n\n\n\n  \n  \n    \n    Previous\n  \n  \n    \n    Next\n  \n\n\n  Setting up a new working directory (click to toggle).\n\n\n\nStart RStudio.\nUnder the File menu, click on New Project. Choose New Directory, then New Project.\nEnter a name for this new folder (or “directory”), and choose a convenient location for it. This will be your working directory for the rest of the day (e.g., ~/data-analysis).\nClick on Create Project.\nTick Open in new session to ensure RStudio starts afresh.\n\nR will show you your current working directory in the Files pane. Alternatively, you can get it by typing in and running the getwd() command.\n\n\nIn Figure 4.2 we can see the file browser on the left and the Launcher window on the right. The file browser has a folder called data-analysis, with three subfolders: data, images and scripts.\nTo recreate this, launch JupyterLab by opening the terminal, activating your environment (mamba activate pycourse) and launching JupyterLab (jupyter lab).\nNavigate to the folder you want to store your working directory (here we’ve put it under Desktop) and create a new folder called data-analysis. Within this folder you can create the necessary subfolders.\nTo check which folder is currently your working directory, run:\n\nimport os\nprint(os.getcwd())\n\nIf it’s incorrect, then the easiest way of setting it correctly is to close JupyterLab, go to the terminal, navigate to the data-analysis folder and launch JupyterLab from there, e.g.\n\ncd /Desktop/data-analysis\njupyter lab\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nComplete setting up a working directory before proceeding.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#working-with-r-or-python",
    "href": "materials/da1-01-intro-software.html#working-with-r-or-python",
    "title": "4  Intro to software",
    "section": "4.3 Working with R or Python",
    "text": "4.3 Working with R or Python\nThe basis of programming is that we create or code instructions for the computer to follow. Next, we tell the computer to follow the instructions by executing or running those instructions.\n\n4.3.1 Scripts versus console\nThere are two main ways of interacting with the language: by using the console or by using script files (plain text files that contain your code). The console is the place where commands can be typed and executed immediately by the computer. It is also where the results will be shown for commands that have been executed. However, no record is kept and any information is lost once the session is closed.\nInstead, we uses scripts to write all the commands. That way there is a complete record of what we did, and anyone (including our future selves!) can easily replicate the results on their computer.\n\n\n\n\n\n\n\nConsole/terminal \nScript/Notebook \n\n\n\n\nruns code directly\nin essence, a text file\n\n\ninteractive\nneeds to be told to run\n\n\nno record\nrecords actions\n\n\ndifficult to trace progress\ntransparent workflow\n\n\n\n\nRPython\n\n\nWe can type commands directly in the Console pane and press EnterEnter.\nRStudio also allows you to execute commands directly from the script editor by using the ControlControl + EnterEnter shortcut (on Macs,  +  will work, too). The command on the current line in the script (indicated by the cursor) or all of the commands in the currently selected text will be sent to the console and executed when you press ControlControl + EnterEnter. You can find other keyboard shortcuts in this RStudio cheatsheet about the RStudio IDE (PDF).\n\n\n\n\n\n\nWarningThe R prompt\n\n\n\nIf R is ready to accept commands, the R console shows a &gt; prompt. If it receives a command (by typing, copy-pasting or sent from the script editor using ControlControl + EnterEnter), R will try to execute it, and when ready, will show the results and come back with a new &gt; prompt to wait for new commands.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. This is because you have not ‘closed’ a parenthesis or quotation, i.e. you don’t have the same number of left-parentheses as right-parentheses, or the same number of opening and closing quotation marks. When this happens, and you thought you finished typing your command, click inside the console window and press EscapeEscape. This will cancel the incomplete command and return you to the &gt; prompt.\n\n\n\n\nWe can open a Console from the Launcher window and directly type and execute code. We simply type the code and press ShiftShift + EnterEnter (or the “play” button in the top-left).\n\n  \n  \n                \n          \n            \n          \n  \n  \n    \n\n  \n  \n    \n    Previous\n  \n  \n    \n    Next\n  \n\n\n  Running code from the console (click to toggle).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#running-code",
    "href": "materials/da1-01-intro-software.html#running-code",
    "title": "4  Intro to software",
    "section": "4.4 Running code",
    "text": "4.4 Running code\nThe simplest way of using a programming language is to use it interactively. We can do this by typing directly into the console / terminal.\n\nRPython\n\n\nFor example, you can use R as a glorified calculator:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\n\n\nFor example, you can use Python as a glorified calculator:\n\n3 + 5\n\n8\n\n12 / 7\n\n1.7142857142857142\n\n\n\n\n\nRunning code like this directly in the console is generally not a good idea, because then we can’t keep track of what we are doing. So, we first need to create a script to save our code in. Then, we can then play around. Let’s focus on that next.\n\n\n\n\n\n\nImportant\n\n\n\nComplete creating a script before proceeding.\n\n\n\n4.4.1 Splitting code\nAs your analysis grows, so does your code. So, often we want to split analyses into multiple scripts, for example:\n\n01_preprocessing may contain data cleaning steps\n02_exploration may contain exploratory plots of your data\n03_analysis could contain (statistical) analyses of your data\n04_figures could contain code for figures, ready for publication\n\n\nRPython\n\n\nEach of these files could be hundreds of lines long. So, keeping track of your code makes sense. We can do that with code headings, which use the # heading ---- syntax. You can even add different heading levels, by increasing the number of # at the start.\nThis creates a little table of contents in the bottom-left corner of the script pane:\n\n\n\nCode headings\n\n\n\n\nEach of these files could be hundreds of lines long. So, keeping track of your code makes sense. We can do that with code headings, which use the markdown syntax. We won’t go into details on how markdown works (see here for basic syntax), but suffice for now is that you can even add different heading levels, by increasing the number of # at the start.\nWe define these by clicking on a cell, and changing the type from Code to Markdown (next to the fast-forward icon at the top).\nThis creates a table of contents, accessible in the TOC pane on the left:\n\n\n\nCode headings\n\n\n\n\n\n\n\n4.4.2 Comments in code\nIt’s always a good idea to add explanations to your code. We can do that with the hash tag # symbol, for example:\n\n# This code calculates the sum of two numbers\n1 + 9\n\nIt’s always a good idea to add lots of comments to your code. What makes sense to you in that moment, might not a week later. Similarly, when sharing code with colleagues and collaborators, it’s always good to be as clear as possible.\n\n\n\n\n\n\nImportantComplete before proceeding\n\n\n\nPlease complete the exercises to create a script and trial running code.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#functions-and-their-arguments",
    "href": "materials/da1-01-intro-software.html#functions-and-their-arguments",
    "title": "4  Intro to software",
    "section": "4.5 Functions and their arguments",
    "text": "4.5 Functions and their arguments\nFunctions are “canned scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing packages (more on that later). A function usually takes one or more inputs called arguments. Functions often (but not always) return a value. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number.\n\nRPython\n\n\n\nsqrt(9)\n\n\n\nThe sqrt() function is not available by default, but is stored in the math module. Before we can use it, we need to load this module:\n\nimport math\n\nNext, we can use the sqrt() function, specifying that it comes from the mathmodule. We separate the two with a full-stop (.):\n\nmath.sqrt(9)\n\n3.0\n\n\n\n\n\nHere, the value 9 is given to the sqrt() function. This function calculates the square root, and returns the value. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a data set. We’ll see that when we read data files.\n\n4.5.1 Arguments\nArguments allow you to control the behaviour of a function. They can be anything, not only numbers or file names. Exactly what each argument means differs per function and can be looked up in the documentation. Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options.\nOptions are typically used to alter the way the function operates, such as if it should ignore missing values, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\nLet’s try a function that can take multiple arguments: round().\n\nRPython\n\n\n\nround(3.14159)\n\n[1] 3\n\n\n\n\n\nround(3.14159)\n\n3\n\n\n\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number. If we want more digits we can see how to do that by getting information about the round() function.\n\nRPython\n\n\nWe can use args(round) to find what arguments it takes, or look at the help for this function using ?round.\n\nargs(round)\n\nfunction (x, digits = 0, ...) \nNULL\n\n\nWe see that if we want a different number of digits, we can type digits = 2 or however many we want. For example:\n\nround(x = 3.14159, digits = 2)\n\nIf you provide the arguments in the exact same order as they are defined you don’t have to name them:\n\nround(3.14159, 2)\n\nAnd if you do name the arguments, you can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n\n\n\n\n\n\nTipGetting help\n\n\n\nYou can have a more detailed look by checking the help page of the function. You can access any help page by typing help(function_name) or ?function_name. So, for the round function this would be help(round) or ?round.\n\n\n\n\nWe can use help(round) to find what arguments it takes.\n\nhelp(round)\n\nWe see that if we want a different number of digits, we can type ndigits = 2 or however many we want. For example:\n\nround(3.14159, ndigits = 2)\n\n3.14\n\n\nIf you provide the arguments in the exact same order as they are defined you don’t have to name them:\n\nround(3.14159, 2)\n\n3.14\n\n\nPython still expects the arguments in the correct order, so this gives an error:\n\nround(ndigits = 2, 3.14159)\n\n\n\n\nIt’s good practice be explicit about the names of the arguments. That way you can avoid confusion later on when looking back at your code or when sharing your code.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#adding-functionality-using-packages",
    "href": "materials/da1-01-intro-software.html#adding-functionality-using-packages",
    "title": "4  Intro to software",
    "section": "4.6 Adding functionality using packages",
    "text": "4.6 Adding functionality using packages\n\nRPython\n\n\nAdditional packages can be installed to extend the functionality of R. Most packages are available in a central repository called CRAN and can be installed from within R using the install.packages() function.\nFor example, to install (or update) the tidyverse package, you would run the following command on the console:\n\ninstall.packages(\"tidyverse\")\n\nBecause the install process accesses the CRAN repository, you will need an Internet connection to install packages.\nAfter this, you can then load the package to use it in your analysis. You load packages using the library() command (note the lack of quotation marks):\n\nlibrary(tidyverse)\n\nThere are other repositories available. A very popular one is the Bioconductor project, which contains thousands of packages for bioinformatics applications. These use a different installation command, but the instructions are always given in each package’s page.\n\n\nAdditional packages can be installed to extend the functionality of Python. In the next section we’ll need some functionality from the numpy package, so we’ll install it here. We can do this directly from within JupyterLab:\n\n!mamba install -c conda-forge numpy -y\n\nThis uses the mamba package manager to install numpy. It gets the information from the conda-forge channel and the -y simply auto-confirms the installation.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#exercises",
    "href": "materials/da1-01-intro-software.html#exercises",
    "title": "4  Intro to software",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\n4.7.1 Creating a working directory\n\n\n\n\n\n\nExerciseExercise 1 - Creating a working directory\n\n\n\n\n\n\nLevel: \nCreate a working directory called data-analysis. When you’ve done this, add the following sub folders:\n\ndata\nscripts\nimages\n\nNote: programming languages are case-sensitive, so data is not treated the same way as Data.\n\n\n\n\n\n\n4.7.2 Creating a script\n\n\n\n\n\n\nExerciseExercise 2 - Creating a script\n\n\n\n\n\n\nLevel: \nCreate a script and save it as 01_session in the scripts folder within your working directory.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\nGo to File &gt; New File &gt; R Script in RStudio and save the script. Remember to add the .R extension.\n\n\nIn the Launcher window, click on Notebook, Python 3 and save the file (Save symbol at the top-left):\n\n  \n  \n                \n          \n            \n          \n            \n          \n            \n          \n            \n          \n  \n  \n    \n\n\n\n\n  \n  \n    \n    Previous\n  \n  \n    \n    Next\n  \n\n\n  Creating a new Notebook (click to toggle).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.3 Running code\n\n\n\n\n\n\nExerciseExercise 3 - Running code\n\n\n\n\n\n\nLevel: \nIn your new script 01_session, run some mathematical operations, such as:\n\n8 * 4\n6 - 9\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nRemember, you run the code using Ctrl + Enter (or Command + Enter on Mac).\n\n\n\n\n\n\n\n\n\n\n4.7.4 Functions\n\n\n\n\n\n\nExerciseExercise 4 - Functions\n\n\n\n\n\n\nLevel: \nIn your new script 01_session, find the absolute value for -11. Use Google or a chatbot to find which function you need.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\nabs(-11)\n\n[1] 11\n\n\n\n\n\nabs(-11)\n\n11",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-01-intro-software.html#summary",
    "href": "materials/da1-01-intro-software.html#summary",
    "title": "4  Intro to software",
    "section": "4.8 Summary",
    "text": "4.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe using a working directory as a starting point for our analysis\nAlthough we can run code interactively in the console, it’s best to create a script to run your code, so you can keep a record.\nFunctions allows you to automate more complicated sets of commands and many are predefined.\nFunctions often have arguments that allow you to control how the function behaves.\nWe can add additional functionality to our programming language using packages.\nPackages need to be installed once and loaded every time we restart our session.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Intro to software</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html",
    "href": "materials/da1-02-basic-objects-and-data-types.html",
    "title": "5  Data types & structures",
    "section": "",
    "text": "5.1 Context\nWe’ve seen examples where we entered data directly into a function. Most of the time we have data from elsewhere, such as a spreadsheet. In the previous section we created single objects. We’ll build up from this and introduce vectors and tabular data. We’ll also briefly mention other data types, such as matrices, arrays.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#explained-data-types-structures",
    "href": "materials/da1-02-basic-objects-and-data-types.html#explained-data-types-structures",
    "title": "5  Data types & structures",
    "section": "5.2 Explained: Data types & structures",
    "text": "5.2 Explained: Data types & structures\nComputers are picky when it comes to data and they like consistency. As such, it’s good to be aware of the fact that data can be viewed or interpreted in different ways by the computer.\nFor example, you might have research data where the presence or absence of a tumour is scored. This would often be recorded as 0 when absent and 1 as present. Your computer views these values as numbers and would happily calculate the average of those values. Not ideal, because a tumour being, on average, 0.3 present makes no sense!\nSo, it is important to spend a bit of time looking at your data to make sure that the computer sees it in the correct way.\n\n5.2.1 Quantitative data\n\nDiscrete data\nDiscrete data are numerical data that can only take distinct values. They can be counted and only take whole numbers. Examples of discrete data include, for example, the number of planets in a solar system or the number of questions answered on an exam.\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\nThe number of questions answered on an exam (e.g. 12 out of 20)\n\n\n\nIf somebody has completed a survey (binary data; yes/no)\n\n\n\nThe number of students in a class (e.g. 20, 32)\n\n\n\n\n\nContinuous data\nContinuous data can take any value within a given range. These data can be measured and can include decimals or fractions.\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\nTemperature of a liquid (e.g. 20 °C)\n\n\n\nHeight of people in a cohort (e.g. 168 cm)\n\n\n\nAverage heart rate in a patient (e.g. 70 beats per minute)\n\n\n\nWater levels in an aquifer (e.g. 2.4 metres)\n\n\n\n\n\n\n5.2.2 Qualitative data\nQualitative data are data that describe qualities which can’t be measured or quantified numerically. We can roughly split these data into two types: ones with an inherent order to them, and ones without.\n\nNominal data: categories\nThese are categorical data that represent categories or distinct groups, without any inherent order or ranking.\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\nEye colour (e.g. blue, brown)\n\n\n\nEducation level (e.g. primary school, secondary school)\n\n\n\nTreatment group (e.g. control, treatment)\n\n\n\n\n\nOrdinal data: categories with ranking or ordering\nOrdinal data are similar to nominal data, in that they represent different categories or groups. However, these also have an inherent ordering to them.\n\n\n\n\n\n\n\n\nDescription\n\n\n\n\n\nRating scale (e.g., 1 to 5 stars for difficulty levels)\n\n\n\nRank or position (e.g., 1st, 2nd, 3rd place in a tournament)\n\n\n\nOrder or progression (e.g., low, medium, high priority)\n\n\n\n\n\n\n5.2.3 Getting the computer to see the right way\nIn general, computers can view these different types of data in specific ways.\n\nRPython\n\n\nR has the following main data types:\n\n\n\n\n\n\n\nData type\nDescription\n\n\n\n\nnumeric\nRepresents numbers; can be whole (integers) or decimals\n(e.g., 19or 2.73).\n\n\ninteger\nSpecific type of numeric data; can only be an integer\n(e.g., 7L where L indicates an integer).\n\n\ncharacter\nAlso called text or string\n(e.g., \"Rabbits are great!\").\n\n\nlogical\nAlso called boolean values; takes either TRUE or FALSE.\n\n\nfactor\nA type of categorical data that can have inherent ordering\n(e.g., low, medium, high).\n\n\n\n\n\nPython has the following main data types:\n\n\n\n\n\n\n\nData type\nDescription\n\n\n\n\nint\nSpecific type of numeric data; can only be an integer\n(e.g., 7 or 56).\n\n\nfloat\nDecimal numbers\n(e.g., 3.92 or 9.824).\n\n\nstr\nText or string data\n(e.g., \"Rabbits are great!\").\n\n\nbool\nLogical or boolean values; takes either True or False.\n\n\n\n\n\n\n\n\n5.2.4 Data structures\nIn the section on running code we saw how we can run code interactively. However, we frequently need to save values so we can work with them. We’ve just seen that we can have different types of data. We can save these into different data structures. Which data structure you need is often determined by the type of data and the complexity.\nIn the following sections we look at simple data structures.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#objects",
    "href": "materials/da1-02-basic-objects-and-data-types.html#objects",
    "title": "5  Data types & structures",
    "section": "5.3 Objects",
    "text": "5.3 Objects\nWe can store values into objects. To do this, we assign values to them. An object acts as a container for that value.\nTo create an object, we need to give it a name followed by the assignment operator and the value we want to give it, for example:\n\nRPython\n\n\n\ntemperature &lt;- 23\n\nWe can read the code as: the value 23 is assigned (&lt;-) to the object temperature. Note that when you run this line of code the object you just created appears on your environment tab (top-right panel).\nWhen assigning a value to an object, R does not print anything on the console. You can print the value by typing the object name on the console or within your script and running that line of code.\n\n\n\ntemperature = 23\n\nWe can read the code as: the value 23 is assigned (=) to the object temperature.\nWhen assigning a value to an object, Python does not print anything on the console. You can print the value by typing the object name on the console or within your script and running that line of code.\n\n\n\n\n\n\n\n\n\nImportantThe assignment operator\n\n\n\nWe use an assignment operator to assign values on the right to objects on the left.\n\nRPython\n\n\nIn R we use &lt;- as the assignment operator.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke on a PC, while typing Option + - (push Option at the same time as the - key) does the same on a Mac. \n\n\n\n\n\n\nNote\n\n\n\nAlthough R also supports the use of = as an assignment operator, there are some very slight differences in their use, as illustrated here. Generally, people just stick to &lt;- for those reasons.\n\n\n\n\nIn Python we use = as the assignment operator. \n\n\n\n\n\nObjects can be given almost any name such as x, current_temperature, or subject_id. You want the object names to be explicit and short. There are some exceptions / considerations (see below).\n\n\n\n\n\n\nWarningRestrictions on object names\n\n\n\nObject names can contain letters, numbers, underscores and periods.\nThey cannot start with a number nor contain spaces. Different people use different conventions for long variable names, two common ones being:\nUnderscore: my_long_named_object\nCamel case: myLongNamedObject\nWhat you use is up to you, but be consistent. Programming languages are case-sensitive so temperature is different from Temperature.\n\nSome names are reserved words or keywords, because they are the names of core functions (e.g., if, else, for, see R or Python for a complete list).\nAvoid using function names (e.g., c, T, mean, data, df, weights), even if allowed. If in doubt, check the help to see if the name is already in use.\nAvoid full-stops (.) within an object name as in my.data. Full-stops often have meaning in programming languages, so it’s best to avoid them.\nUse consistent styling.\n\nWhatever style you use, be consistent!\n\n\n\n5.3.1 Using objects\nNow that we have the temperature in memory, we can use it to perform operations. For example, this might the temperature in Celsius and we might want to calculate it to Kelvin.\nTo do this, we need to add 273.15:\n\nRPython\n\n\n\ntemperature + 273.15\n\n[1] 296.15\n\n\n\n\n\ntemperature + 273.15\n\n296.15\n\n\n\n\n\nWe can change an object’s value by assigning a new one:\n\nRPython\n\n\n\ntemperature &lt;- 36\ntemperature + 273.15\n\n[1] 309.15\n\n\n\n\n\ntemperature = 36\ntemperature + 273.15\n\n309.15\n\n\n\n\n\nFinally, assigning a value to one object does not change the values of other objects. For example, let’s store the outcome in Kelvin into a new object temp_K:\n\nRPython\n\n\n\ntemp_K &lt;- temperature + 273.15\n\n\n\n\ntemp_K = temperature + 273.15\n\n\n\n\nChanging the value of temperature does not change the value of temp_K.\n\nRPython\n\n\n\ntemperature &lt;- 14\ntemp_K\n\n[1] 309.15\n\n\n\n\n\ntemperature = 14\ntemp_K\n\n309.15",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#collections-of-data",
    "href": "materials/da1-02-basic-objects-and-data-types.html#collections-of-data",
    "title": "5  Data types & structures",
    "section": "5.4 Collections of data",
    "text": "5.4 Collections of data\nIn the examples above we have stored single values into an object. Of course we often have to deal with more than that. Generally speaking, we can create collections of data. This enables us to organise our data, for example by creating a collection of numbers or text values.\nCreating a collection of data is pretty straightforward, particularly if you are doing it manually. Conceptually, we can of these collections in 4 distinct ways, based on the type of data they contain. We’ll cover tabular data in the next chapter.\n\n\n\nCollection\nR\nPython\n\n\n\n\n1D homogeneous\nvector\nNumPy array (1D)\n\n\n2D homogeneous\nmatrix / array\nNumPy array (2D)\n\n\nGeneral container\nlist\nlist / tuple\n\n\nTabular (mixed)\ndata.frame/tibble\npandas DataFrame\n\n\n\n\n\n\n\n\n\nImportantHaving a type\n\n\n\nDifferent data types result in slightly different types of objects. It can be quite useful to check how your data is viewed by the computer.\n\nRPython\n\n\nWe can use the class() function to check what type of object we’re dealing with.\n\nclass(temp_K)\n\n[1] \"numeric\"\n\n\n\n\nWe can use the type() function to check what type of object we’re dealing with.\n\ntype(temp_K)\n\n&lt;class 'float'&gt;\n\n\n\n\n\n\n\n\n5.4.1 Homogeneous (1D)\n\nRPython\n\n\nThe simplest collection of data in R is called a vector. This really is the workhorse of R.\nA vector is composed by a series of values, which can numbers, text or any of the data types described. However, they are expected to all be of the same type.\nWe can assign a series of values to a vector using the c() function. For example, we can create a vector of temperatures and assign it to a new object temp_c:\n\ntemp_c &lt;- c(23, 24, 31, 27, 18, 21)\n\ntemp_c        # check object contents\n\n[1] 23 24 31 27 18 21\n\nclass(temp_c) # check object type\n\n[1] \"numeric\"\n\n\nA vector can also contain text. For example, let’s create a vector that contains weather descriptions:\n\nweather &lt;- c(\"sunny\", \"cloudy\", \"partial_cloud\", \"cloudy\", \"sunny\", \"rainy\")\n\nweather        # check object contents \n\n[1] \"sunny\"         \"cloudy\"        \"partial_cloud\" \"cloudy\"       \n[5] \"sunny\"         \"rainy\"        \n\nclass(weather) # check object type\n\n[1] \"character\"\n\n\n\n\nIn Python NumPy arrays are incredibly efficient for computing, so they are widely used. We can access NumPy as follows:\n\nimport numpy as np\n\nNext, we can create a simple NumPy array that contains numbers.\n\ntemp_c = np.array([23, 24, 31, 27, 18, 21])\n\ntemp_c       # check object contents\n\narray([23, 24, 31, 27, 18, 21])\n\ntype(temp_c) # check object type\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nWe can do something similar using text / character strings:\n\nweather = ([\"sunny\", \"cloudy\", \"partial_cloud\", \"cloudy\", \"sunny\", \"rainy\"])\n\nweather       # check object contents\n\n['sunny', 'cloudy', 'partial_cloud', 'cloudy', 'sunny', 'rainy']\n\ntype(weather) # check object type\n\n&lt;class 'list'&gt;\n\n\n\n\n\nNote that when we define text (e.g. \"cloudy\" or \"sunny\"), we need to use quotes.\nWhen we deal with numbers - whole or decimal (e.g. 23, 18.5) - we do not use quotes.\n\n\n5.4.2 Homogeneous (2D)\nOften we have more than just one set of data points. Following our temperature example, let’s say we measured minimum and maximum temperatures across a range of days.\nWe could arrange them in two columns, one with the minimum and one with the maximum values. All of the data is of the same type: numerical.\n\nRPython\n\n\nIn R we can do this by creating an array / matrix.\n\ntemps &lt;- array(c(18, 20, 25, 22, 15, 17,\n                 23, 24, 31, 27, 18, 21),\n               dim = c(6, 2))\n\ntemps        # check object contents\n\n     [,1] [,2]\n[1,]   18   23\n[2,]   20   24\n[3,]   25   31\n[4,]   22   27\n[5,]   15   18\n[6,]   17   21\n\nclass(temps) # check object type\n\n[1] \"matrix\" \"array\" \n\n\n\n\nIn Python we can do this by creating a NumPy array:\n\ntemps = np.array([\n    [18, 23],\n    [20, 24],\n    [25, 31],\n    [22, 27],\n    [15, 18],\n    [17, 21]\n])\n\ntemps       # check object contents\n\narray([[18, 23],\n       [20, 24],\n       [25, 31],\n       [22, 27],\n       [15, 18],\n       [17, 21]])\n\ntemps.shape # check object dimensions\n\n(6, 2)\n\ntype(temps) # check object type\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\n\n\n\n\n5.4.3 General container\n\nRPython\n\n\nIn R we can use a list to store different types of data - which do not need to be of the same length (this is different to tabular data, which we’ll cover in the next chapter).\nHave a look at the following example:\n\nlist_example &lt;- list(\n  temperature = c(18, 20, 25, 22, 15, 17),     # numeric vector\n  weather     = c(\"sunny\", \"cloudy\", \"rainy\"), # character vector\n  flag        = TRUE,                          # logical\n  note        = \"Weather observations\"         # string\n)\n\nlist_example        # check object contents\n\n$temperature\n[1] 18 20 25 22 15 17\n\n$weather\n[1] \"sunny\"  \"cloudy\" \"rainy\" \n\n$flag\n[1] TRUE\n\n$note\n[1] \"Weather observations\"\n\nclass(list_example) # check object type\n\n[1] \"list\"\n\n\nThis returns all the individual parts of the list. We won’t work much with lists in this course, but you’re likely to encounter them in the future - for example if you’re doing statistical analysis.\n\n\nGeneral data containers in Python can either a list or a tuple. Both can hold items of the same of different types. The difference between the two is that a list can be changed (mutable), whereas a tuple cannot be changed after it’s created (immutable).\nWe can assign a collection of numbers to a list:\n\ntemp_c = [23, 24, 31, 27, 18, 21]\n\ntemp_c       # check object contents\n\n[23, 24, 31, 27, 18, 21]\n\ntype(temp_c) # check object type\n\n&lt;class 'list'&gt;\n\n\nA list can also contain text. For example, let’s create a list that contains weather descriptions:\n\nweather = [\"sunny\", \"cloudy\", \"partial_cloud\", \"cloudy\", \"sunny\", \"rainy\"]\n\nweather       # check object contents\n\n['sunny', 'cloudy', 'partial_cloud', 'cloudy', 'sunny', 'rainy']\n\ntype(weather) # check object type\n\n&lt;class 'list'&gt;\n\n\nWe can also create a tuple. Remember, this is like a list, but it cannot be altered after creating it. Note the difference in the type of brackets, where we use ( ) round brackets instead of [ ] square brackets:\n\ntemp_c_tuple = (23, 24, 31, 27, 18, 21)\n\ntemp_c_tuple       # check object contents\n\n(23, 24, 31, 27, 18, 21)\n\ntype(temp_c_tuple) # check object type\n\n&lt;class 'tuple'&gt;",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#type-coercion",
    "href": "materials/da1-02-basic-objects-and-data-types.html#type-coercion",
    "title": "5  Data types & structures",
    "section": "5.5 Type coercion",
    "text": "5.5 Type coercion\nThis occurs when there are more than one type of data (e.g. numerical, text, logical) in an object that expects all data to be the same. What the computer then does is to coerce all the data to a common type that avoids data loss.\nTake a look at the following example, where we’re mixing different data types.\n\nRPython\n\n\n\nmixed_data &lt;- c(264, NA, \"Bob\", 12)\n\nmixed_data        # object contents\n\n[1] \"264\" NA    \"Bob\" \"12\" \n\nclass(mixed_data) # object type\n\n[1] \"character\"\n\n\nWhat has happened is that all values have been coerced to character.\n\n\nIn Python, lists are fine with different types of data. NumPy arrays however expect a single type.\n\nmixed_data = np.array([264, None, \"Bob\", 12])\n\nmixed_data       # object contents\n\narray([264, None, 'Bob', 12], dtype=object)\n\ntype(mixed_data) # object type\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nWe can see that the object type is a NumPy array, but at the output of the data we see dtype=object, which means character data.\n\n\n\nThis happens because the computer doesn’t know what to do when it encounters more than one data type (numerical, logical and text, in this case). To preserve as much data, it converts everything to text.\n\n5.5.1 Converting types\nIn some cases you might want to enforce a certain data type. If you do this, just be aware that some data could get lost.\nLook at the following example, where we create a very simple 1D collection of data, where we introduced a number in quotes, so it’s viewed as text. In this case, forcing all the data as numeric would fix that error.\n\nRPython\n\n\n\ntemp_error &lt;- c(12, 23, \"18\", 26)\n\nclass(temp_error)\n\n[1] \"character\"\n\n\n\ntemp_error &lt;- as.numeric(temp_error)\n\nclass(temp_error)\n\n[1] \"numeric\"\n\n\n\n\nHere we create a NumPy array and check the data type:\n\ntemp_error = np.array([12, 23, \"18\", 26])\n\nprint(temp_error, temp_error.dtype) # check the contents and data type\n\n['12' '23' '18' '26'] &lt;U21\n\n\nIt gives us &lt;U21 as a data type. This indicates that NumPy sized the array as a Unicode string array with a maximum of 21 characters. That’s quite a long-winded way of saying “they are not viewed as numbers”.\nThankfully we can fix that, by converting the type to int or integers.\n\ntemp_error = temp_error.astype(int)\n\nprint(temp_error, temp_error.dtype)\n\n[12 23 18 26] int64",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#making-changes",
    "href": "materials/da1-02-basic-objects-and-data-types.html#making-changes",
    "title": "5  Data types & structures",
    "section": "5.6 Making changes",
    "text": "5.6 Making changes\nQuite often we would want to make some changes to a collection of data. There are different ways we can do this.\nLet’s say we gathered some new temperature data and wanted to add this to the original temp_c data.\n\nRPython\n\n\nWe’d use the c() function to combine the new data:\n\nc(temp_c, 22, 34)\n\n[1] 23 24 31 27 18 21 22 34\n\n\n\n\nWe take the original temp_c list and add the new values:\n\ntemp_c + [22, 34]\n\n[23, 24, 31, 27, 18, 21, 22, 34]\n\n\n\n\n\nLet’s consider another scenario. Again, we went out to gather some new temperature data, but this time we stored the measurements into an object called temp_new and wanted to add these to the original temp_c data.\n\nRPython\n\n\n\ntemp_new &lt;- c(5, 16, 8, 12)\n\nNext, we wanted to combine these new data with the original data, which we stored in temp_c.\nAgain, we can use the c() function:\n\nc(temp_c, temp_new)\n\n [1] 23 24 31 27 18 21  5 16  8 12\n\n\n\n\n\ntemp_new = [5, 16, 8, 12]\n\nWe can use the + operator to add the two lists together:\n\ntemp_c + temp_new\n\n[23, 24, 31, 27, 18, 21, 5, 16, 8, 12]\n\n\n\n\n\n\n5.6.1 Number sequences\nWe often need to create sequences of numbers when analysing data. There are some useful shortcuts available to do this, which can be used in different situations. Run the following code to see the output.\n\nRPython\n\n\n\n1:10                                # integers from 1 to 10\n10:1                                # integers from 10 to 1\nseq(1, 10, by = 2)                  # from 1 to 10 by steps of 2\nseq(10, 1, by = -0.5)               # from 10 to 1 by steps of -0.5\nseq(1, 10, length.out = 21)         # 21 equally spaced values from 1 to 10\n\n\n\nPython has some built-in functionality to deal with number sequences, but the numpy library is particularly helpful. We installed and loaded it previously, but if needed, re-run the following:\n\nimport numpy as np\n\nNext, we can create several different number sequences:\n\nlist(range(1, 11))                 # integers from 1 to 10\nlist(range(10, 0, -1))             # integers from 10 to 1\nlist(range(1, 11, 2))              # from 1 to 10 by steps of 2\nlist(np.arange(10, 1, -0.5))       # from 10 to 1 by steps of -0.5\nlist(np.linspace(1, 10, num = 21)) # 21 equally spaced values from 1 to 10",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#subsetting",
    "href": "materials/da1-02-basic-objects-and-data-types.html#subsetting",
    "title": "5  Data types & structures",
    "section": "5.7 Subsetting",
    "text": "5.7 Subsetting\nSometimes we want to extract one or more values from a collection of data. We will go into more detail later, but for now we’ll see how to do this on the simple data structures we’ve covered so far.\nFor simple subsetting we can use square brackets [ ] to extract values. Let’s explore this using our weather object.\n\nRPython\n\n\n\nweather          # remind ourselves of the data\n\n[1] \"sunny\"         \"cloudy\"        \"partial_cloud\" \"cloudy\"       \n[5] \"sunny\"         \"rainy\"        \n\nweather[2]       # extract the second value\n\n[1] \"cloudy\"\n\nweather[2:4]     # extract the second to fourth value\n\n[1] \"cloudy\"        \"partial_cloud\" \"cloudy\"       \n\nweather[c(3, 1)] # extract the third and first value\n\n[1] \"partial_cloud\" \"sunny\"        \n\nweather[-1]      # extract all apart from the first value\n\n[1] \"cloudy\"        \"partial_cloud\" \"cloudy\"        \"sunny\"        \n[5] \"rainy\"        \n\n\n\n\n\nweather          # remind ourselves of the data\n\n['sunny', 'cloudy', 'partial_cloud', 'cloudy', 'sunny', 'rainy']\n\nweather[1]       # extract the second value\n\n'cloudy'\n\nweather[1:4]     # extract the second to fourth value (end index is exclusive)\n\n['cloudy', 'partial_cloud', 'cloudy']\n\nweather[1:]      # extract all apart from the first value\n\n['cloudy', 'partial_cloud', 'cloudy', 'sunny', 'rainy']\n\n\n\n\n\n\n\n\n\n\n\nWarningTechnical: Differences in indexing between R and Python\n\n\n\n\n\nIn the course materials we keep R and Python separate in most cases. However, if you end up using both languages at some point then it’s important to be aware about some key differences. One of them is indexing.\nEach item in a collection of data has a number, called an index. Now, it would be great if this was consistent across all programming languages, but it’s not.\nR uses 1-based indexing whereas Python uses zero-based indexing. What does this mean? Compare the following:\n\nplants &lt;- c(\"tree\", \"shrub\", \"grass\") # the index of \"tree\" is 1, \"shrub\" is 2 etc.\n\n\nplants = [\"tree\", \"shrub\", \"grass\"]   # the index of \"tree\" is 0, \"shrub\" is 1 etc.  \n\nBehind the scenes of any programming language there is a lot of counting going on. So, it matters if you count starting at zero or one. So, if I’d ask:\n“Hey, R - give me the items with index 1 and 2 in plants” then I’d get tree and shrub.\nIf I’d ask that question in Python, then I’d get shrub and grass. Fun times.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#missing-data",
    "href": "materials/da1-02-basic-objects-and-data-types.html#missing-data",
    "title": "5  Data types & structures",
    "section": "5.8 Dealing with missing data",
    "text": "5.8 Dealing with missing data\nIt may seem weird that you have to consider what isn’t there, but that’s exactly what we do when we have missing data. Ideally, when we’re collecting data we entries for every single thing we measure. But, alas, life is messy. That one patient may have missed an appointment, or one eppendorf tube got dropped, or etc etc.\n\nRPython\n\n\nR includes the concept of missing data, meaning we can specify that a data point is missing. Missing data are represented as NA.\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. This makes it harder to overlook the cases where you are dealing with missing data. This is a good thing!\nFor example, let’s look at the following data, where we have measured six different patients and recorded their systolic blood pressure.\n\nsystolic_pressure &lt;- c(125, 134, NA, 145, NA, 141)\n\nWe can see that we’re missing measurements for two of them. If we want to calculate the average systolic blood pressure across these patients, then we could use the mean() function. However, this results in NA.\n\nmean(systolic_pressure)\n\n[1] NA\n\n\nThe reason that happens is because missing values are obviously not numbers and, as such, the mean() function doesn’t know what to do with the.\nTo overcome this, we need to tell it to ignore missing values and then calculate the mean. We do this by adding the argument na.rm = TRUE to it. This argument works on many different functions and instructs it to remove missing values before any calculation takes place.\n\nmean(systolic_pressure, na.rm = TRUE)\n\n[1] 136.25\n\n\nThere are quite a few ways that you can deal with missing data and we’ll discuss more of them in later sessions.\nWe can also count the number of missing values we have, by using the is.na() function, together with sum(). Look at the following code:\n\nis.na(systolic_pressure)\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n\nFor each value in systolic_pressure we get a TRUE or FALSE value. If the value is NA, it returns TRUE. If not missing, FALSE. Behind the scenes R sees TRUE as a value of 1 and FALSE as a value of 0. We can thus count the number of TRUE values with:\n\nsum(is.na(systolic_pressure))\n\n[1] 2\n\n\n\n\nThe built-in functionality of Python is not very good at dealing with missing data. This means that you normally need to deal with them manually.\nOne of the ways you can denote missing data in Python is with None or NaN (“Not A Number”). Let’s look at the following data, where we have measured six different patients and recorded their systolic blood pressure.\n\nsystolic_pressure = [125, 134, None, 145, None, 141]\n\nNext, we’d have to filter out the missing values (don’t worry about the exact meaning of the code at this point):\n\nfiltered_data = [x for x in systolic_pressure if x is not None]\n\nAnd lastly we would be able to calculate the mean value:\n\nsum(filtered_data) / len(filtered_data)\n\n136.25\n\n\nThere are quite a few (easier!) ways that you can deal with missing data and we’ll discuss more of them in later sessions, once we start dealing with tabular data.\n\n\n\n\n\n\n\n\n\nNoteTo exclude or not exclude?\n\n\n\nIt may be tempting to simply remove all observations that contain missing data. It often makes the analysis easier! However, there is good reason to be more subtle: throwing away good data.\nLet’s look at the following hypothetical data set, where we use NA to denote missing values. We are interested in the average weight and age across the patients.\npatient_id    weight_kg   age\nN982          72          47\nN821          68          49\nN082          NA          63\nN651          78          NA\nWe could remove all the rows that contain any missing data, thereby getting rid of the last two observations. However, that would mean we’d lose data on age from the penultimate row, and data on weight_kg from the last row.\nInstead, it would be better to tell the computer to ignore missing values on a variable-by-variable basis and calculate the averages on the data that is there.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#exercises",
    "href": "materials/da1-02-basic-objects-and-data-types.html#exercises",
    "title": "5  Data types & structures",
    "section": "5.9 Exercises",
    "text": "5.9 Exercises\n\n5.9.1 Creating objects\n\n\n\n\n\n\nExerciseExercise 1 - Creating objects\n\n\n\n\n\n\nLevel: \nCreate an object that contains a sequence of even numbers between 1 and 21.\nUsing code, how many numbers are in the sequence?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can do this as follows:\n\nRPython\n\n\n\nnum_seq &lt;- seq(2, 21, by = 2)\n\nDetermine the length:\n\nlength(num_seq)\n\n[1] 10\n\n\n\n\n\nnum_seq = list(range(2, 21, 2))\n\nDetermine the length:\n\nlen(num_seq)\n\n10\n\n\n\n\n\nThere are 10 numbers in the sequence.\n\n\n\n\n\n\n\n\n\n\n5.9.2 Summation\n\n\n\n\n\n\nExerciseExercise 2 - Summation\n\n\n\n\n\n\nLevel: \nFor this exercise, create a series of uneven numbers of 10 to 30.\nUsing programming, answer the following: what is the sum of the resulting series of numbers?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\nnum_seq &lt;- seq(11, 29, by = 2)\n\n\nsum(num_seq)\n\n[1] 200\n\n\n\n\n\nnum_seq = list(range(11, 30, 2))\n\n\nsum(num_seq)\n\n200\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.9.3 Data types\n\n\n\n\n\n\nExerciseExercise 3 - Data types\n\n\n\n\n\n\nLevel: \nProgramming languages have a habit of coercing data types. What data types do you expect the following collections to be?\n\nRPython\n\n\n\nex1 &lt;- c(22, 87, NA, 32)\nex2 &lt;- c(22, 87, 96, \"unsure\")\nex3 &lt;- c(22, 87, 96.8, 102)\nex4 &lt;- c(89, \"rain\", 12, TRUE)\nex5 &lt;- c(TRUE, FALSE, TRUE, TRUE, \"1\", TRUE)\nex6 &lt;- c(TRUE, FALSE, TRUE, TRUE, 1, TRUE)\n\n\n\n\nex1 = np.array([22, 87, None, 32])\nex2 = np.array([22, 87, 96, \"unsure\"])\nex3 = np.array([22, 87, 96.8, 102])\nex4 = np.array([89, \"rain\", 12, True])\nex5 = np.array([True, False, True, True, \"1\", True])\nex6 = np.array([True, False, True, True, 1, True])\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\nclass(ex1)\n\n[1] \"numeric\"\n\nclass(ex2)\n\n[1] \"character\"\n\nclass(ex3)\n\n[1] \"numeric\"\n\nclass(ex4)\n\n[1] \"character\"\n\nclass(ex5)\n\n[1] \"character\"\n\nclass(ex6)\n\n[1] \"numeric\"\n\n\nThere are some perhaps unexpected data types in there, so let’s focus on it a bit more.\n\n\n\n\n\n\n\nOutput\nExplanation\n\n\n\n\n\"numeric\"\nAll numeric values + NA (which is allowed in numeric vectors).\n\n\n\"character\"\nMixing numbers with a string (\"unsure\") coerces everything to character.\n\n\n\"numeric\"\nIntegers and decimal values are both numeric in R (stored as doubles).\n\n\n\"character\"\nPresence of \"rain\" (a string) forces all elements to become character.\n\n\n\"character\"\nMixing logical (TRUE/FALSE) with string (\"1\") coerces all to character.\n\n\n\"numeric\"\nLogical values (TRUE/FALSE) are coerced to 1 and 0, making the vector numeric.\n\n\n\n\n\n\nex1.dtype\n\ndtype('O')\n\nex2.dtype\n\ndtype('&lt;U21')\n\nex3.dtype\n\ndtype('float64')\n\nex4.dtype\n\ndtype('&lt;U21')\n\nex5.dtype\n\ndtype('&lt;U5')\n\nex6.dtype\n\ndtype('int64')\n\n\nThere are some weird data types in there, so let’s unpack that a bit more.\n\n\n\n\n\n\n\n\ndtype\nMeaning\nExplanation\n\n\n\n\nO\nObject\nThe array holds generic Python objects (mixed types, e.g. int + None). No coercion possible.\n\n\n&lt;U21, &lt;U5\nUnicode string\nThe array holds text (U = Unicode). The number (21, 5) is the maximum string length in that array. NumPy converted everything to strings.\n\n\nfloat64\n64-bit floating-point numbers\nPurely numeric (can hold integers and floats together).\n\n\nint64\n64-bit integers\nPurely integer values (booleans are treated as 1 and 0).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da1-02-basic-objects-and-data-types.html#summary",
    "href": "materials/da1-02-basic-objects-and-data-types.html#summary",
    "title": "5  Data types & structures",
    "section": "5.10 Summary",
    "text": "5.10 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nThe most common data types include numerical, text and logical data.\nWe can store data in single objects, enabling us to use the data\nMultiple data points and types can be stored as different collections of data\nWe can make changes to objects and collections of data\nWe need to be explicit about missing data",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA1: Getting started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data types & structures</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html",
    "href": "materials/da2-03-tabular-data.html",
    "title": "6  Working with tabular data",
    "section": "",
    "text": "6.1 Context\nIn the previous section we dealt with single objects and vectors/lists. Here we expand towards tabular data, which can be seen as a set of these grouped together.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#tabular-data",
    "href": "materials/da2-03-tabular-data.html#tabular-data",
    "title": "6  Working with tabular data",
    "section": "6.2 Tabular data",
    "text": "6.2 Tabular data\nTables are organised in columns (vertical) and rows (horizontal). An example of a tabular data set is given in Figure 6.1.\nThere, each column contains a variable (a thing that we’ve measured). Each row is a unique observation.\n\n\n\n\n\n\nFigure 6.1: An example of tabular data",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#working-with-data",
    "href": "materials/da2-03-tabular-data.html#working-with-data",
    "title": "6  Working with tabular data",
    "section": "6.3 Working with data",
    "text": "6.3 Working with data\nTabular data sets are often created in spreadsheet programmes, such as Excel. These programmes are actually very well-suited, since they make it easy to enter data and keep an overview. When it comes to analysing these data, we’re better off using coding - that way we can keep track of our analysis.\nThe default Excel format is not great, since it’s a propriety format and not natively readable by other computer programmes. Good alternatives are .csv (comma-separated values) files or .tsv (tab-separated values) files.\nFor the next few sections we’ll be using the surveys.csv data set (which you should now have in your data-analysis/data sub folder).\nWe’ll read in these data now. Some of the functions we use are not available natively, so we need to load some additional packages.\n\nRPython\n\n\nIf you haven’t done so already, make sure to load the tidyverse package:\n\nlibrary(tidyverse)\n\nNext, we read in the file using the read_csv() function from tidyverse. Note that there is also a read.csv() function, but that one behaves slightly differently, so ensure you use the correct function name!\n\nread_csv(\"data/surveys.csv\")\n\nRows: 35549 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): species_id, sex\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 35,549 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         6     7    16  1977       1 PF         M                  14     NA\n 7         7     7    16  1977       2 PE         F                  NA     NA\n 8         8     7    16  1977       1 DM         M                  37     NA\n 9         9     7    16  1977       1 DM         F                  34     NA\n10        10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n\n\n\n\nWe’ll be using the read_csv() function from pandas, so make sure to load this module first.\n\nimport pandas as pd\n\nFrom hereon, when using tabular data, we’ll be using the pandas package. We will cover its idiosyncrasies as and when we encounter them, but there are two general concepts to be aware of at this stage.\n\n\n\n\n\n\nImportantViews versus copies\n\n\n\nWhen you select parts of a table in pandas you don’t get a completely new object, but you get a view of the original data. This is great when you’re just inspecting or viewing the data.\nHowever, if you’re making changes to the object that you’ve stored the table in (in our case surveys) then you might want to save these changes in a new object. If you want to do this, you create a copy of the data using the .copy() method.\nWhich one you use is dependent on what you’re trying to achieve. Don’t worry - we’ll be explicit when we need to make copies!\n\n\n\n\n\n\n\n\nImportantMethods versus attributes\n\n\n\nWhen interacting with pandas tables (called DataFrames) we often use methods and attributes. The difference between the two can appear a bit subtle, so we illustrate the difference below.\n\nMethods are functions that belong to an object. They are always used with parentheses, because they do something (e.g. surveys.head() gives the first few rows of the surveys object).\nAttributes are properties of an object and are accessed without parentheses. They describe the object (e.g. surveys.shape returns the dimensions of surveys).\n\n\n\nRight, with that out of the way we can read in the data:\n\npd.read_csv(\"data/surveys.csv\")\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n0              1      7   16  1977  ...          NL    M            32.0     NaN\n1              2      7   16  1977  ...          NL    M            33.0     NaN\n2              3      7   16  1977  ...          DM    F            37.0     NaN\n3              4      7   16  1977  ...          DM    M            36.0     NaN\n4              5      7   16  1977  ...          DM    M            35.0     NaN\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n35544      35545     12   31  2002  ...          AH  NaN             NaN     NaN\n35545      35546     12   31  2002  ...          AH  NaN             NaN     NaN\n35546      35547     12   31  2002  ...          RM    F            15.0    14.0\n35547      35548     12   31  2002  ...          DO    M            36.0    51.0\n35548      35549     12   31  2002  ...         NaN  NaN             NaN     NaN\n\n[35549 rows x 9 columns]\n\n\n\n\n\nThis actually spits out quite a bit of information onto the screen! This is because we’ve not assigned the output of reading in the file to an object. As such, we can’t work with the data yet. So, we’ll have to fix this. I’ve done this on purpose, of course, to show that the command itself works.\nIt is always good practice to run commands like this without assigning things. That way you can double-check what gets stored into an object! We’ll save the data into an object called surveys.\n\nRPython\n\n\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\n\n\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\n\n\n\n\n\n\n\n\n\n\nNoteReading in different types of data\n\n\n\nThere are many different functions available to read in various data formats. Below are some of the most common ones.\n\nRPython\n\n\nThe readr package (part of tidyverse) has several functions to read data in different formats.\n\nread_csv() - for comma separated values\nread_tsv() - for tab separated values\nread_csv2() - for CSV files exported from non-English spreadsheet programs that use the semi-colon ; as a separator and a comma , as the decimal place.\nread_table() - to read data where each column is separated by one or more spaces.\nread_delim() - a flexible function that allows you to define your own delimiter.\n\nThese functions have equivalents in base R (the default installation of R), which you can also use. They are very similarly named, for example: read.csv() and read.table() (notice the . instead of _ in the function name). However, they have different default options, so pay attention to which one you use!\n\n\nPython’s pd.read_csv() function from pandas can read in many different types of (tabular) data. The way it recognises the different formats is by specifying the separator:\n\npd.read_csv() - for comma separated values\npd.read_csv(file.tsv, sep = \"\\t\") - for tab separated values\npd.read_csv(file.csv, sep = \";\") - for CSV files exported from non-English spreadsheet programs that use the semi-colon ; as a separator and a comma , as the decimal place.\npd.read_table(file.txt) - for general delimited text files and equivalent to pd.read_csv() with a default delimiter of \\t (tab)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#table-structure",
    "href": "materials/da2-03-tabular-data.html#table-structure",
    "title": "6  Working with tabular data",
    "section": "6.4 Table structure",
    "text": "6.4 Table structure\nNow that we’ve read in the surveys data set, we can start exploring it a bit more. It’s quite a substantial data set, with 9 columns and 35549 rows.\n\n6.4.1 Getting the first few rows\nA good starting point is to get a snippet of the data. We can use the head() function to get the first few rows of the table.\n\nRPython\n\n\n\nhead(surveys)\n\n# A tibble: 6 × 9\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2         2     7    16  1977       3 NL         M                  33     NA\n3         3     7    16  1977       2 DM         F                  37     NA\n4         4     7    16  1977       7 DM         M                  36     NA\n5         5     7    16  1977       3 DM         M                  35     NA\n6         6     7    16  1977       1 PF         M                  14     NA\n\n\n\n\n\nsurveys.head()\n\n   record_id  month  day  year  plot_id species_id sex  hindfoot_length  weight\n0          1      7   16  1977        2         NL   M             32.0     NaN\n1          2      7   16  1977        3         NL   M             33.0     NaN\n2          3      7   16  1977        2         DM   F             37.0     NaN\n3          4      7   16  1977        7         DM   M             36.0     NaN\n4          5      7   16  1977        3         DM   M             35.0     NaN\n\n\n\n\n\n\n\n6.4.2 Understanding overall structure\nIt’s also useful to have a bit of an overview of the overall structure of the table. This may seems trivial with smaller data sets, but the bigger the data set, the harder this can become!\n\nRPython\n\n\nIf we are just interested in finding out which columns we have, we can use:\n\ncolnames(surveys)\n\n[1] \"record_id\"       \"month\"           \"day\"             \"year\"           \n[5] \"plot_id\"         \"species_id\"      \"sex\"             \"hindfoot_length\"\n[9] \"weight\"         \n\n\nHowever, sometimes we want more detailed information. We can do this as follows:\n\nstr(surveys)\n\nspc_tbl_ [35,549 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:35549] 1 2 3 4 5 6 7 8 9 10 ...\n $ month          : num [1:35549] 7 7 7 7 7 7 7 7 7 7 ...\n $ day            : num [1:35549] 16 16 16 16 16 16 16 16 16 16 ...\n $ year           : num [1:35549] 1977 1977 1977 1977 1977 ...\n $ plot_id        : num [1:35549] 2 3 2 7 3 1 2 1 1 6 ...\n $ species_id     : chr [1:35549] \"NL\" \"NL\" \"DM\" \"DM\" ...\n $ sex            : chr [1:35549] \"M\" \"M\" \"F\" \"M\" ...\n $ hindfoot_length: num [1:35549] 32 33 37 36 35 14 NA 37 34 20 ...\n $ weight         : num [1:35549] NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\nIf we are just interested in finding out which columns we have, we can use:\n\nsurveys.columns\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')\n\n\nHowever, sometimes we want more detailed information. We can do this as follows:\n\nsurveys.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35549 entries, 0 to 35548\nData columns (total 9 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   record_id        35549 non-null  int64  \n 1   month            35549 non-null  int64  \n 2   day              35549 non-null  int64  \n 3   year             35549 non-null  int64  \n 4   plot_id          35549 non-null  int64  \n 5   species_id       34786 non-null  object \n 6   sex              33038 non-null  object \n 7   hindfoot_length  31438 non-null  float64\n 8   weight           32283 non-null  float64\ndtypes: float64(2), int64(5), object(2)\nmemory usage: 2.4+ MB\n\n\n\n\n\nThis gives quite a bit of information, but overall it’s quite straightforward: we can see the number of rows and column and we have information on the type of data that is contained in each column.\n\n\n6.4.3 Summary values\nLastly, we can get some more information by creating some summary statistics.\nThis can be quite useful to quickly check if there are any strange values in your data. For example, you might have expectations on what is a plausible weight value, so if there are typos or errors (e.g. weight = 0), they will quickly show up.\n\nRPython\n\n\n\nsummary(surveys)\n\n   record_id         month             day             year         plot_id    \n Min.   :    1   Min.   : 1.000   Min.   : 1.00   Min.   :1977   Min.   : 1.0  \n 1st Qu.: 8888   1st Qu.: 4.000   1st Qu.: 9.00   1st Qu.:1984   1st Qu.: 5.0  \n Median :17775   Median : 6.000   Median :16.00   Median :1990   Median :11.0  \n Mean   :17775   Mean   : 6.478   Mean   :15.99   Mean   :1990   Mean   :11.4  \n 3rd Qu.:26662   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:1997   3rd Qu.:17.0  \n Max.   :35549   Max.   :12.000   Max.   :31.00   Max.   :2002   Max.   :24.0  \n                                                                               \n  species_id            sex            hindfoot_length     weight      \n Length:35549       Length:35549       Min.   : 2.00   Min.   :  4.00  \n Class :character   Class :character   1st Qu.:21.00   1st Qu.: 20.00  \n Mode  :character   Mode  :character   Median :32.00   Median : 37.00  \n                                       Mean   :29.29   Mean   : 42.67  \n                                       3rd Qu.:36.00   3rd Qu.: 48.00  \n                                       Max.   :70.00   Max.   :280.00  \n                                       NA's   :4111    NA's   :3266    \n\n\n\n\n\nsurveys.describe()\n\n          record_id         month  ...  hindfoot_length        weight\ncount  35549.000000  35549.000000  ...     31438.000000  32283.000000\nmean   17775.000000      6.477847  ...        29.287932     42.672428\nstd    10262.256696      3.396925  ...         9.564759     36.631259\nmin        1.000000      1.000000  ...         2.000000      4.000000\n25%     8888.000000      4.000000  ...        21.000000     20.000000\n50%    17775.000000      6.000000  ...        32.000000     37.000000\n75%    26662.000000     10.000000  ...        36.000000     48.000000\nmax    35549.000000     12.000000  ...        70.000000    280.000000\n\n[8 rows x 7 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#basic-subsetting-of-data",
    "href": "materials/da2-03-tabular-data.html#basic-subsetting-of-data",
    "title": "6  Working with tabular data",
    "section": "6.5 Basic subsetting of data",
    "text": "6.5 Basic subsetting of data\nAlthough we’ll go into more detail on how to select portions of a larger data set, we’ll briefly cover some very basic subsetting techniques here - just to get us going.\n\n6.5.1 Selecting columns\nWe saw that there are 9 different columns in our data set. One of them is weight, which holds weight measurements for different animal records.\n\nRPython\n\n\nWe can easily select an individual column using the $ symbol. We type the name of the object and then specify which column we’re interested in:\n\nsurveys$weight\n\n\n\nWe can easily select an individual column in a pandas DataFrame using the . notation. We type the name of the object and then specify which column we’re interested in:\n\nsurveys.weight\n\n0         NaN\n1         NaN\n2         NaN\n3         NaN\n4         NaN\n         ... \n35544     NaN\n35545     NaN\n35546    14.0\n35547    51.0\n35548     NaN\nName: weight, Length: 35549, dtype: float64\n\n\n\n\n\n\n\n6.5.2 Subsetting rows and columns\nWe can subset specific rows and columns using the square bracket [ ] notation. The way this is ordered is [rows, columns].\nWe can divide the way we extract the data into two methods: based on their label/name in the table (label-based subsetting) or based on their numerical index (index-based subsetting).\n\nRPython\n\n\nHere, we are asking to return all the rows for the species_id column (its label). Note the comma before the \"species_id\" notation. The blank before the comma indicates that we’re selecting all the rows.\n\nsurveys[ , \"species_id\"]\n\n# A tibble: 35,549 × 1\n   species_id\n   &lt;chr&gt;     \n 1 NL        \n 2 NL        \n 3 DM        \n 4 DM        \n 5 DM        \n 6 PF        \n 7 PE        \n 8 DM        \n 9 DM        \n10 PF        \n# ℹ 35,539 more rows\n\n\nWe can also select a subset of rows for this column, for example the first 3 rows:\n\nsurveys[1:3, \"species_id\"]\n\n# A tibble: 3 × 1\n  species_id\n  &lt;chr&gt;     \n1 NL        \n2 NL        \n3 DM        \n\n\n\n\nHere, we are asking to return all the rows for the species_id column. We use .loc to use label-based indexing. Note the : , before the \"species_id\" notation. This tells Python to get all the rows.\n\nsurveys.loc[: , \"species_id\"]\n\n0         NL\n1         NL\n2         DM\n3         DM\n4         DM\n        ... \n35544     AH\n35545     AH\n35546     RM\n35547     DO\n35548    NaN\nName: species_id, Length: 35549, dtype: object\n\n\nWe can also select a subset of rows for this column. The .loc attribute uses label-based indexing, so it looks at the row and column names. When we’re slicing rows with .loc the slices are inclusive of the stop. For example the first 3 rows:\n\nsurveys.loc[0:2, \"species_id\"]\n\n0    NL\n1    NL\n2    DM\nName: species_id, dtype: object\n\n\n\n\n\n\nRPython\n\n\nAlternatively, we use index-based subsetting. Remember, this is based on the numerical position in the data set.\nLet’s grab the first 3 rows, and columns 2 to 4:\n\nsurveys[1:3, 2:4]\n\n# A tibble: 3 × 3\n  month   day  year\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     7    16  1977\n2     7    16  1977\n3     7    16  1977\n\n\nor rows 10 to 15, and columns 2, 6 and 8:\n\nsurveys[10:15, c(2, 6, 8)]\n\n# A tibble: 6 × 3\n  month species_id hindfoot_length\n  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;\n1     7 PF                      20\n2     7 DS                      53\n3     7 DM                      38\n4     7 DM                      35\n5     7 DM                      NA\n6     7 DM                      36\n\n\n\n\nAlternatively, we use index-based subsetting with .iloc. Remember, this is based on the numerical position in the data set. Importantly, slicing with .iloc is exclusive of the stop. Yes, we know - it’s terribly confusing that this is different from .loc!\nLet’s grab the first 3 rows, and columns 2 to 4:\n\nsurveys.iloc[0:3, 1:4]\n\n   month  day  year\n0      7   16  1977\n1      7   16  1977\n2      7   16  1977\n\n\nor rows 10 to 15, and columns 2, 6 and 8:\n\nsurveys.iloc[9:15, [1, 5, 7]]\n\n    month species_id  hindfoot_length\n9       7         PF             20.0\n10      7         DS             53.0\n11      7         DM             38.0\n12      7         DM             35.0\n13      7         DM              NaN\n14      7         DM             36.0\n\n\nRemember, Python’s indexing is zero-based - so you have to be quite careful/accurate when you’re after specific indexes!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#saving",
    "href": "materials/da2-03-tabular-data.html#saving",
    "title": "6  Working with tabular data",
    "section": "6.6 Saving",
    "text": "6.6 Saving\nBefore we move on to the next section, we’ll practice saving data to file. You might want to do this if you created new tables or subsetted your data and don’t want to repeat that every single time you do your analysis.\n\n\n\n\n\n\nWarning\n\n\n\nRemember: never overwrite your raw data but always keep this as a separate copy!\n\n\n\nRPython\n\n\nLet’s create a practice data set to save, for example by taking the first 20 rows of our surveys data set and saving it in a file surveys_snippet.csv.\n\nsurveys_snippet &lt;- surveys[1:20, ]\n\nWe can now save this. We do this with the write_csv() function. We need to tell it which data set to save (this comes first) and then tell it with the file = argument where we want to save it. Here, we’re saving it in our data/ folder, as a file called surveys_snippet.csv. The file extension .csv is important, so don’t forget to add it!\n\nwrite_csv(surveys_snippet, file = \"data/surveys_snippet.csv\")\n\n\n\nLet’s create a practice data set to save, for example by taking the first 20 rows of our surveys data set and saving it in a file surveys_snippet.csv.\n\nsurveys_snippet = surveys.iloc[0:20, :]\n\nWe can now save this. We do this using .to_csv.\nWe need to tell it which data set to save (this comes first) and then tell it where we want to save it. Here, we’re saving it in our data/ folder, as a file called surveys_snippet.csv. The file extension .csv is important, so don’t forget to add it!\nWe also include the index = False argument, so that the row numbers are not written to file.\n\nsurveys_snippet.to_csv(\"data/surveys_snippet.csv\", index = False)\n\n\n\n\n\n\n\n\n\n\nTipData tip: quality control checks\n\n\n\nWhenever you read in your data, it’s always good practice to do some quality checks. Here’s a list of things to look out for:\n\nDo you have the expected number of rows and columns?\nAre your variables (columns) of the expected type? (e.g. numeric, character)\nIs the range of numeric data within expected boundaries? For example: a column with months should go from 1 to 12; a column with human heights in cm should not have values below 30 or so; etc…\nDo you have the expected number of unique values in categorical (character) variables?\nDo you have missing values in the data, and were these imported correctly?\n\nThe table below gives an overview of common functions, where df denotes a table with data, col is a column within that table and x denotes a simple collection of data.\n\n\n\n\n\n\n\n\nTask\nR function\nPython (pandas)\n\n\n\n\nStructure of object\nstr(df)\ndf.info()\n\n\nSummary statistics\nsummary(df)\ndf.describe()\n\n\nNumber of rows\nnrow(df)\ndf.shape[0]\n\n\nNumber of columns\nncol(df)\ndf.shape[1]\n\n\nNumber of elements in a vector\nlength(x)\nlen(x) or x.size (for arrays)\n\n\nUnique values in a column\nunique(df$col)\ndf[\"col\"].unique()\n\n\nCount unique values\nlength(unique(x))\ndf[\"col\"].nunique()\n\n\nMissing values (logical)\nis.na(x)\ndf[\"col\"].isna()\n\n\nCount missing values\nsum(is.na(x))\ndf[\"col\"].isna().sum()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#exercises",
    "href": "materials/da2-03-tabular-data.html#exercises",
    "title": "6  Working with tabular data",
    "section": "6.7 Exercises",
    "text": "6.7 Exercises\n\n6.7.1 Data structure: surveys\n\n\n\n\n\n\nExerciseExercise 1 - Data structure\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/surveys.csv. Read in the data and answer the following:\n\nHow many rows are in surveys?\nHow many columns are in surveys?\nHow many unique values do we have in the species_id column?\nHow many missing values are there in weight and hindfoot_length?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nFirst we load the data.\n\nRPython\n\n\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\nRows: 35549 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): species_id, sex\ndbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\n\n\n\n\nWe can then answer the following questions:\n\n1. How many rows are in surveys?\n\nRPython\n\n\n\nnrow(surveys)\n\n[1] 35549\n\n\n\n\n\nsurveys.shape[0]\n\n35549\n\n\n\n\n\n\n\n2. How many columns are in surveys?\n\nRPython\n\n\n\nncol(surveys)\n\n[1] 9\n\n\n\n\n\nsurveys.shape[1]\n\n9\n\n\n\n\n\n\n\n3. How many unique values do we have in the species_id column?\n\nRPython\n\n\n\nunique(surveys$species_id)\n\n [1] \"NL\" \"DM\" \"PF\" \"PE\" \"DS\" \"PP\" \"SH\" \"OT\" \"DO\" \"OX\" \"SS\" \"OL\" \"RM\" NA   \"SA\"\n[16] \"PM\" \"AH\" \"DX\" \"AB\" \"CB\" \"CM\" \"CQ\" \"RF\" \"PC\" \"PG\" \"PH\" \"PU\" \"CV\" \"UR\" \"UP\"\n[31] \"ZL\" \"UL\" \"CS\" \"SC\" \"BA\" \"SF\" \"RO\" \"AS\" \"SO\" \"PI\" \"ST\" \"CU\" \"SU\" \"RX\" \"PB\"\n[46] \"PL\" \"PX\" \"CT\" \"US\"\n\n\nThis gives us the unique values, but doesn’t count them. To do this we use the length() function:\n\nlength(unique(surveys$species_id))\n\n[1] 49\n\n\nNote that one of the entries is NA - missing data. We can tell it’s different, because it’s not in quotes (\" \"). So, there are actually 48 unique values in species_id!\n\n\n\nsurveys[\"species_id\"].unique()\n\narray(['NL', 'DM', 'PF', 'PE', 'DS', 'PP', 'SH', 'OT', 'DO', 'OX', 'SS',\n       'OL', 'RM', nan, 'SA', 'PM', 'AH', 'DX', 'AB', 'CB', 'CM', 'CQ',\n       'RF', 'PC', 'PG', 'PH', 'PU', 'CV', 'UR', 'UP', 'ZL', 'UL', 'CS',\n       'SC', 'BA', 'SF', 'RO', 'AS', 'SO', 'PI', 'ST', 'CU', 'SU', 'RX',\n       'PB', 'PL', 'PX', 'CT', 'US'], dtype=object)\n\n\nThis gives us the unique values, but doesn’t count them. We can use the len() function to count the number of items in the array:\n\nlen(surveys[\"species_id\"].unique())\n\n49\n\n\nOr we can use the .nunique() function to do this directly on the column:\n\nsurveys[\"species_id\"].nunique()\n\n48\n\n\nThis gives a slightly different result! Why? Well, there are missing data in the species_id column, which get counted as an entry if we get the unique values. With the .nunique() function missing values are automatically ignored.\n\n\n\n\n\n4. How many missing values are there in weight and hindfoot_length?\n\nRPython\n\n\nWe can count the number of missing values, by asking how many times the value in the weight column is missing. The answer you get is either TRUE or FALSE and R counts TRUE as 1 and FALSE as 0. This is why you can use the sum() function to tally the number of TRUE values.\n\nsum(is.na(surveys$weight))\n\n[1] 3266\n\nsum(is.na(surveys$hindfoot_length))\n\n[1] 4111\n\n\n\n\n\nsurveys[\"weight\"].isna().sum()\n\nnp.int64(3266)\n\nsurveys[\"hindfoot_length\"].isna().sum()\n\nnp.int64(4111)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.2 Subsetting data: parasites\n\n\n\n\n\n\nExerciseExercise 2 - Subsetting data\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/parasites.csv. Please do the following:\n\nLoad the data.\nCheck the structure of the data. What column types do we have?\nSelect the lake column. How many lakes are there?\nSelect rows 12 to 29 of the fish_length column. Using code, what are the minimum and maximum values?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\n1. Load the data.\nFirst we load the data.\n\nRPython\n\n\n\nparasites &lt;- read_csv(\"data/parasites.csv\")\n\nRows: 64 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): lake\ndbl (2): parasite_count, fish_length\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nparasites = pd.read_csv(\"data/parasites.csv\")\n\n\n\n\n\n\n2. Check the structure of the data.\nNow we can check the structure of the data.\n\nRPython\n\n\n\nstr(parasites)\n\nspc_tbl_ [64 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ parasite_count: num [1:64] 46 138 23 35 118 43 35 19 101 32 ...\n $ lake          : chr [1:64] \"C\" \"C\" \"C\" \"B\" ...\n $ fish_length   : num [1:64] 21.1 26.4 18.9 27.2 29 24.2 31.2 20.3 27 27 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   parasite_count = col_double(),\n  ..   lake = col_character(),\n  ..   fish_length = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\nparasites.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 64 entries, 0 to 63\nData columns (total 3 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   parasite_count  64 non-null     int64  \n 1   lake            64 non-null     object \n 2   fish_length     64 non-null     float64\ndtypes: float64(1), int64(1), object(1)\nmemory usage: 1.6+ KB\n\n\n\n\n\nWe can see that we have 3 columns and 64 rows. The parasite_count column is numerical (an integer - whole numbers); the lake' column is text andfish_length` is numerical (continuous).\n\n\n3. Select the lake column.\nLet’s select the lake column.\n\nRPython\n\n\n\nparasites$lake\n\n [1] \"C\" \"C\" \"C\" \"B\" \"C\" \"B\" \"B\" \"B\" \"C\" \"A\" \"B\" \"B\" \"A\" \"B\" \"C\" \"A\" \"C\" \"C\" \"A\"\n[20] \"A\" \"A\" \"A\" \"C\" \"B\" \"C\" \"B\" \"A\" \"B\" \"C\" \"B\" \"A\" \"C\" \"C\" \"A\" \"C\" \"B\" \"A\" \"C\"\n[39] \"A\" \"A\" \"B\" \"C\" \"C\" \"A\" \"C\" \"A\" \"C\" \"B\" \"A\" \"B\" \"A\" \"A\" \"C\" \"A\" \"B\" \"A\" \"A\"\n[58] \"C\" \"A\" \"B\" \"A\" \"C\" \"A\" \"C\"\n\n\nWe can check how many unique values there are:\n\nlength(unique(parasites$lake))\n\n[1] 3\n\n\nWe read that as follows: “in the lake column in parasites (parasites$lake), find the unique values (unique()) and count how many items there are (length()).”\n\n\nWe can define the column explicitly, using the [\"col\"] notation.\n\nparasites[\"lake\"]\n\n0     C\n1     C\n2     C\n3     B\n4     C\n     ..\n59    B\n60    A\n61    C\n62    A\n63    C\nName: lake, Length: 64, dtype: object\n\n\nOr just use the . notation.\n\nparasites.lake\n\n0     C\n1     C\n2     C\n3     B\n4     C\n     ..\n59    B\n60    A\n61    C\n62    A\n63    C\nName: lake, Length: 64, dtype: object\n\n\nWe can then tally the number of unique values, using the .nunique() method:\n\nparasites[\"lake\"].nunique()\n\n3\n\n\n\n\n\n\n\n4. Select rows 12 to 29 of the fish_length column.\nWe need to do two things: just select the fish_length column and then specifically rows 12 to 29.\n\nRPython\n\n\nWe can use the [ ] notation to subset our data. This takes the order [rows, columns].\n\nparasites[12:29, \"fish_length\"]\n\n# A tibble: 18 × 1\n   fish_length\n         &lt;dbl&gt;\n 1        18.4\n 2        25.1\n 3        22.8\n 4        33.4\n 5        31.1\n 6        26.4\n 7        19.8\n 8        22.4\n 9        33.1\n10        19.6\n11        33.4\n12        23.8\n13        22.7\n14        21.1\n15        35.7\n16        18.3\n17        27.5\n18        31.2\n\n\nWe can find the minimum and maximum values with:\n\nmin(parasites[12:29, \"fish_length\"])\n\n[1] 18.3\n\n\n\nmax(parasites[12:29, \"fish_length\"])\n\n[1] 35.7\n\n\n\n\nWe can use the [ ] notation to subset our data. This takes the order [rows, columns].\nRemember that Python has zero-based indexing. Also remember that .loc is inclusive. So, if we want rows 12 to 29 (included), we need to select index [11:28]. We’re using .loc because we are using label-based indexing (by selecting the fish_length column using it’s name).\n\nparasites.loc[11:28, \"fish_length\"]\n\n11    18.4\n12    25.1\n13    22.8\n14    33.4\n15    31.1\n16    26.4\n17    19.8\n18    22.4\n19    33.1\n20    19.6\n21    33.4\n22    23.8\n23    22.7\n24    21.1\n25    35.7\n26    18.3\n27    27.5\n28    31.2\nName: fish_length, dtype: float64\n\n\nWe can find the minimum and maximum values with:\n\nparasites.loc[11:28, \"fish_length\"].min()\n\nnp.float64(18.3)\n\n\n\nparasites.loc[11:28, \"fish_length\"].max()\n\nnp.float64(35.7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.3 Random sampling: parasites\n\n\n\n\n\n\nExerciseExercise 3 - Random sampling\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/parasites.csv.\nUsing subsetting, select 10 random rows in the data - without manually selecting them!\nWhat happens if you repeatedly randomly sample?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can do this as follows.\n\nRPython\n\n\n\nset.seed(123)  # optional, for reproducibility\n\nsample_rows &lt;- sample(1:nrow(parasites), size = 10)  # pick 10 random row indices\nparasites_subset &lt;- parasites[sample_rows, ]\n\nWe use the sample() function, to create a vector of random row numbers - based on the length of the parasites data set. We then use this to subset our data.\nThis gives us the following subset of data:\n\nparasites_subset\n\n# A tibble: 10 × 3\n   parasite_count lake  fish_length\n            &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1             16 A            27.1\n 2            213 C            33.4\n 3             51 A            26.5\n 4             13 B            22.8\n 5             23 C            18.9\n 6            133 C            21.1\n 7             80 B            28.3\n 8             17 A            22.6\n 9             24 C            21.1\n10              7 A            17.9\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe are also using set.seed(123). What this does is “set” the random number generator that R uses. This allows us to recreate output and provide reproducibility in the course materials. If we would not set it, then every time we create sample_rows it would pick 10 different random ones.\n\n\n\n\nPandas has built-in random sampling, using the .sample() method.\nWe need to give it three pieces of information:\n\nThe data (parasites)\nThe number of samples to take (n =)\n(optional) setting the random seed (random_state =)\n\n\nparasites_subset = parasites.sample(n = 10, random_state = 123)\n\n\nparasites_subset.head()\n\n    parasite_count lake  fish_length\n51              43    A         25.4\n31             133    C         29.0\n63             229    C         33.9\n53              17    A         22.6\n23               9    B         22.7\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe are also using random_state = 123. What this does is “set” the random number generator that Python uses. This allows us to recreate output and provide reproducibility in the course materials. If we would not set it, then every time we create parasites_subset it would pick 10 different random rows.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-03-tabular-data.html#summary",
    "href": "materials/da2-03-tabular-data.html#summary",
    "title": "6  Working with tabular data",
    "section": "6.8 Summary",
    "text": "6.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nTabular data is structured into columns (variables) and rows (observations)\nCommon data types include CSVs and TSVs - widely accessible formats where data are separated by commas or tabs\nIt is good practice to get insight into your data by examining the structure, creating summary statistics and checking for missing values\nBasic subsetting allows us to quickly pull out certain variables or observations\nWrite modified tables to file, but always keep the original, raw, data intact!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with tabular data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html",
    "href": "materials/da2-04-plotting.html",
    "title": "7  Plotting data",
    "section": "",
    "text": "7.1 Context\nWe now have a good grasp of how data is commonly structured, with variables in columns and observations in rows. This is the perfect format for visualising data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#setup_plotting",
    "href": "materials/da2-04-plotting.html#setup_plotting",
    "title": "7  Plotting data",
    "section": "7.2 Section setup",
    "text": "7.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 02_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 02_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsurveys = pd.read_csv(\"data/surveys.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#introducing-plotting",
    "href": "materials/da2-04-plotting.html#introducing-plotting",
    "title": "7  Plotting data",
    "section": "7.3 Introducing plotting",
    "text": "7.3 Introducing plotting\nTo create a plot we’ll need three things:\n\n\n\n\n\n\n\n\n\n1. Data\nyour data\n\n\n\n2. Mapping aesthetics\nvariables used to create the visual (e.g. x/y data, colours)\n\n\n\n3. Specify the type of plot\ne.g. scatter plot, boxplot, line graph\n\n\n\nThis breakdown of plotting is often associated with R’s ggplot2 package, but the underlying principles of the gg (grammar of graphics) is a much more universal approach to creating graphs.\nThe idea is that you consistently build up plots, layer-by-layer. I like the concept, because it creates consistency in our approach - regardless of the language. There is a Python implementation of ggplot2, called plotnine.\nWe’ll be using these libraries/modules here, but will also show you some examples of other commonly-used plotting packages. You’ll probably develop your own preference - this is absolutely fine!\n\n7.3.1 Start plotting\nIf needed, add and run the code from Section setup.\nHere we are using the surveys data set. Let’s assume that we’re interested in the relationship between two variables: weight and hindfoot_length. We can plot weight on the x-axis and hindfoot_length on the y-axis.\nSince they are both continuous data, a scatter plot would be a good way to represent these data.\nSo, we need three things: (1) data; (2) mapping of aesthetics and (3) specify the type of plot.\n\nRPython\n\n\nWe use the ggplot() function to do this:\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 7.1: Scatterplot of weight vs hindfoot_length\n\n\n\n\n\n\n\nWe use the ggplot() function to do this. Note that the whole code chunk below is wrapped inside another set of parentheses ( ). This allows us to break up the code a bit for clarity. Also, the variable names that we’re giving to ggplot() are inside quotes \" \" - this is different from R, where this is not necessary.\nAdditionally, if you’re running this directly from a Python script, it might not always output the plot inline. To avoid issues with this, we assign the plot to an object p and specifically ask Python to display it using p.show().\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"weight\", y = \"hindfoot_length\")) + \n  geom_point())\n\np.show()\n\n\n\n\n\n\n\nFigure 7.2: Scatterplot of weight vs hindfoot_length\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.scatterplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\"\n)\n\nplt.title(\"Scatterplot of weight vs hindfoot_length\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.3: Scatterplot of weight vs hindfoot_length\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePandas equivalent\n\n\n\n\n\nFor simple plots you can also use the built-in plotting functionality from pandas. We’ll only show this once, because there are so many different ways to plot data that we don’t want to clutter the materials. However, it can be useful if you want to have a quick preview of your data. More information on this can be found in the official documentation.\n\nsurveys.plot(x = \"weight\", y = \"hindfoot_length\", kind = \"scatter\")\n\n\n\n\n\n\n\nFigure 7.4: Scatterplot of weight vs hindfoot_length\n\n\n\n\n\n\n\n\n\n\n\nLet’s unpack that a bit. We specify which data to use with the data = argument (our surveys data set in this case).\nNext, we define what goes onto the x and y axes, using the mapping = argument. This needs a so-called helper function aes(), which stands for aesthetics. Within this helper function we define what goes onto the x-axis (x =) and y-axis (y =).\nFinally, we need to tell it what kind of plot we want. Here, we want to use a scatter plot. The type of plot is determined by the geom_. This literally gets added to the ggplot() function: note the + symbol at the end of the line of code.\nMost geom_ functions are logically named. For example, a line graph will be geom_line(), a boxplot geom_boxplot() etc. The odd one out is the scatter plot, which is geom_point(), because we’re plotting individual data points.\nWe don’t have to add any information within the geom_point() function, because it’s taking all it needs from the ggplot() function above. More on this later.\n\n\n7.3.2 Building up plots\nThe good thing about ggplot() is that it builds up the plot layer-by-layer. We don’t even have to provide it with a geometry to start with and it’ll still create the outline of a plot. Let’s illustrate this with another example, where we plot hindfoot_length on the y-axis for each sex group.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length))\n\n\n\n\n\n\n\nFigure 7.5: Geometries are needed to visualise the data.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.6: Geometries are needed to visualise the data.\n\n\n\n\n\n\n\n\nHowever, that obviously is not very useful. The nice thing is that we can add multiple layers to a single plot. We have a column sex in the data. This contains three possible values:F (female), M (male) and NA (not recorded).\nLet’s look at the hindfoot length distribution across these groups.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 7.7: Scatterplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_point())\n\np.show()\n\n\n\n\n\n\n\nFigure 7.8: Scatterplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.stripplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    jitter = True    # to spread points horizontally like geom_point on a category\n)\n\nplt.title(\"Hindfoot length by sex\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.9: Scatterplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\n\n\n\nA lot of the points are overlapping, which makes it a bit hard to see how the data are distributed. We can do something about that (more on that in the next session), but we can also add some summary statistics in the form of a boxplot. We can simply add a layer to the plot that displays the boxes.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_point() +\n  geom_boxplot()\n\n\n\n\n\n\n\nFigure 7.10: Scatterplot and boxplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_point() +\n        geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\nFigure 7.11: Scatterplot and boxplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nimport sys, warnings\nprint(sys.version)\nprint(warnings.__file__)\nhelp(warnings.warn)\n# boxplot first\nsns.boxplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    showfliers = False  # hide outliers, since we’ll show raw points\n)\n\n# points overlaid\nsns.stripplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    color = \"black\",  # colour points for contrast\n    jitter = True,    # add jitter to avoid overlap\n    alpha = 0.6       # use transparency for clarity\n)\n\nplt.title(\"Hindfoot length by sex (points + boxplot)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.12: Scatterplot and boxplot of hindfoot length for each sex.\n\n\n\n\n\n\n\n\n\n\n\nThe layers are added in the order we provide them, so here the boxes are on top of the individual data points. You might want to rearrange that, so that the boxes are behind the data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#changing-plots",
    "href": "materials/da2-04-plotting.html#changing-plots",
    "title": "7  Plotting data",
    "section": "7.4 Changing plots",
    "text": "7.4 Changing plots\nOften we want to control other parts of the plot as well. There is a whole range of things we can change about the appearance of a plot - in fact, anything in a plot can be changed! Don’t try to remember every tiny detail. You might want to change the orientation of the text labels on the x-axis, but a quick search is probably easier than keeping that information in your head!\n\n7.4.1 Colour\nChanging colour is pretty straightforward. We use the colour = argument. There are a whole range of default colours available, but we’ll go with blue here.\nLet’s illustrate that using our original weight vs hindfoot_length scatter plot.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(colour = \"blue\")\n\n\n\n\n\n\n\nFigure 7.13: Colouring points by a defined colour.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"weight\", y = \"hindfoot_length\")) +\n        geom_point(colour = \"blue\"))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.14: Colouring points by a defined colour.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.scatterplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    color = \"blue\"   # fixed color for all points\n)\n\nplt.title(\"Scatterplot of weight vs hindfoot_length (blue points)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.15: Colouring points by a defined colour.\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 Fill\nThe fill = argument is used to fill surface areas. It doesn’t work on individual points, but instead on geometries that have an area, such as a boxplot, bar chart or violin plot.\nWe can’t create a boxplot with two continuous variables, so we’ll plot hindfoot_length for the different sex groups again. We fill the boxes using magenta.\nWhat happens if you use colour = \"magenta\" instead?\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_boxplot(fill = \"magenta\")\n\n\n\n\n\n\n\nFigure 7.16: Geometries with surfaces use fill for colours.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_boxplot(fill = \"magenta\"))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.17: Geometries with surfaces use fill for colours.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.boxplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    color = \"magenta\"   # sets the fill color of the box\n)\n\nplt.title(\"Hindfoot length by sex (magenta boxplot)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.18: Seaborn uses color for fill colours.\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.3 Aesthetics based on data\nManually assigning colours can be very helpful, but quite often we want to colour data based on another variable. For example, we might be interested in the potential relationship between weight and hindfoot length, but are wondering if this looks different across the sex groups.\nIn that case, we’d want to colour all the data points belonging to the male group different to those of the female group. The same goes for the missing values.\nThe way we can do this is by adding the sex variable inside the aesthetics.\n\n\n\n\n\n\nNoteWithin aes() or not?\n\n\n\nAn easy way of remembering where your colour = or fill = argument goes is to ask: is the colour based on the data or not? If the answer is yes, it goes inside the aesthetics. If not, then outside.\n\n\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length, colour = sex)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 7.19: Colouring data based on another variable.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"weight\", y = \"hindfoot_length\", colour = \"sex\")) +\n        geom_point())\n\np.show()\n\n\n\n\n\n\n\nFigure 7.20: Colouring data based on another variable.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.scatterplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    hue = \"sex\"   # map colors to sex\n)\n\nplt.title(\"Scatterplot of weight vs hindfoot_length by sex\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.21: Colouring data based on another variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.4 Dealing with overlap\nIn the example of hindfoot length for the different sex groups we noticed that there is quite a bit of overlap in the data. One of the ways of dealing with this is by adding a little bit of jitter. What that does is add a tiny bit of random noise to the data, to avoid overlap.\nWe can do this with the geom_jitter() geometry. The amount of jitter that is added can be regulated with the width = argument, as a fraction of the available width. Compare the differences in the following plots.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_jitter()\n\n\n\n\n\n\n\nFigure 7.22: Adding jitter to the data can help with overlapping data.\n\n\n\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_jitter(width = 0.1)\n\n\n\n\n\n\n\nFigure 7.23: You can control the amount of jitter.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_jitter())\n\np.show()\n\n\n\n\n\n\n\nFigure 7.24: Adding jitter to the data can help with overlapping data.\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_jitter(width = 0.1))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.25: You can control the amount of jitter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.stripplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    jitter = True,    # add horizontal jitter\n    alpha = 0.6       # add transparency\n)\n\nplt.title(\"Hindfoot length by sex (jittered points)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.26: Adding jitter to the data can help with overlapping data.\n\n\n\n\n\n\nsns.stripplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    jitter = 0.05,   # controls the amount of horizontal jitter\n    alpha = 0.6     # add transparency\n)\n\nplt.title(\"Hindfoot length by sex (jitter = 0.05)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.27: You can control the amount of jitter.\n\n\n\n\n\n\n\n\n\n\n7.4.5 Transparency\nEven with jittering the data, we still have quite some overlap. There probably is a limit to what we can do about it, but adding some transparency can also help. Here, where there is more overlap, areas will appear darker whereas less overlap will appear lighter.\nWe control this with the alpha = argument. Again, this takes a value between 0 (full transparency) and 1 (no transparency).\nCompare the following plot with the previous ones.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = sex, y = hindfoot_length)) +\n  geom_jitter(width = 0.1, alpha = 0.4)\n\n\n\n\n\n\n\nFigure 7.28: You can also use transparency to deal with overlapping data.\n\n\n\n\n\n\n\n\np = (ggplot(data = surveys,\n        mapping = aes(x = \"sex\", y = \"hindfoot_length\")) +\n        geom_jitter(width = 0.1, alpha = 0.4))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.29: You can also use transparency to deal with overlapping data.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.stripplot(\n    data = surveys,\n    x = \"sex\",\n    y = \"hindfoot_length\",\n    jitter = 0.1,     # horizontal jitter\n    alpha = 0.4       # transparency of 40%\n)\n\nplt.title(\"Hindfoot length by sex (jitter=0.1, alpha=0.4)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.30: You can also use transparency to deal with overlapping data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.6 Point size and line width\nThe size = argument is used to control the size of points, whereas the linewidth = argument is used to specify line thickness. Look at the following examples.\nIn the next two panels we’re using geom_point() with different sizes.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(size = 1) +\n  labs(title = \"geom_point(size = 1)\")\n\n\n\n\n\n\n\nFigure 7.31: Data point sizes can be adjusted.\n\n\n\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point(size = 3) +\n  labs(title = \"geom_point(size = 3)\")\n\n\n\n\n\n\n\nFigure 7.32: Data point sizes can be adjusted.\n\n\n\n\n\n\n\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n      geom_point(size = 1) +\n      labs(title = \"geom_point(size = 1)\"))\n      \np.show()\n\n\n\n\n\n\n\nFigure 7.33: Data point sizes can be adjusted.\n\n\n\n\n\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n      geom_point(size = 3) +\n      labs(title = \"geom_point(size = 3)\"))\n      \np.show()\n\n\n\n\n\n\n\nFigure 7.34: Data point sizes can be adjusted.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 5), sharey = True)\n\n# p1: small points\nsns.scatterplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    s = 20,             # size of points (similar to size=1 in ggplot)\n    ax = axes[0]\n)\naxes[0].set_title(\"point size 20\")\n\n# p2: larger points\nsns.scatterplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    s = 70,             # larger points (similar to size=3 in ggplot)\n    ax = axes[1]\n)\naxes[1].set_title(\"point size 70\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 7.35: Data point sizes can be adjusted.\n\n\n\n\n\n\n\n\n\n\n\nTo illustrate the change in line width, we’re using a line across the data.\n\nRPython\n\n\nTo achieve this, we use a different geometry: geom_smooth(). The width of the line can be changed with the linewidth = argument.\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  geom_smooth(linewidth = 1) +\n  labs(title = \"geom_smooth(linewidth = 1)\")\n\n\n\n\n\n\n\nFigure 7.36: Line sizes can also be adjusted.\n\n\n\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  geom_smooth(linewidth = 3) +\n  labs(title = \"geom_smooth(linewidth = 3)\")\n\n\n\n\n\n\n\nFigure 7.37: Line sizes can also be adjusted.\n\n\n\n\n\n\n\nTo achieve this, we use a different geometry: geom_smooth(). The width of the line can be changed with the size = argument.\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n      geom_point() +\n      geom_smooth(size = 1, colour = \"blue\") +\n      labs(title = \"size = 1\"))\n      \np.show()\n\n\n\n\n\n\n\nFigure 7.38: Data point sizes can be adjusted.\n\n\n\n\n\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n      geom_point() +\n      geom_smooth(size = 3, colour = \"blue\") +\n      labs(title = \"size = 3\"))\n      \np.show()\n\n\n\n\n\n\n\nFigure 7.39: Data point sizes can be adjusted.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.regplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    scatter = True,\n    color = \"black\",\n    scatter_kws = {'s': 20},                      # point size\n    line_kws = {'color': 'blue', 'linewidth': 1}  # smooth line style\n)\n\nplt.title(\"size = 1\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.40: Line sizes can also be adjusted.\n\n\n\n\n\n\nsns.regplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    scatter = True,\n    color = \"black\",\n    scatter_kws = {'s': 20},                      # point size\n    line_kws = {'color': 'blue', 'linewidth': 3}  # smooth line style\n)\n\nplt.title(\"size = 3\")\nplt.show()\n\n\n\n\n\n\n\nFigure 7.41: Line sizes can also be adjusted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantNote on geom_smooth() in R vs Python\n\n\n\nWhen comparing R and Python, you may notice that the smooth line drawn by geom_smooth() looks different between ggplot2 and plotnine (or seaborn).\n\nIn R/ggplot2, the default smoother is LOESS (Locally Estimated Scatterplot Smoothing).\n\nIn Python/plotnine, the default is a linear regression (method = \"lm\").\n\nIn Python/seaborn, the default is also a linear regression, just like plotnine.\n\nThis is why the default smooth lines look different, even though the function name is the same.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#facets",
    "href": "materials/da2-04-plotting.html#facets",
    "title": "7  Plotting data",
    "section": "7.5 Facets",
    "text": "7.5 Facets\nPlots can split into multiple panels using faceting. This is a very useful tool to quickly see data distributions across different groups. We can split them into two types:\n\nfacet_wrap() arranges a one-dimensional sequence of panels (based on a single splitting variable) to fit on one page\nfacet_grid() allows you to form a matrix of rows and columns of panels (based on two different variables)\n\nThis is best illustrated with an example. Let’s say we want to split the weight vs hindfoot length scatter plot by the different sex groups, where the data belonging to each group has its own sub-panel. We can do this as follows.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\nFigure 7.42: Facetting data splits it into mini-panels.\n\n\n\n\n\nNote the added code:\n\nfacet_wrap(facets = vars(sex))\n\nWe used facet_wrap(), because we’re only splitting the data by a single variable: sex. We also need to tell the function which variable to split by, which we do in the facets = argument. Annoyingly - and for reasons unbeknownst to me - this requires the use of a helper function, vars().\n\n\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n        geom_point() +\n        facet_wrap(\"~ sex\"))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.43: Facetting data splits it into mini-panels.\n\n\n\n\n\nNote the added code:\n\nfacet_wrap(\"~ sex\")\n\nWe used facet_wrap(), because we’re only splitting the data by a single variable: sex. We also need to tell the function which variable to split by, which we do by using the ~ symbol. I completely agree that this is a weird notation. Just read it as split by…\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\n# Using relplot (shorthand for FacetGrid + scatterplot)\np = sns.relplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    col = \"sex\",           # facet by sex\n    kind = \"scatter\"       # scatterplot\n)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 7.44: Facetting data splits it into mini-panels.\n\n\n\n\n\n\n\n\n\n\n\nIn the end, our data is split into three sub-panels - one for each group (if missing values are included). This makes it easy to see trends across the groups. Or, in this case, that there doesn’t seem to be much difference in the distribution across the female and male observations.\nLastly, we can also split by two variables.\n\nRPython\n\n\n\nggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  facet_grid(rows = vars(plot_id), cols = vars(sex))\n\n\n\n\n\n\n\nFigure 7.45: Splitting into rows and columns.\n\n\n\n\n\n\n\n\np = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n        geom_point() +\n        facet_grid(\"plot_id ~ sex\"))\n\np.show()\n\n\n\n\n\n\n\nFigure 7.46: Splitting into rows and columns.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\n# Using relplot (shorthand for FacetGrid + scatterplot)\np = sns.relplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    col = \"sex\",           # facet by sex\n    row = \"plot_id\",\n    kind = \"scatter\"       # scatterplot\n)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 7.47: Splitting into rows and columns.\n\n\n\n\n\n\n\n\n\n\n\nTruly a terrible plot, because there are far too many groups in plot_id! We’ll see how to deal with this in later chapters.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#saving-plots",
    "href": "materials/da2-04-plotting.html#saving-plots",
    "title": "7  Plotting data",
    "section": "7.6 Saving plots",
    "text": "7.6 Saving plots\nSometimes you might want to save a plot you created. This is pretty straightforward. Here, we are assuming that you have an images subfolder in your working directory.\nWe save a plot in two steps:\n\nAssign the plot to an object\nThen use ggsave() to save this object\n\n\nRPython\n\n\n\nplot_r &lt;- ggplot(data = surveys,\n       mapping = aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  facet_wrap(facets = vars(sex))\n\n\nggsave(filename = \"images/weight_vs_hindfootlength.png\",\n       plot = plot_r,\n       width = 7,\n       height = 5,\n       units = \"in\")\n\n\n\n\nplot_python = (ggplot(surveys, aes(x = \"weight\", y = \"hindfoot_length\")) +\n        geom_point() +\n        facet_wrap(\"~ sex\"))\n\n\n(ggsave(plot_python,\n        filename = \"images/weight_vs_hindfootlength.png\",\n        units = \"in\",\n        width = 7,\n        height = 5,\n        dpi = 300))\n\n/Users/martinvanrongen/miniforge3/envs/mv372/lib/python3.13/site-packages/plotnine/ggplot.py:630: PlotnineWarning: Saving 7 x 5 in image.\n/Users/martinvanrongen/miniforge3/envs/mv372/lib/python3.13/site-packages/plotnine/ggplot.py:631: PlotnineWarning: Filename: images/weight_vs_hindfootlength.png\n/Users/martinvanrongen/miniforge3/envs/mv372/lib/python3.13/site-packages/plotnine/layer.py:372: PlotnineWarning: geom_point : Removed 4811 rows containing missing values.\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\n# Create faceted scatterplot\nplot_seaborn = sns.relplot(\n    data = surveys,\n    x = \"weight\",\n    y = \"hindfoot_length\",\n    col = \"sex\",\n    kind = \"scatter\"\n)\n\n\nplot_seaborn.savefig(\"images/weight_vs_hindfootlength_sns.png\", dpi = 300, bbox_inches = \"tight\")\n\n\n\n\n\n\n\nHere, I’ve added a few extra arguments to demonstrate what you can change. The only two things that are required are (1) the plot you want to save and (2) the name of the plot, including the filename extension.\nThe other arguments, such as units =, width = and height = are used to define the units size (inches in this case) and corresponding width/height values. It’s a good idea to add the dpi = argument (dots-per-inch) to specify the resolution, particularly when there are minimum resolution requirements when publishing!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#exercises",
    "href": "materials/da2-04-plotting.html#exercises",
    "title": "7  Plotting data",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\n\n7.7.1 Plot colours: parasites\n\n\n\n\n\n\nExerciseExercise 1 - Plot colours\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll revisit the data from data/parasites.csv. We’ll use the data to practise changing colours. After loading the data, do the following:\nCreate the plot: create a violin plot for fish_length and colour by lake - include the data points, avoiding overlap.\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nFirst we load the data.\n\nRPython\n\n\n\nparasites &lt;- read_csv(\"data/parasites.csv\")\n\n\n\n\nparasites = pd.read_csv(\"data/parasites.csv\")\n\n\n\n\nNow we can create the plot. We are creating a violin plot and colouring by lake. We’re also asked to include the data points and to avoid overlap.\n\nRPython\n\n\nWe can use geom_violin() to create a violin plot. We use the aes() to assign a colour (fill = because we’re colouring a surface area) to our data.\nAdding the data points could be done with geom_point() but then we’d end up with quite a bit of overlap. We could fix this using transparency (alpha =) but that would probably only solve part of the issue. We’re better off using geom_jitter(), where we add a tiny bit of noise to the data to avoid overlap. We can set the width of this to make sure the jittered points are not too visually overbearing.\n\nggplot(parasites, aes(x = lake, y = fish_length)) +\n  geom_violin(aes(fill = lake)) +\n  geom_jitter(width = 0.05)\n\n\n\n\n\n\n\n\n\n\nIn plotnine we can use geom_violin() to create a violin plot. We use the aes() to assign a colour (fill = because we’re colouring a surface area) to our data.\nAdding the data points could be done with geom_point() but then we’d end up with quite a bit of overlap. We could fix this using transparency (alpha =) but that would probably only solve part of the issue. We’re better off using geom_jitter(), where we add a tiny bit of noise to the data to avoid overlap. We can set the width of this to make sure the jittered points are not too visually overbearing.\n\np = (ggplot(parasites, aes(x = \"lake\", y = \"fish_length\")) +\n  geom_violin(aes(fill = \"lake\")) +\n  geom_jitter(width = 0.05))\n  \np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.7.2 Facetting plots: parasites\n\n\n\n\n\n\nExerciseExercise 2 - Facetting plots\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll again use the data from data/parasites.csv.\nTo practice creating sub panels using facetting, you’ll be plotting fish_length against parasite_count for each lake.\nWhich lake has the highest parasite count value?\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe’ll assume you still have the data loaded.\n\nRPython\n\n\n\nggplot(parasites, aes(x = fish_length, y = parasite_count)) +\n  geom_point() +\n  facet_wrap(facets = vars(lake))\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(parasites, aes(x = \"fish_length\", y = \"parasite_count\")) +\n  geom_point() +\n  facet_wrap(\"lake\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\nLake C has the highest parasite count (over 225).\n\n\n\n\n\n\n\n\n\n\n7.7.3 Customising visuals: parasites\n\n\n\n\n\n\nExerciseExercise 3 - Customising visuals\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll again be using the data from data/parasites.csv.\nThe goal of this exercise is to become more familiar with some of the customisations that are possible. You’ll be focussing on adding titles and custom axis labels to a plot. You’ll also practice customising the colour scheme of a plot.\nRecreate the following plot:\n\nRPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can refer to the Clean, style & arrange chapter for suggestions on how to do this!\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\ncb_palette   &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                  \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\n\nggplot(data = parasites,\n       mapping = aes(x = lake, y = fish_length, fill = lake)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1) +\n  scale_fill_manual(values = cb_palette) +\n  labs(title = \"Fish length by lake\",\n       x = \"Lake\",\n       y = \"Fish length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\ncb_palette = [\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n              \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"]\n\n\np = (ggplot(parasites,\n       mapping = aes(x = \"lake\", y = \"fish_length\", fill = \"lake\")) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, fill = \"black\") +\n  scale_fill_manual(values = cb_palette) +\n  labs(title = \"Fish length by lake\",\n       x = \"Lake\",\n       y = \"Fish length (mm)\"))\n\np.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da2-04-plotting.html#summary",
    "href": "materials/da2-04-plotting.html#summary",
    "title": "7  Plotting data",
    "section": "7.8 Summary",
    "text": "7.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe can build up plots layer-by-layer, adding multiple geometries in a single plot\nPlot aesthetics can be changed based on data or manually defined\nColour, fill, transparency and jittering can all be useful ways to improve clarity\nPlots can be subdivided into panels, called facets, which are based on a variable within the data. This allows easy visual comparison across groups.\nWe use functions like ggsave() to export plots to file",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA2: Data & plotting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Plotting data</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html",
    "href": "materials/da3-05-manipulating-columns.html",
    "title": "8  Manipulating columns",
    "section": "",
    "text": "8.1 Context\nIn the tabular data section we learned to deal with, well, tabular data in the form of our surveys data set. This data set isn’t huge, but sometimes we have many variables and we might only want to work with a subset of them. Or, we might want to create new columns based on existing data. In this section we’ll cover how we can do this.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#setup_manipulating_columns",
    "href": "materials/da3-05-manipulating-columns.html#setup_manipulating_columns",
    "title": "8  Manipulating columns",
    "section": "8.2 Section setup",
    "text": "8.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 03_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 03_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\nsurveys = pd.read_csv(\"data/surveys.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#selecting-columns",
    "href": "materials/da3-05-manipulating-columns.html#selecting-columns",
    "title": "8  Manipulating columns",
    "section": "8.3 Selecting columns",
    "text": "8.3 Selecting columns\nLet’s remind ourselves to which columns we have in our surveys data set. After that, we’ll start making some changes.\n\nRPython\n\n\n\ncolnames(surveys)\n\n[1] \"record_id\"       \"month\"           \"day\"             \"year\"           \n[5] \"plot_id\"         \"species_id\"      \"sex\"             \"hindfoot_length\"\n[9] \"weight\"         \n\n\n\n\n\nsurveys.columns\n\nIndex(['record_id', 'month', 'day', 'year', 'plot_id', 'species_id', 'sex',\n       'hindfoot_length', 'weight'],\n      dtype='object')\n\n\n\n\n\n\n8.3.1 Selecting individual columns\nLet’s say we wanted to select only the record_id and year columns. We’ve briefly done this when we looked at subsetting rows and columns.\n\nRPython\n\n\nHowever, there is an alternative way of doing this using the dplyr package - which is part of tidyverse.\nWe can use the select() function:\n\nselect(surveys, record_id, year)\n\n# A tibble: 35,549 × 2\n   record_id  year\n       &lt;dbl&gt; &lt;dbl&gt;\n 1         1  1977\n 2         2  1977\n 3         3  1977\n 4         4  1977\n 5         5  1977\n 6         6  1977\n 7         7  1977\n 8         8  1977\n 9         9  1977\n10        10  1977\n# ℹ 35,539 more rows\n\n\nUsing the base R syntax, this is equivalent to surveys[, c(\"record_id\", \"year\")]. Notice that with the select() function (and generally with dplyr functions) we didn’t need to quote ” the column names. This is because the first input to the function is the table name, and so everything after is assumed to be column names of that table.\n\n\n\n\n\n\nTipGetting a better view\n\n\n\nIf we wanted to get the resulting output in a separate tab, so we could view it better, we would be able to wrap this inside the View() function:\n\nView(select(surveys, record_id, year))\n\n\n\n\n\nThe way we need to specify this is by giving a list of column names [\"record_id\", \"year\"] and subsetting the surveys data set with this.\nThe way we subset is with surveys[ ], so we end up with double square brackets:\n\nsurveys[[\"record_id\", \"year\"]]\n\n       record_id  year\n0              1  1977\n1              2  1977\n2              3  1977\n3              4  1977\n4              5  1977\n...          ...   ...\n35544      35545  2002\n35545      35546  2002\n35546      35547  2002\n35547      35548  2002\n35548      35549  2002\n\n[35549 rows x 2 columns]\n\n\n\n\n\n\n\n8.3.2 Selecting with helper functions\n\nRPython\n\n\nThe select() function becomes particularly useful when we combine it with other helper functions. For example, this code will select all the columns where the column name contains the string (text) \"_id\":\n\n# returns all columns where the column name contains the text \"_id\"\nselect(surveys, contains(\"_id\"))\n\n# A tibble: 35,549 × 3\n   record_id plot_id species_id\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1         1       2 NL        \n 2         2       3 NL        \n 3         3       2 DM        \n 4         4       7 DM        \n 5         5       3 DM        \n 6         6       1 PF        \n 7         7       2 PE        \n 8         8       1 DM        \n 9         9       1 DM        \n10        10       6 PF        \n# ℹ 35,539 more rows\n\n\nAnother thing we often want to do is select columns by their data type. For example, if we wanted to select all numerical columns we could do this:\n\nselect(surveys, where(is.numeric))\n\n# A tibble: 35,549 × 7\n   record_id month   day  year plot_id hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2              32     NA\n 2         2     7    16  1977       3              33     NA\n 3         3     7    16  1977       2              37     NA\n 4         4     7    16  1977       7              36     NA\n 5         5     7    16  1977       3              35     NA\n 6         6     7    16  1977       1              14     NA\n 7         7     7    16  1977       2              NA     NA\n 8         8     7    16  1977       1              37     NA\n 9         9     7    16  1977       1              34     NA\n10        10     7    16  1977       6              20     NA\n# ℹ 35,539 more rows\n\n\n\n\nThe subsetting becomes a bit tedious when we’re looking for patterns in the column names. Here, we can instead use the .filter attribute of the surveys data set, and look for a string (text) where the column name contains \"_id\".\n\nsurveys.filter(like = \"_id\")\n\n       record_id  plot_id species_id\n0              1        2         NL\n1              2        3         NL\n2              3        2         DM\n3              4        7         DM\n4              5        3         DM\n...          ...      ...        ...\n35544      35545       15         AH\n35545      35546       15         AH\n35546      35547       10         RM\n35547      35548        7         DO\n35548      35549        5        NaN\n\n[35549 rows x 3 columns]\n\n\nAnother thing we often want to do is select columns by their data type. For example, if we wanted to select all numerical columns we could do this:\n\nsurveys.select_dtypes(include = \"number\")\n\n       record_id  month  day  year  plot_id  hindfoot_length  weight\n0              1      7   16  1977        2             32.0     NaN\n1              2      7   16  1977        3             33.0     NaN\n2              3      7   16  1977        2             37.0     NaN\n3              4      7   16  1977        7             36.0     NaN\n4              5      7   16  1977        3             35.0     NaN\n...          ...    ...  ...   ...      ...              ...     ...\n35544      35545     12   31  2002       15              NaN     NaN\n35545      35546     12   31  2002       15              NaN     NaN\n35546      35547     12   31  2002       10             15.0    14.0\n35547      35548     12   31  2002        7             36.0    51.0\n35548      35549     12   31  2002        5              NaN     NaN\n\n[35549 rows x 7 columns]\n\n\n\n\n\n\n\n8.3.3 Selecting a range of columns\nLet’s say we’re interested in all the columns from record_id to year.\n\nRPython\n\n\nIn that case, we can use the : symbol.\n\n# returns all columns between and including record_id and year\nselect(surveys, record_id:year)\n\n# A tibble: 35,549 × 4\n   record_id month   day  year\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1         1     7    16  1977\n 2         2     7    16  1977\n 3         3     7    16  1977\n 4         4     7    16  1977\n 5         5     7    16  1977\n 6         6     7    16  1977\n 7         7     7    16  1977\n 8         8     7    16  1977\n 9         9     7    16  1977\n10        10     7    16  1977\n# ℹ 35,539 more rows\n\n\nWe can also combine this with the previous method:\n\n# returns all columns between and including record_id and year\n# and all columns where the column name contains the text \"_id\"\nselect(surveys, record_id:year, contains(\"_id\"))\n\n# A tibble: 35,549 × 6\n   record_id month   day  year plot_id species_id\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     \n 1         1     7    16  1977       2 NL        \n 2         2     7    16  1977       3 NL        \n 3         3     7    16  1977       2 DM        \n 4         4     7    16  1977       7 DM        \n 5         5     7    16  1977       3 DM        \n 6         6     7    16  1977       1 PF        \n 7         7     7    16  1977       2 PE        \n 8         8     7    16  1977       1 DM        \n 9         9     7    16  1977       1 DM        \n10        10     7    16  1977       6 PF        \n# ℹ 35,539 more rows\n\n\n\n\nIn that case, we can use the : symbol, in combination with the .loc indexer.\n\nsurveys.loc[:, \"record_id\":\"year\"]\n\n       record_id  month  day  year\n0              1      7   16  1977\n1              2      7   16  1977\n2              3      7   16  1977\n3              4      7   16  1977\n4              5      7   16  1977\n...          ...    ...  ...   ...\n35544      35545     12   31  2002\n35545      35546     12   31  2002\n35546      35547     12   31  2002\n35547      35548     12   31  2002\n35548      35549     12   31  2002\n\n[35549 rows x 4 columns]\n\n\n\n\n\n\n\n8.3.4 Unselecting columns\nLastly, we can also unselect columns. This can be useful when you want most columns, apart from some.\n\nRPython\n\n\nTo do this, we use the - symbol before the column name.\n\n# returns all columns apart from record_id\nselect(surveys, -record_id)\n\n# A tibble: 35,549 × 8\n   month   day  year plot_id species_id sex   hindfoot_length weight\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1     7    16  1977       2 NL         M                  32     NA\n 2     7    16  1977       3 NL         M                  33     NA\n 3     7    16  1977       2 DM         F                  37     NA\n 4     7    16  1977       7 DM         M                  36     NA\n 5     7    16  1977       3 DM         M                  35     NA\n 6     7    16  1977       1 PF         M                  14     NA\n 7     7    16  1977       2 PE         F                  NA     NA\n 8     7    16  1977       1 DM         M                  37     NA\n 9     7    16  1977       1 DM         F                  34     NA\n10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n\n\n\n\nTo do this, we use the .drop attribute. Here, we only unselect one column, but we can easily extend this by providing a list of columns do the column = argument.\n\nsurveys.drop(columns = \"record_id\")\n\n       month  day  year  plot_id species_id  sex  hindfoot_length  weight\n0          7   16  1977        2         NL    M             32.0     NaN\n1          7   16  1977        3         NL    M             33.0     NaN\n2          7   16  1977        2         DM    F             37.0     NaN\n3          7   16  1977        7         DM    M             36.0     NaN\n4          7   16  1977        3         DM    M             35.0     NaN\n...      ...  ...   ...      ...        ...  ...              ...     ...\n35544     12   31  2002       15         AH  NaN              NaN     NaN\n35545     12   31  2002       15         AH  NaN              NaN     NaN\n35546     12   31  2002       10         RM    F             15.0    14.0\n35547     12   31  2002        7         DO    M             36.0    51.0\n35548     12   31  2002        5        NaN  NaN              NaN     NaN\n\n[35549 rows x 8 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#renaming-and-reshuffling-columns",
    "href": "materials/da3-05-manipulating-columns.html#renaming-and-reshuffling-columns",
    "title": "8  Manipulating columns",
    "section": "8.4 Renaming and reshuffling columns",
    "text": "8.4 Renaming and reshuffling columns\n\n8.4.1 Renaming columns\nFor example, we might want to change the weight column name to weight_g, to reflect that the values are in grams.\n\nRPython\n\n\nWe can use the rename() function to change a column name. We do this as follows:\n\nrename(surveys, weight_g = weight)\n\n# A tibble: 35,549 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight_g\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32       NA\n 2         2     7    16  1977       3 NL         M                  33       NA\n 3         3     7    16  1977       2 DM         F                  37       NA\n 4         4     7    16  1977       7 DM         M                  36       NA\n 5         5     7    16  1977       3 DM         M                  35       NA\n 6         6     7    16  1977       1 PF         M                  14       NA\n 7         7     7    16  1977       2 PE         F                  NA       NA\n 8         8     7    16  1977       1 DM         M                  37       NA\n 9         9     7    16  1977       1 DM         F                  34       NA\n10        10     7    16  1977       6 PF         F                  20       NA\n# ℹ 35,539 more rows\n\n\n\n\nWe can use the .rename() attribute of the surveys pandas DataFrame:\n\nsurveys.rename(columns = {'weight': 'weight_g'})\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight_g\n0              1      7   16  1977  ...          NL    M            32.0       NaN\n1              2      7   16  1977  ...          NL    M            33.0       NaN\n2              3      7   16  1977  ...          DM    F            37.0       NaN\n3              4      7   16  1977  ...          DM    M            36.0       NaN\n4              5      7   16  1977  ...          DM    M            35.0       NaN\n...          ...    ...  ...   ...  ...         ...  ...             ...       ...\n35544      35545     12   31  2002  ...          AH  NaN             NaN       NaN\n35545      35546     12   31  2002  ...          AH  NaN             NaN       NaN\n35546      35547     12   31  2002  ...          RM    F            15.0      14.0\n35547      35548     12   31  2002  ...          DO    M            36.0      51.0\n35548      35549     12   31  2002  ...         NaN  NaN             NaN       NaN\n\n[35549 rows x 9 columns]\n\n\n\n\n\n\n\n8.4.2 Reshuffling columns\nIt might be that you want to reorder/reshuffle a column. Here, the year column is our fourth variable. Let’s say we’d want to move this to the second position (after record_id).\n\nRPython\n\n\nWe can use the relocate() function to do this. The function has several arguments, starting with ., such as .before = or .after =. These allow you to specify where you want to reinsert the column.\n\nrelocate(surveys, year, .after = record_id)\n\n# A tibble: 35,549 × 9\n   record_id  year month   day plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1  1977     7    16       2 NL         M                  32     NA\n 2         2  1977     7    16       3 NL         M                  33     NA\n 3         3  1977     7    16       2 DM         F                  37     NA\n 4         4  1977     7    16       7 DM         M                  36     NA\n 5         5  1977     7    16       3 DM         M                  35     NA\n 6         6  1977     7    16       1 PF         M                  14     NA\n 7         7  1977     7    16       2 PE         F                  NA     NA\n 8         8  1977     7    16       1 DM         M                  37     NA\n 9         9  1977     7    16       1 DM         F                  34     NA\n10        10  1977     7    16       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n\n\n\n\nUnlike in R, there isn’t a very clear, straightforward way of reinserting columns in a pandas DataFrame. We could show you convoluted ways of doing so, but at this point that’s just confusing. So, we’ll leave you with a link to a Stackoverflow solution.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#creating-new-columns",
    "href": "materials/da3-05-manipulating-columns.html#creating-new-columns",
    "title": "8  Manipulating columns",
    "section": "8.5 Creating new columns",
    "text": "8.5 Creating new columns\nSometimes we need to create new columns. For example, we might have a variable that is not in the unit of measurement we need (e.g. in millimeters, instead of centimeters).\nConceptually, that looks something like this:\n\n\n\n\n\n\nFigure 8.1: Creating a new column using data from an existing one.\n\n\n\nLet’s illustrate this with an example on our surveys data set. Let’s say we wanted to get hindfoot_length in centimeters, instead of millimeters. We’d have to go through each row, take the hindfoot_length value and divide it by 10. We then need to store this output in a column called, for example, hindfoot_length_cm.\n\nRPython\n\n\nWe can use the mutate() function to create new columns:\n\nmutate(surveys, hindfoot_length_cm = hindfoot_length / 10)\n\n# A tibble: 35,549 × 10\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         6     7    16  1977       1 PF         M                  14     NA\n 7         7     7    16  1977       2 PE         F                  NA     NA\n 8         8     7    16  1977       1 DM         M                  37     NA\n 9         9     7    16  1977       1 DM         F                  34     NA\n10        10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n# ℹ 1 more variable: hindfoot_length_cm &lt;dbl&gt;\n\n\n\n\nWe use the square brackets to define the name of the new column, then specify what needs to go in the new column:\n\nsurveys['hindfoot_length_cm'] = surveys['hindfoot_length'] / 10\n\n\n\n\nAlthough it has created the column, we can’t quite see it because we have too many columns. So, let’s save the new column to the data set and then select the relevant columns.\n\nRPython\n\n\nFirst, we update our data:\n\nsurveys &lt;- mutate(surveys, hindfoot_length_cm = hindfoot_length / 10)\n\nNext, we can select the columns.\n\nselect(surveys, record_id, hindfoot_length, hindfoot_length_cm)\n\n# A tibble: 35,549 × 3\n   record_id hindfoot_length hindfoot_length_cm\n       &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;\n 1         1              32                3.2\n 2         2              33                3.3\n 3         3              37                3.7\n 4         4              36                3.6\n 5         5              35                3.5\n 6         6              14                1.4\n 7         7              NA               NA  \n 8         8              37                3.7\n 9         9              34                3.4\n10        10              20                2  \n# ℹ 35,539 more rows\n\n\n\n\nOur previous step already added the new column to the DataFrame, so we can directly select the relevant columns, by giving a list of the columns we’re interested in:\n\nsurveys[['record_id', 'hindfoot_length', 'hindfoot_length_cm']]\n\n       record_id  hindfoot_length  hindfoot_length_cm\n0              1             32.0                 3.2\n1              2             33.0                 3.3\n2              3             37.0                 3.7\n3              4             36.0                 3.6\n4              5             35.0                 3.5\n...          ...              ...                 ...\n35544      35545              NaN                 NaN\n35545      35546              NaN                 NaN\n35546      35547             15.0                 1.5\n35547      35548             36.0                 3.6\n35548      35549              NaN                 NaN\n\n[35549 rows x 3 columns]\n\n\n\n\n\nWe can see that each value in hindfoot_length_cm is a tenth of the value of hindfoot_length. This is exactly what we expected!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#exercises",
    "href": "materials/da3-05-manipulating-columns.html#exercises",
    "title": "8  Manipulating columns",
    "section": "8.6 Exercises",
    "text": "8.6 Exercises\n\n8.6.1 Selecting columns: infections\n\n\n\n\n\n\nExerciseExercise 1 - Selecting columns\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using a new data set, called infections. These are synthetic data (see the script if you would like to know more).\nPlease do the following:\n\nRead in the data\nGet to grips with the structure of the data\nSelect the patient_id, systolic_pressure and body_temperature columns. How many rows have no missing data at all?\nSelect all numerical columns. How many columns are there?\nUnselect all logical columns. How many columns are left?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\n1. Read in the data\nFirst we load the data, and have a glimpse of it.\n\nRPython\n\n\n\ninfections &lt;- read_csv(\"data/infections.csv\")\n\n\nhead(infections)\n\n# A tibble: 6 × 11\n  patient_id hospital   quarter infection_type vaccination_status age_group\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;    \n1 ID_0001    hospital_3 Q2      none           &lt;NA&gt;               65+      \n2 ID_0002    hospital_3 Q2      viral          &lt;NA&gt;               18 - 64  \n3 ID_0003    hospital_2 Q2      none           unknown            65+      \n4 ID_0004    hospital_2 Q3      fungal         unvaccinated       &lt; 18     \n5 ID_0005    hospital_3 Q2      fungal         vaccinated         65+      \n6 ID_0006    hospital_5 Q3      none           vaccinated         65+      \n# ℹ 5 more variables: icu_admission &lt;lgl&gt;, symptoms_count &lt;dbl&gt;,\n#   systolic_pressure &lt;dbl&gt;, body_temperature &lt;dbl&gt;, crp_level &lt;dbl&gt;\n\n\n\n\n\ninfections = pd.read_csv(\"data/infections.csv\")\n\ninfections.head()\n\n  patient_id    hospital  ... body_temperature crp_level\n0    ID_0001  hospital_3  ...             37.8     12.05\n1    ID_0002  hospital_3  ...             39.1      8.11\n2    ID_0003  hospital_2  ...             38.5      5.24\n3    ID_0004  hospital_2  ...             39.4     41.73\n4    ID_0005  hospital_3  ...             36.9     10.51\n\n[5 rows x 11 columns]\n\n\n\n\n\n\n\n2. Get to grips with the structure of the data\nBefore delving into any analysis, it’s always good to have a good look at your data, so you know what you’re dealing with. We can look at the overall structure (focussing on the column types & checking if they make sense), at number of columns/observations, column names, summary statistics etc.\n\nRPython\n\n\n\nstr(infections) # overall structure\n\nspc_tbl_ [1,400 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patient_id        : chr [1:1400] \"ID_0001\" \"ID_0002\" \"ID_0003\" \"ID_0004\" ...\n $ hospital          : chr [1:1400] \"hospital_3\" \"hospital_3\" \"hospital_2\" \"hospital_2\" ...\n $ quarter           : chr [1:1400] \"Q2\" \"Q2\" \"Q2\" \"Q3\" ...\n $ infection_type    : chr [1:1400] \"none\" \"viral\" \"none\" \"fungal\" ...\n $ vaccination_status: chr [1:1400] NA NA \"unknown\" \"unvaccinated\" ...\n $ age_group         : chr [1:1400] \"65+\" \"18 - 64\" \"65+\" \"&lt; 18\" ...\n $ icu_admission     : logi [1:1400] FALSE FALSE TRUE TRUE TRUE FALSE ...\n $ symptoms_count    : num [1:1400] 1 6 3 7 7 5 10 12 13 7 ...\n $ systolic_pressure : num [1:1400] 117 115 120 129 114 124 133 120 124 127 ...\n $ body_temperature  : num [1:1400] 37.8 39.1 38.5 39.4 36.9 36.8 39.4 39.3 39.6 39.1 ...\n $ crp_level         : num [1:1400] 12.05 8.11 5.24 41.73 10.51 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patient_id = col_character(),\n  ..   hospital = col_character(),\n  ..   quarter = col_character(),\n  ..   infection_type = col_character(),\n  ..   vaccination_status = col_character(),\n  ..   age_group = col_character(),\n  ..   icu_admission = col_logical(),\n  ..   symptoms_count = col_double(),\n  ..   systolic_pressure = col_double(),\n  ..   body_temperature = col_double(),\n  ..   crp_level = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nnrow(infections) # row number\n\n[1] 1400\n\nncol(infections) # column number\n\n[1] 11\n\n\n\ncolnames(infections) # column names\n\n [1] \"patient_id\"         \"hospital\"           \"quarter\"           \n [4] \"infection_type\"     \"vaccination_status\" \"age_group\"         \n [7] \"icu_admission\"      \"symptoms_count\"     \"systolic_pressure\" \n[10] \"body_temperature\"   \"crp_level\"         \n\n\n\nsummary(infections) # summary statistics\n\n  patient_id          hospital           quarter          infection_type    \n Length:1400        Length:1400        Length:1400        Length:1400       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n vaccination_status  age_group         icu_admission   symptoms_count  \n Length:1400        Length:1400        Mode :logical   Min.   : 0.000  \n Class :character   Class :character   FALSE:814       1st Qu.: 6.000  \n Mode  :character   Mode  :character   TRUE :513       Median : 9.000  \n                                       NA's :73        Mean   : 8.549  \n                                                       3rd Qu.:11.000  \n                                                       Max.   :21.000  \n                                                       NA's   :67      \n systolic_pressure body_temperature   crp_level     \n Min.   : 87.0     Min.   :36.30    Min.   : 1.000  \n 1st Qu.:118.0     1st Qu.:38.20    1st Qu.: 9.117  \n Median :125.0     Median :38.80    Median :16.215  \n Mean   :125.1     Mean   :38.75    Mean   :19.465  \n 3rd Qu.:132.0     3rd Qu.:39.40    3rd Qu.:26.433  \n Max.   :163.0     Max.   :41.50    Max.   :58.860  \n NA's   :74        NA's   :67       NA's   :156     \n\n\n\n\n\ninfections.info() # overall structure\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1400 entries, 0 to 1399\nData columns (total 11 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   patient_id          1400 non-null   object \n 1   hospital            1332 non-null   object \n 2   quarter             1325 non-null   object \n 3   infection_type      1326 non-null   object \n 4   vaccination_status  1325 non-null   object \n 5   age_group           1329 non-null   object \n 6   icu_admission       1327 non-null   object \n 7   symptoms_count      1333 non-null   float64\n 8   systolic_pressure   1326 non-null   float64\n 9   body_temperature    1333 non-null   float64\n 10  crp_level           1244 non-null   float64\ndtypes: float64(4), object(7)\nmemory usage: 120.4+ KB\n\n\n\ninfections.shape[0] # row number\n\n1400\n\ninfections.shape[1] # column number\n\n11\n\n\n\ninfections.columns # column names\n\nIndex(['patient_id', 'hospital', 'quarter', 'infection_type',\n       'vaccination_status', 'age_group', 'icu_admission', 'symptoms_count',\n       'systolic_pressure', 'body_temperature', 'crp_level'],\n      dtype='object')\n\n\n\ninfections.describe() # summary statistics\n\n       symptoms_count  systolic_pressure  body_temperature    crp_level\ncount     1333.000000        1326.000000       1333.000000  1244.000000\nmean         8.549137         125.070136         38.751988    19.464751\nstd          3.825004          10.346761          0.883689    13.083907\nmin          0.000000          87.000000         36.300000     1.000000\n25%          6.000000         118.000000         38.200000     9.117500\n50%          9.000000         125.000000         38.800000    16.215000\n75%         11.000000         132.000000         39.400000    26.432500\nmax         21.000000         163.000000         41.500000    58.860000\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the icu_admission column has a data type of object (text). This is not correct: it should be logical / boolean (True or False). Think about why this might have happened and how to fix this. You’ll need to consider this later on in the exercise!\n\n\n\n\n\nNow that we have a much better view of what our data looks like, let’s play around with selecting and manipulating columns.\n\n\n3. Select the patient_id, systolic_pressure and body_temperature columns\n\nRPython\n\n\n\nsubset &lt;- select(infections, patient_id, systolic_pressure, body_temperature)\n\n\nnrow(drop_na(subset))\n\n[1] 1263\n\n\n\n\nWe use the [[ ]] notations, because we are subsetting the DataFrame (infections[ ]) and we are subsetting it by using a list of column names [\"patient_id\", \"systolic_pressure\", \"body_temperature\"]:\n\nsubset = infections[[\"patient_id\", \"systolic_pressure\", \"body_temperature\"]]\n\n\nsubset.dropna().shape[0]\n\n1263\n\n\n\n\n\n\n\n4. Select all numerical columns\n\nRPython\n\n\n\nsubset &lt;- select(infections, where(is.numeric))\n\n\nncol(subset)\n\n[1] 4\n\n\n\n\n\nsubset = infections.select_dtypes(include = [\"number\"])\n\n\nsubset.shape[1]\n\n4\n\n\n\n\n\n\n\n5. Unselect all logical columns\n\nRPython\n\n\nWe can do something similar as with the numerical column, just specifying the logical type and then negating it:\n\nsubset &lt;- select(infections, -where(is.logical))\n\n\nncol(subset)\n\n[1] 10\n\n\n\n\nWe can do something similar as with the numerical column, just specifying the bool type and now excluding it. However, as we saw earlier, the only column that is supposed to have logical / boolean data (icu_admission) did not get read in accordingly. So, we need to (1) find out why and (2) fix it.\nLet’s see what kind of data we have in the column.\n\ninfections.icu_admission.dtype \n\ndtype('O')\n\n\nThis gives us a data type of O for object. Let’s see what categories we have in the column. We can count the number of occurrences using .value_counts(), but als include the missing data - if there are any present.\n\ninfections[\"icu_admission\"].value_counts(dropna = False)\n\nicu_admission\nFalse    814\nTrue     513\nNaN       73\nName: count, dtype: int64\n\n\nWe can see that we have ‘True’ and ‘False’ (as expected). But we also have missing data, which is the reason why pandas is reading the column in as text. We can coerce (force) the column to be viewed as logical data.\n\ninfections[\"icu_admission\"] = infections[\"icu_admission\"].astype(\"boolean\")\n\nWe can see that the dtype has changed:\n\ninfections.icu_admission.dtype\n\nBooleanDtype\n\n\nLastly, we can now finally unselect our column and count the number of remaining columns:\n\nsubset = infections.select_dtypes(exclude = [\"bool\"])\n\n\nsubset.shape[1]\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.6.2 Creating columns: body_temperature_f\n\n\n\n\n\n\nExerciseExercise 2 - Creating columns\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll again use the data from data/infections.csv. We’ll assume you’ve still have it read in. Please do the following:\nCreate a column called body_temperature_f that contains the body temperature measurements in Fahrenheit, rounded to a whole number.\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nTo convert Celsius to Fahrenheit, do the following:\n\\[\nF = \\left( C \\times \\frac{9}{5} \\right) + 32\n\\]\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe need to update the existing table, so we view the result.\n\nRPython\n\n\n\ninfections &lt;- mutate(infections, body_temperature_f = round(body_temperature * (9/5) + 32))\n\nLet’s check if the result is what we expected.\n\nselect(infections, body_temperature, body_temperature_f)\n\n# A tibble: 1,400 × 2\n   body_temperature body_temperature_f\n              &lt;dbl&gt;              &lt;dbl&gt;\n 1             37.8                100\n 2             39.1                102\n 3             38.5                101\n 4             39.4                103\n 5             36.9                 98\n 6             36.8                 98\n 7             39.4                103\n 8             39.3                103\n 9             39.6                103\n10             39.1                102\n# ℹ 1,390 more rows\n\n\n\n\n\ninfections[\"body_temperature_f\"] = round(infections[\"body_temperature\"] * (9/5) + 32)\n\nLet’s check if the result is what we expected.\n\ninfections[[\"body_temperature\", \"body_temperature_f\"]]\n\n      body_temperature  body_temperature_f\n0                 37.8               100.0\n1                 39.1               102.0\n2                 38.5               101.0\n3                 39.4               103.0\n4                 36.9                98.0\n...                ...                 ...\n1395              39.0               102.0\n1396              38.8               102.0\n1397               NaN                 NaN\n1398              38.8               102.0\n1399              37.0                99.0\n\n[1400 rows x 2 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.6.3 Creating and plotting columns\n\n\n\n\n\n\nExerciseExercise 3 - Creating and plotting columns\n\n\n\n\n\n\nLevel: \nWe’ll again use the infections data set, but this time we want you to fix the code below to recreate the image. To do so, please change all the &lt;FIXME&gt; entries to the correct code.\n\nRPython\n\n\n\n# create the column for the y-axis values\ninfections &lt;- mutate(infections, systolic_pressure_kpa = &lt;FIXME&gt; / 7.5006)\n\n# plot the data\nggplot(infections, aes(x = &lt;FIXME&gt;, y = systolic_pressure_kpa)) +\n  geom_&lt;FIXME&gt;(aes(fill = &lt;FIXME&gt;))\n\n\n\n\n\n\n\n\n\n\n\n\n\n# create the column for the y-axis values\ninfections[\"systolic_pressure_kpa\"] = infections[\"&lt;FIXME&gt;\"] / 7.5006\n\n# plot the data\np = (ggplot(infections, aes(x = \"&lt;FIXME&gt;\", y = \"systolic_pressure_kpa\")) +\n  geom_&lt;FIXME&gt;(aes(fill = infections[\"&lt;FIXME&gt;\"].astype(str))))\n  \np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotemmHg to kPa\n\n\n\n\n\n\\[\n1\\ \\text{kPa} \\approx 7.5006\\ \\text{mmHg}\n\\]\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\nRPython\n\n\n\n# create the column for the y-axis values\ninfections &lt;- mutate(infections,\n                     systolic_pressure_kpa = systolic_pressure / 7.5006)\n\n# plot the data\nggplot(infections, aes(x = age_group, y = systolic_pressure_kpa)) +\n  geom_boxplot(aes(fill = icu_admission))\n\n\n\n\n# create the column for the y-axis values\ninfections[\"systolic_pressure_kpa\"] = infections[\"systolic_pressure\"] / 7.5006\n\n# plot the data\np = (ggplot(infections, aes(x = \"age_group\", y = \"systolic_pressure_kpa\")) +\n  geom_boxplot(aes(fill = infections[\"icu_admission\"].astype(str))))\n  \np.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-05-manipulating-columns.html#summary",
    "href": "materials/da3-05-manipulating-columns.html#summary",
    "title": "8  Manipulating columns",
    "section": "8.7 Summary",
    "text": "8.7 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe have several functions available that allow us to select, move, rename and create new columns",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Manipulating columns</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html",
    "href": "materials/da3-06-chaining-operations.html",
    "title": "9  Chaining operations",
    "section": "",
    "text": "9.1 Context\nIn the section above we performed several operations on a single data set. Often there is a sequence to this, where the output of one operation gets fed into the next. We can simplify this by chaining commands.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html#setup_chaining_operations",
    "href": "materials/da3-06-chaining-operations.html#setup_chaining_operations",
    "title": "9  Chaining operations",
    "section": "9.2 Section setup",
    "text": "9.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 03_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 03_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\nsurveys = pd.read_csv(\"data/surveys.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html#pipes-or-chaining-commands",
    "href": "materials/da3-06-chaining-operations.html#pipes-or-chaining-commands",
    "title": "9  Chaining operations",
    "section": "9.3 Pipes or chaining commands",
    "text": "9.3 Pipes or chaining commands\nSo far, we’ve used single operations when we were manipulating our data. For example, we can select columns with:\n\nRPython\n\n\n\nselect(surveys, record_id, hindfoot_length)\n\nLet’s say we wanted combine that with creating a new column, for example hindfoot length in centimeters.\nWe would have to do the following:\n\n# grab the relevant columns and store in a new object\nsubset_surveys &lt;- select(surveys, record_id, hindfoot_length)\n\n# create the new column\nmutate(subset_surveys, hindfoot_length_cm = hindfoot_length / 10)\n\n# A tibble: 35,549 × 3\n   record_id hindfoot_length hindfoot_length_cm\n       &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;\n 1         1              32                3.2\n 2         2              33                3.3\n 3         3              37                3.7\n 4         4              36                3.6\n 5         5              35                3.5\n 6         6              14                1.4\n 7         7              NA               NA  \n 8         8              37                3.7\n 9         9              34                3.4\n10        10              20                2  \n# ℹ 35,539 more rows\n\n\nWe had to create a new object (here, called subset_surveys) to store the intermediate data we were interested in, and then continue with creating the new column.\nThis clutters up your computer’s memory rather quickly when dealing with lots of data. A much better way is that we pipe or chain one after the other. To do this, we start with the data and use a pipe symbol (|&gt; or %&gt;%) as follows:\n\nsurveys |&gt; \n  select(record_id, hindfoot_length) |&gt;\n  mutate(hindfoot_length_cm = hindfoot_length / 10)\n\n# A tibble: 35,549 × 3\n   record_id hindfoot_length hindfoot_length_cm\n       &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;\n 1         1              32                3.2\n 2         2              33                3.3\n 3         3              37                3.7\n 4         4              36                3.6\n 5         5              35                3.5\n 6         6              14                1.4\n 7         7              NA               NA  \n 8         8              37                3.7\n 9         9              34                3.4\n10        10              20                2  \n# ℹ 35,539 more rows\n\n\nAn easy way of remembering what the pipe does is to replace (in your head) the pipe symbol with the phrase “and then…”.\nSo, we select() the record_id and hindfoot_length columns and then use mutate() to create a new column called hindfoot_length_cm.\n\n\n\n\n\n\nNoteWhich pipe symbol do I use?\n\n\n\nYou’ll find that people use two pipe symbols quite interchangeably in R: the |&gt; pipe (native, built-in R) and %&gt;% from the magrittr package.\nThe native, built-in pipe is a rather new addition, since version 4.1. It is slightly different in its behaviour than the %&gt;% pipe (if you want to know more, see here), but for most purposes they work the same.\nWe tend to use the native, built-in pipe throughout the materials. But the magrittr pipe works just as well! You can change your preference in RStudio by going to Tools &gt; Global options &gt; Code and changing the tickbox enabling/disabling the native pipe operator.\n\n\n\n\n\nsurveys[[\"record_id\", \"hindfoot_length\"]].copy()\n\n       record_id  hindfoot_length\n0              1             32.0\n1              2             33.0\n2              3             37.0\n3              4             36.0\n4              5             35.0\n...          ...              ...\n35544      35545              NaN\n35545      35546              NaN\n35546      35547             15.0\n35547      35548             36.0\n35548      35549              NaN\n\n[35549 rows x 2 columns]\n\n\nLet’s say we wanted combine that with creating a new column, for example hindfoot length in centimeters.\nWe would have to do the following:\n\n# select the required columns and store in a new data set\nsubset_surveys = surveys[[\"record_id\", \"hindfoot_length\"]].copy()\n\n# take the new data set and calculate the new column\nsubset_surveys[\"hindfoot_length_cm\"] = subset_surveys[\"hindfoot_length\"] / 10\n\nWe had to create a new object (here, called subset_surveys) to store the intermediate data we were interested in, and then continue with creating the new column.\nThis clutters up your computer’s memory rather quickly when dealing with lots of data. So, it’d be good if we could pipe or chain these commands, like we can do in R.\nPython does not have an exact equivalent to pipes in R, but you can chain methods in pandas, which can make your life a lot easier. Let’s explore that a bit further.\nIn the code below we are using .loc[:, [\"col1\", \"col2\"]] to select columns. This is a bit more chain-friendly than using [[\"col1\", \"col2\"]] because it also allows filtering (more on this in the next chapter).\n\n(\n  surveys\n  .loc[:, ['record_id', 'hindfoot_length']]\n  .assign(hindfoot_length_cm = lambda df: df[\"hindfoot_length\"] / 10)\n)\n\n       record_id  hindfoot_length  hindfoot_length_cm\n0              1             32.0                 3.2\n1              2             33.0                 3.3\n2              3             37.0                 3.7\n3              4             36.0                 3.6\n4              5             35.0                 3.5\n...          ...              ...                 ...\n35544      35545              NaN                 NaN\n35545      35546              NaN                 NaN\n35546      35547             15.0                 1.5\n35547      35548             36.0                 3.6\n35548      35549              NaN                 NaN\n\n[35549 rows x 3 columns]\n\n\nHere, we do the following:\n\n.loc[:, ['record_id', 'hindfoot_length']] selects the columns you want\n.assign(...) then creates a new column\nlambda df tells pandas to compute the new column using the current data frame in the chain\n\n\n\n\n\n\n\nImportantOh, lambda!\n\n\n\nWhy Use lambda in Pandas .assign()?\nIn Python, a lambda is an anonymous function — a quick, in-place function without a name.\nWhy is lambda important in method chaining?\nThis all relates to the evaluation order: .assign() evaluates its arguments before the previous steps in the chain are fully applied.\nSo if you reference a column that was renamed or created earlier in the chain, pandas won’t find it unless you use lambda to delay evaluation.\nUsing lambda in .assign() ensures that pandas waits to evaluate the new column until the DataFrame is fully updated by the earlier steps.\nSuppose we want to:\n\nSelect two columns\nCreate a new column weight_kg as weight_g / 1000\n\nWithout lambda — this will raise an error:\n(\n  surveys\n  .loc[:, [\"record_id\", \"weight\"]]\n  .assign(weight_kg = surveys[\"weight_g\"] / 1000)  # KeyError: \"weight_g\"\n)\n\n\n\n\n\n\n\n\nNotePipe-style packages in Python\n\n\n\nThere are some dplyr-style implementations in Python, that also include a pipe. One is siuba but it does not seem to be actively maintained. Another one is dfply, which has not been updated for 7 years and counting…\nSo, rather than being frustrated about this, I suggest we accept the differences between the two languages and move on! :-)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html#chaining-different-commands",
    "href": "materials/da3-06-chaining-operations.html#chaining-different-commands",
    "title": "9  Chaining operations",
    "section": "9.4 Chaining different commands",
    "text": "9.4 Chaining different commands\nWe can extend this concept a bit further, beyond just the creation of new columns.\n\nRPython\n\n\n\nsurveys |&gt; \n  select(record_id, weight) |&gt;  # select columns\n  rename(weight_g = weight) |&gt;  # rename weight\n  head()                        # show the first few rows\n\n# A tibble: 6 × 2\n  record_id weight_g\n      &lt;dbl&gt;    &lt;dbl&gt;\n1         1       NA\n2         2       NA\n3         3       NA\n4         4       NA\n5         5       NA\n6         6       NA\n\n\n\n\n\n(\n  surveys\n  .loc[:, ['record_id', 'weight']]          # select columns\n  .rename(columns = {'weight': 'weight_g'}) # rename column\n  .head()                                   # show first few rows\n)\n\n   record_id  weight_g\n0          1       NaN\n1          2       NaN\n2          3       NaN\n3          4       NaN\n4          5       NaN",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html#exercises",
    "href": "materials/da3-06-chaining-operations.html#exercises",
    "title": "9  Chaining operations",
    "section": "9.5 Exercises",
    "text": "9.5 Exercises\n\n9.5.1 Chaining\n\n\n\n\n\n\nExerciseExercise 1 - Chaining\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/parasites.csv.\nThe fish_length variable is the length of the fish measured in centimeters. Do the following:\n\nselect all but the parasite_count column\nrename the fish_length column to fish_length_cm\ncreate a new column that contains the fish length in inches (divide by 2.54)\ndo all of this in a single chain\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nLoad the data, if needed.\n\nRPython\n\n\n\nparasites &lt;- read_csv(\"data/parasites.csv\")\n\n\n\n\nparasites = pd.read_csv(\"data/parasites.csv\")\n\n\n\n\nPerform chaining operations:\n\nRPython\n\n\n\nparasites |&gt; \n  select(lake, fish_length) |&gt; \n  rename(fish_length_cm = fish_length) |&gt; \n  mutate(fish_length_in = fish_length_cm / 2.54)\n\n# A tibble: 64 × 3\n   lake  fish_length_cm fish_length_in\n   &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 C               21.1           8.31\n 2 C               26.4          10.4 \n 3 C               18.9           7.44\n 4 B               27.2          10.7 \n 5 C               29            11.4 \n 6 B               24.2           9.53\n 7 B               31.2          12.3 \n 8 B               20.3           7.99\n 9 C               27            10.6 \n10 A               27            10.6 \n# ℹ 54 more rows\n\n\n\n\n\n(\n  parasites\n  .loc[:, [\"lake\", \"fish_length\"]]\n  .rename(columns = {\"fish_length\": \"fish_length_cm\"})\n  .assign(fish_length_in = lambda df: df[\"fish_length_cm\"] / 2.54)\n)\n\n   lake  fish_length_cm  fish_length_in\n0     C            21.1        8.307087\n1     C            26.4       10.393701\n2     C            18.9        7.440945\n3     B            27.2       10.708661\n4     C            29.0       11.417323\n..  ...             ...             ...\n59    B            26.5       10.433071\n60    A            24.6        9.685039\n61    C            27.1       10.669291\n62    A            25.9       10.196850\n63    C            33.9       13.346457\n\n[64 rows x 3 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.5.2 Converting dates: surveys\n\n\n\n\n\n\nExerciseExercise 2 - Converting dates\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll go back to using the data from data/surveys.csv.\nIn our data we have the day, month and year values in separate columns. This is a safe way of storing the data, but when it comes to plotting or working with the data it’s useful to have it in a different format.\nCreate two new columns:\n\nrecord_date that contains the full date in the format yyyy-mm-dd\nmonth_abbr that contains the month abbreviation (e.g. Jan, Feb, …)\nDo this using chains, so you don’t end up needing to save lots of intermediate objects\n\nFeel free to use new packages…\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can do this as follows.\n\nRPython\n\n\nIn R there is the lubridate package, which allows us to manipulate dates efficiently. After installing it, make sure to load it.\n\nwe use lubridate::make_date() to create the date column\nwe use the lubridate::month() function to create the month label, using abbreviations\n\n\nlibrary(lubridate)\n\nsurveys |&gt; \n  mutate(record_date = make_date(year, month, day)) |&gt; \n  mutate(month_abbr = month(month, label = TRUE, abbr = TRUE)) |&gt; \n  select(record_id, year, month, day, record_date, month_abbr)\n\n# A tibble: 35,549 × 6\n   record_id  year month   day record_date month_abbr\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;      &lt;ord&gt;     \n 1         1  1977     7    16 1977-07-16  Jul       \n 2         2  1977     7    16 1977-07-16  Jul       \n 3         3  1977     7    16 1977-07-16  Jul       \n 4         4  1977     7    16 1977-07-16  Jul       \n 5         5  1977     7    16 1977-07-16  Jul       \n 6         6  1977     7    16 1977-07-16  Jul       \n 7         7  1977     7    16 1977-07-16  Jul       \n 8         8  1977     7    16 1977-07-16  Jul       \n 9         9  1977     7    16 1977-07-16  Jul       \n10        10  1977     7    16 1977-07-16  Jul       \n# ℹ 35,539 more rows\n\n\n\n\nWe can use some functionality from the calendar package, as well as pandas.\n\nwe can use to_datetime() to convert the year, month and day columns to a date.\nwe can use calendar.month_abbr to convert the numeric value of month to a month abbreviation\n\n\nimport calendar\n\nsurveys = (\n    surveys\n    .assign(record_date = pd.to_datetime(surveys[['year', 'month', 'day']]))\n    .assign(month_abbr = surveys['month'].apply(lambda x: calendar.month_abbr[x]))\n)\n\n\nsurveys.head()\n\n   record_id  month  day  year  ...  hindfoot_length weight record_date  month_abbr\n0          1      7   16  1977  ...             32.0    NaN  1977-07-16         Jul\n1          2      7   16  1977  ...             33.0    NaN  1977-07-16         Jul\n2          3      7   16  1977  ...             37.0    NaN  1977-07-16         Jul\n3          4      7   16  1977  ...             36.0    NaN  1977-07-16         Jul\n4          5      7   16  1977  ...             35.0    NaN  1977-07-16         Jul\n\n[5 rows x 11 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-06-chaining-operations.html#summary",
    "href": "materials/da3-06-chaining-operations.html#summary",
    "title": "9  Chaining operations",
    "section": "9.6 Summary",
    "text": "9.6 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nIn Python we use method chaining, which apply a series of transformations to a pandas DataFrame.\nIn R we have dedicated pipe symbols and we can use |&gt; (built-in) or %&gt;% (via magrittr package) to chain operations.\nBoth of these approaches allow us to run multiple lines of code sequentially, simplifying pipelines and making them easier to read.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Chaining operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html",
    "href": "materials/da3-07-manipulating-rows.html",
    "title": "10  Manipulating rows",
    "section": "",
    "text": "10.1 Context\nData sets can contain large quantities of observations. Often we are only interested in part of the data at a given time. We can deal with this by manipulating rows.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#setup_manipulating_rows",
    "href": "materials/da3-07-manipulating-rows.html#setup_manipulating_rows",
    "title": "10  Manipulating rows",
    "section": "10.2 Section setup",
    "text": "10.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 03_session. If needed, add the following code to the top of your script and run it.\nIn this section we will use a new package, called naniar, to visualise where we have missing data. Install it as follows:\n\n# install if needed\ninstall.packages(\"naniar\")\n\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# Package to visualise missing data\nlibrary(naniar)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 03_session. Add the following code to the first cell and run it.\nWe’ll use a new package, so install if needed from the terminal:\n\nmamba install conda-forge::missingno\n\nThen, if needed, add the following code to the top of your script and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\nimport numpy as np\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load missingno\nimport missingno as msno\n\nsurveys = pd.read_csv(\"data/surveys.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#arranging-data",
    "href": "materials/da3-07-manipulating-rows.html#arranging-data",
    "title": "10  Manipulating rows",
    "section": "10.3 Arranging data",
    "text": "10.3 Arranging data\nWe often want to order data in a certain way, for example ordering by date or in alphabetically. The example below illustrates how we would order data based on weight:\n\n\n\n  \n  \n                \n          \n            \n          \n  \n  \n    \n\n  \n  \n    \n    Previous\n  \n  \n    \n    Next\n  \n\n\n  Ordering by weight in ascending order (click to toggle).\n\n\nLet’s illustrate this with the surveys data set, arranging the data based on year.\n\nRPython\n\n\n\nsurveys |&gt; \n  arrange(year)\n\n# A tibble: 35,549 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         6     7    16  1977       1 PF         M                  14     NA\n 7         7     7    16  1977       2 PE         F                  NA     NA\n 8         8     7    16  1977       1 DM         M                  37     NA\n 9         9     7    16  1977       1 DM         F                  34     NA\n10        10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n\n\nIf we’d want to arrange the data in descending order (most recent to oldest), we would employ the desc() helper function:\n\nsurveys |&gt; \n  arrange(desc(year))\n\n# A tibble: 35,549 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1     33321     1    12  2002       1 DM         M                  38     44\n 2     33322     1    12  2002       1 DO         M                  37     58\n 3     33323     1    12  2002       1 PB         M                  28     45\n 4     33324     1    12  2002       1 AB         &lt;NA&gt;               NA     NA\n 5     33325     1    12  2002       1 DO         M                  35     29\n 6     33326     1    12  2002       2 OT         F                  20     26\n 7     33327     1    12  2002       2 OT         M                  20     24\n 8     33328     1    12  2002       2 OT         F                  21     22\n 9     33329     1    12  2002       2 DM         M                  37     47\n10     33330     1    12  2002       2 DO         M                  35     51\n# ℹ 35,539 more rows\n\n\nWe can read that bit of code as “take the surveys data set, send it to the arrange() function and ask it to arrange the data in descending order (using desc()) based on the year column”.\nWe can also combine this approach with multiple variables, for example arranging data based on descending year and (ascending) hindfoot length:\n\nsurveys |&gt; \n  arrange(desc(year), hindfoot_length)\n\n# A tibble: 35,549 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1     33647     3    14  2002       3 PF         M                   9      8\n 2     35301    12     8  2002       4 PF         M                  13      6\n 3     35506    12    31  2002       6 PF         M                  13      8\n 4     34281     6    15  2002      23 RM         F                  14      9\n 5     34663     7    14  2002      16 RM         M                  14      7\n 6     35101    11    10  2002       9 PF         M                  14      7\n 7     35487    12    29  2002      23 RO         F                  14     13\n 8     33429     2     9  2002       3 PF         M                  15      8\n 9     33535     2    10  2002      13 PF         F                  15      7\n10     33556     2    10  2002       5 RO         M                  15      9\n# ℹ 35,539 more rows\n\n\n\n\n\n\n\n\nTipShorthand for desc()\n\n\n\nWe can also use a shorthand for desc(), by using preceding the variable with -:\n\nsurveys |&gt; \n  arrange(-year)\n\n\n\n\n\n\nsurveys.sort_values(by = \"year\")\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n0              1      7   16  1977  ...          NL    M            32.0     NaN\n343          344     10   18  1977  ...          NL  NaN             NaN     NaN\n342          343     10   18  1977  ...          PF  NaN             NaN     NaN\n341          342     10   18  1977  ...          DM    M            34.0    25.0\n340          341     10   18  1977  ...          DS    M            50.0     NaN\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n34063      34064      5   16  2002  ...          PP    F            23.0    18.0\n34064      34065      5   16  2002  ...          DO    M            36.0    32.0\n34065      34066      5   16  2002  ...          DO    M            37.0    29.0\n34059      34060      5   16  2002  ...          DM    M            36.0    55.0\n35548      35549     12   31  2002  ...         NaN  NaN             NaN     NaN\n\n[35549 rows x 9 columns]\n\n\nIf we’d want to arrange the data in descending order (most recent to oldest), we would specify this with the ascending = False argument:\n\nsurveys.sort_values(by = \"year\", ascending = False)\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n35548      35549     12   31  2002  ...         NaN  NaN             NaN     NaN\n34060      34061      5   16  2002  ...          PB    M            27.0    37.0\n34066      34067      5   16  2002  ...          DM    F            36.0    50.0\n34065      34066      5   16  2002  ...          DO    M            37.0    29.0\n34064      34065      5   16  2002  ...          DO    M            36.0    32.0\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n341          342     10   18  1977  ...          DM    M            34.0    25.0\n342          343     10   18  1977  ...          PF  NaN             NaN     NaN\n343          344     10   18  1977  ...          NL  NaN             NaN     NaN\n344          345     11   12  1977  ...          DM    F            34.0    45.0\n0              1      7   16  1977  ...          NL    M            32.0     NaN\n\n[35549 rows x 9 columns]\n\n\nWe can also combine this approach with multiple variables, for example arranging data based on descending year and (ascending) hindfoot length:\n\nsurveys.sort_values(\n    by = [\"year\", \"hindfoot_length\"],\n    ascending = [False, True]\n)\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n33646      33647      3   14  2002  ...          PF    M             9.0     8.0\n35300      35301     12    8  2002  ...          PF    M            13.0     6.0\n35505      35506     12   31  2002  ...          PF    M            13.0     8.0\n34280      34281      6   15  2002  ...          RM    F            14.0     9.0\n34662      34663      7   14  2002  ...          RM    M            14.0     7.0\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n494          495     12   11  1977  ...          NL  NaN             NaN     NaN\n496          497     12   11  1977  ...          OT  NaN             NaN     NaN\n499          500     12   11  1977  ...          OT  NaN             NaN     NaN\n500          501     12   11  1977  ...          OT  NaN             NaN     NaN\n502          503     12   11  1977  ...         NaN  NaN             NaN     NaN\n\n[35549 rows x 9 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#finding-unique-values",
    "href": "materials/da3-07-manipulating-rows.html#finding-unique-values",
    "title": "10  Manipulating rows",
    "section": "10.4 Finding unique values",
    "text": "10.4 Finding unique values\nSometimes it is useful to retain rows with unique combinations of some of our variables (i.e. remove any duplicated rows). We often use this to get a quick view on what groups we have in the data.\nLet’s say we wanted to see which groups we have within the sex column in our surveys data set.\n\nRPython\n\n\nThis can be do this with the distinct() function.\n\nsurveys |&gt; \n  distinct(sex)\n\n# A tibble: 3 × 1\n  sex  \n  &lt;chr&gt;\n1 M    \n2 F    \n3 &lt;NA&gt; \n\n\n\n\nWe can do this by specifying which column we’d like to get the unique values from. We then use .drop_duplicates() to remove all the duplicate values:\n\nsurveys[[\"sex\"]].drop_duplicates()\n\n    sex\n0     M\n2     F\n13  NaN\n\n\n\n\n\nWe can also combine this approach across multiple variables. For example, we could check which sex groups appear across which years. We simply add year to our original code.\n\nRPython\n\n\n\nsurveys |&gt; \n  distinct(sex, year)\n\n# A tibble: 78 × 2\n   sex    year\n   &lt;chr&gt; &lt;dbl&gt;\n 1 M      1977\n 2 F      1977\n 3 &lt;NA&gt;   1977\n 4 &lt;NA&gt;   1978\n 5 M      1978\n 6 F      1978\n 7 F      1979\n 8 M      1979\n 9 &lt;NA&gt;   1979\n10 M      1980\n# ℹ 68 more rows\n\n\n\n\n\nsurveys[[\"sex\", \"year\"]].drop_duplicates()\n\n       sex  year\n0        M  1977\n2        F  1977\n13     NaN  1977\n503    NaN  1978\n504      M  1978\n...    ...   ...\n31711    M  2001\n31748  NaN  2001\n33320    M  2002\n33323  NaN  2002\n33325    F  2002\n\n[78 rows x 2 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#filtering-by-condition",
    "href": "materials/da3-07-manipulating-rows.html#filtering-by-condition",
    "title": "10  Manipulating rows",
    "section": "10.5 Filtering by condition",
    "text": "10.5 Filtering by condition\nOften we want to filter our data based on specific conditions / properties in the data. For example, in our data set you might want to filter for certain years, a specific weight range or only get all the observations for the first 100 record IDs.\nBefore we delve into this, it is important to understand that when we set a condition like above, the output is a logical vector. Let’s illustrate this with a very basic example.\nAssume we have a series of year values, stored in an object called some_years. If we’d be interested in all the values where the year is before 2000, we would do the following:\n\nRPython\n\n\n\nsome_years &lt;- c(1985, 1990, 1999, 1995, 2010, 2000)\nsome_years &lt; 2000\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\n\nWhat is happening is that, for each value in some_years, the computer checks against the condition: is that value smaller than 2000? If it is, it returns TRUE. If not, it returns FALSE.\n\n\nFor this example we’ll keep using pandas, for consistency. Since we’re only dealing with a simple, one-dimensional bunch of data, we create a pandas Series:\n\nsome_years = pd.Series([1985, 1990, 1999, 1995, 2010, 2000])\nresult = some_years &lt; 2000\n\nresult\n\n0     True\n1     True\n2     True\n3     True\n4    False\n5    False\ndtype: bool\n\n\nWhat is happening is that, for each value in some_years, the computer checks against the condition: is that value smaller than 2000? If it is, it returns True. If not, it returns False.\n\n\n\nWe can expand on this and supply multiple conditions. We do this by using the logical operators & (AND) and | (OR). For example, if we wanted the years between 1990 and 2000:\n\nRPython\n\n\n\n# both conditions have to be true\nsome_years &gt; 1990 & some_years &lt; 2000\n\n[1] FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\nAnd if we wanted the years below 1990 or above 2000, then:\n\n# only one or the other of the conditions has to be true\nsome_years &lt; 1990 | some_years &gt; 2000\n\n[1]  TRUE FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n\n# both conditions have to be true\nresult = (some_years &gt; 1990) & (some_years &lt; 2000)\n\nresult\n\n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\ndtype: bool\n\n\nAnd if we wanted the years below 1990 or above 2000, then:\n\n# only one or the other of the conditions has to be true\nresult = (some_years &lt; 1990) | (some_years &gt; 2000)\n\nresult\n\n0     True\n1    False\n2    False\n3    False\n4     True\n5    False\ndtype: bool\n\n\n\n\n\nThis concept is also applied to tables. We could filter across all rows in the surveys data set, for a hindfoot_length of larger than 31 mm. Conceptually, the computer does what is displayed in the Figure 10.1.\n\n\n\n\n\n\nFigure 10.1: The logic behind filtering: for each row the condition is checked (here: hindfoot_length &gt; 31). If the outcome is TRUE then the row is returned.\n\n\n\nWe implement this as follows.\n\nRPython\n\n\n\nsurveys |&gt; \n  filter(hindfoot_length &gt; 31)\n\n# A tibble: 15,729 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         8     7    16  1977       1 DM         M                  37     NA\n 7         9     7    16  1977       1 DM         F                  34     NA\n 8        11     7    16  1977       5 DS         F                  53     NA\n 9        12     7    16  1977       7 DM         M                  38     NA\n10        13     7    16  1977       3 DM         M                  35     NA\n# ℹ 15,719 more rows\n\n\n\n\n\nsurveys[surveys[\"hindfoot_length\"] &gt; 31]\n\n       record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n0              1      7   16  1977  ...          NL   M            32.0     NaN\n1              2      7   16  1977  ...          NL   M            33.0     NaN\n2              3      7   16  1977  ...          DM   F            37.0     NaN\n3              4      7   16  1977  ...          DM   M            36.0     NaN\n4              5      7   16  1977  ...          DM   M            35.0     NaN\n...          ...    ...  ...   ...  ...         ...  ..             ...     ...\n35533      35534     12   31  2002  ...          DM   M            37.0    56.0\n35534      35535     12   31  2002  ...          DM   M            37.0    53.0\n35535      35536     12   31  2002  ...          DM   F            35.0    42.0\n35536      35537     12   31  2002  ...          DM   F            36.0    46.0\n35547      35548     12   31  2002  ...          DO   M            36.0    51.0\n\n[15729 rows x 9 columns]\n\n\nAnother option would be to use the .query() function. This is functionally equivalent, but a bit easier to read.\n\nsurveys.query(\"hindfoot_length &gt; 31\")\n\n       record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n0              1      7   16  1977  ...          NL   M            32.0     NaN\n1              2      7   16  1977  ...          NL   M            33.0     NaN\n2              3      7   16  1977  ...          DM   F            37.0     NaN\n3              4      7   16  1977  ...          DM   M            36.0     NaN\n4              5      7   16  1977  ...          DM   M            35.0     NaN\n...          ...    ...  ...   ...  ...         ...  ..             ...     ...\n35533      35534     12   31  2002  ...          DM   M            37.0    56.0\n35534      35535     12   31  2002  ...          DM   M            37.0    53.0\n35535      35536     12   31  2002  ...          DM   F            35.0    42.0\n35536      35537     12   31  2002  ...          DM   F            36.0    46.0\n35547      35548     12   31  2002  ...          DO   M            36.0    51.0\n\n[15729 rows x 9 columns]\n\n\n\n\n\nThis only keeps the observations where hindfoot_length &gt; 31, in this case 15729 observations.\n\n\n\n\n\n\nImportantConditional operators\n\n\n\n\nRPython\n\n\nTo set filtering conditions, use the following relational operators:\n\n&gt; is greater than\n&gt;= is greater than or equal to\n&lt; is less than\n&lt;= is less than or equal to\n== is equal to\n!= is different from\n%in% is contained in\n\nTo combine conditions, use the following logical operators:\n\n& AND\n| OR\n\nSome functions return logical results and can be used in filtering operations:\n\nis.na(x) returns TRUE if a value in x is missing\n\nThe ! can be used to negate a logical condition:\n\n!is.na(x) returns TRUE if a value in x is NOT missing\n!(x %in% y) returns TRUE if a value in x is NOT present in y\n\n\n\nTo set filtering conditions, use the following relational operators:\n\n&gt; is greater than\n&gt;= is greater than or equal to\n&lt; is less than\n&lt;= is less than or equal to\n== is equal to\n!= is different from\n.isin([...]) is contained in\n\nTo combine conditions, use the following logical operators:\n\n& AND\n| OR\n\nSome functions return logical results and can be used in filtering operations:\n\ndf[\"x\"].isna() returns True if a value in x is missing\n\nThe ~ (bitwise NOT) can be used to negate a logical condition:\n\n~df[\"x\"].isna() returns True if a value in x is NOT missing\n~df[\"x\"].isin([\"y\"]) returns True if a value in x is NOT present in \"y\"",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#missingdata-revisited",
    "href": "materials/da3-07-manipulating-rows.html#missingdata-revisited",
    "title": "10  Manipulating rows",
    "section": "10.6 Missing data revisited",
    "text": "10.6 Missing data revisited\nIt’s important to carefully consider how to deal with missing data, as we have previously seen. It’s easy enough to filter out all rows that contain missing data, however this is rarely the best course of action, because you might accidentally throw out useful data in columns that you’ll need later.\nFurthermore, it’s often a good idea to see if there is any structure in your missing data. Maybe certain variables are consistently absent, which could tell you something about your data.\n\nRPython\n\n\nWe could filter out all the missing weight values as follows:\n\nsurveys |&gt; \n  filter(!is.na(weight))\n\n# A tibble: 32,283 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1        63     8    19  1977       3 DM         M                  35     40\n 2        64     8    19  1977       7 DM         M                  37     48\n 3        65     8    19  1977       4 DM         F                  34     29\n 4        66     8    19  1977       4 DM         F                  35     46\n 5        67     8    19  1977       7 DM         M                  35     36\n 6        68     8    19  1977       8 DO         F                  32     52\n 7        69     8    19  1977       2 PF         M                  15      8\n 8        70     8    19  1977       3 OX         F                  21     22\n 9        71     8    19  1977       7 DM         F                  36     35\n10        74     8    19  1977       8 PF         M                  12      7\n# ℹ 32,273 more rows\n\n\n\n\n\nsurveys.dropna(subset = [\"weight\"])\n\n       record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n62            63      8   19  1977  ...          DM   M            35.0    40.0\n63            64      8   19  1977  ...          DM   M            37.0    48.0\n64            65      8   19  1977  ...          DM   F            34.0    29.0\n65            66      8   19  1977  ...          DM   F            35.0    46.0\n66            67      8   19  1977  ...          DM   M            35.0    36.0\n...          ...    ...  ...   ...  ...         ...  ..             ...     ...\n35540      35541     12   31  2002  ...          PB   F            24.0    31.0\n35541      35542     12   31  2002  ...          PB   F            26.0    29.0\n35542      35543     12   31  2002  ...          PB   F            27.0    34.0\n35546      35547     12   31  2002  ...          RM   F            15.0    14.0\n35547      35548     12   31  2002  ...          DO   M            36.0    51.0\n\n[32283 rows x 9 columns]\n\n\n\n\n\n\nRPython\n\n\nWe can combine this for multiple columns:\n\nsurveys |&gt; \n  filter(!is.na(weight) & !is.na(hindfoot_length))\n\n# A tibble: 30,738 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1        63     8    19  1977       3 DM         M                  35     40\n 2        64     8    19  1977       7 DM         M                  37     48\n 3        65     8    19  1977       4 DM         F                  34     29\n 4        66     8    19  1977       4 DM         F                  35     46\n 5        67     8    19  1977       7 DM         M                  35     36\n 6        68     8    19  1977       8 DO         F                  32     52\n 7        69     8    19  1977       2 PF         M                  15      8\n 8        70     8    19  1977       3 OX         F                  21     22\n 9        71     8    19  1977       7 DM         F                  36     35\n10        74     8    19  1977       8 PF         M                  12      7\n# ℹ 30,728 more rows\n\n\n\n\n\nsurveys.dropna(subset = [\"weight\", \"hindfoot_length\"])\n\n       record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n62            63      8   19  1977  ...          DM   M            35.0    40.0\n63            64      8   19  1977  ...          DM   M            37.0    48.0\n64            65      8   19  1977  ...          DM   F            34.0    29.0\n65            66      8   19  1977  ...          DM   F            35.0    46.0\n66            67      8   19  1977  ...          DM   M            35.0    36.0\n...          ...    ...  ...   ...  ...         ...  ..             ...     ...\n35540      35541     12   31  2002  ...          PB   F            24.0    31.0\n35541      35542     12   31  2002  ...          PB   F            26.0    29.0\n35542      35543     12   31  2002  ...          PB   F            27.0    34.0\n35546      35547     12   31  2002  ...          RM   F            15.0    14.0\n35547      35548     12   31  2002  ...          DO   M            36.0    51.0\n\n[30738 rows x 9 columns]\n\n\n\n\n\nWe can also combine that with other filters.\n\nRPython\n\n\n\nsurveys |&gt; \n  filter(!is.na(weight) & !is.na(hindfoot_length)) |&gt; \n  filter(hindfoot_length &gt; 40)\n\n# A tibble: 2,045 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1       357    11    12  1977       9 DS         F                  50    117\n 2       362    11    12  1977       1 DS         F                  51    121\n 3       367    11    12  1977      20 DS         M                  51    115\n 4       377    11    12  1977       9 DS         F                  48    120\n 5       381    11    13  1977      17 DS         F                  48    118\n 6       383    11    13  1977      11 DS         F                  52    126\n 7       385    11    13  1977      17 DS         M                  50    132\n 8       392    11    13  1977      11 DS         F                  53    122\n 9       394    11    13  1977       4 DS         F                  48    107\n10       398    11    13  1977       4 DS         F                  50    115\n# ℹ 2,035 more rows\n\n\n\n\nTo do this, we might prefer to use a slightly syntax: the .notna(). This allows us to chain operations a bit cleaner, making our code easier to read:\n\nsurveys[\n    (surveys[\"weight\"].notna()) &\n    (surveys[\"hindfoot_length\"].notna()) &\n    (surveys[\"hindfoot_length\"] &gt; 40)\n]\n\n       record_id  month  day  year  ...  species_id sex hindfoot_length  weight\n356          357     11   12  1977  ...          DS   F            50.0   117.0\n361          362     11   12  1977  ...          DS   F            51.0   121.0\n366          367     11   12  1977  ...          DS   M            51.0   115.0\n376          377     11   12  1977  ...          DS   F            48.0   120.0\n380          381     11   13  1977  ...          DS   F            48.0   118.0\n...          ...    ...  ...   ...  ...         ...  ..             ...     ...\n29572      29573      5   15  1999  ...          DS   M            50.0    96.0\n29705      29706      6   12  1999  ...          DS   M            49.0   102.0\n30424      30425      3    4  2000  ...          DO   F            64.0    35.0\n30980      30981      7    1  2000  ...          DO   F            42.0    46.0\n33367      33368      1   12  2002  ...          PB   M            47.0    27.0\n\n[2045 rows x 9 columns]\n\n\n\n\n\nOften it’s not that easy to get a sense of how missing data are distributed in the data set. We can use summary statistics and visualisations to get a better sense.\n\nRPython\n\n\nThe easiest way of getting some numbers on the missing data is by using the summary() function, which will report the number of NA’s for each column (for example: see the hindfoot_length column):\n\nsummary(surveys)\n\n   record_id         month             day             year         plot_id    \n Min.   :    1   Min.   : 1.000   Min.   : 1.00   Min.   :1977   Min.   : 1.0  \n 1st Qu.: 8888   1st Qu.: 4.000   1st Qu.: 9.00   1st Qu.:1984   1st Qu.: 5.0  \n Median :17775   Median : 6.000   Median :16.00   Median :1990   Median :11.0  \n Mean   :17775   Mean   : 6.478   Mean   :15.99   Mean   :1990   Mean   :11.4  \n 3rd Qu.:26662   3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.:1997   3rd Qu.:17.0  \n Max.   :35549   Max.   :12.000   Max.   :31.00   Max.   :2002   Max.   :24.0  \n                                                                               \n  species_id            sex            hindfoot_length     weight      \n Length:35549       Length:35549       Min.   : 2.00   Min.   :  4.00  \n Class :character   Class :character   1st Qu.:21.00   1st Qu.: 20.00  \n Mode  :character   Mode  :character   Median :32.00   Median : 37.00  \n                                       Mean   :29.29   Mean   : 42.67  \n                                       3rd Qu.:36.00   3rd Qu.: 48.00  \n                                       Max.   :70.00   Max.   :280.00  \n                                       NA's   :4111    NA's   :3266    \n\n\nOften it’s nice to visualise where your missing values are, to see if there are any patterns that are obvious. There are several packages in R that can do this, of which naniar is one.\n\n# install if needed\ninstall.packages(\"naniar\")\n\n# load the library\nlibrary(naniar)\n\n\n# visualise missing data\nvis_miss(surveys)\n\n\n\n\n\n\n\n\n\n\nThe easiest way of counting the number of missing values in Python is by combining .isna() and .sum():\n\n# count missing values\nsurveys.isna().sum()\n\nrecord_id             0\nmonth                 0\nday                   0\nyear                  0\nplot_id               0\nspecies_id          763\nsex                2511\nhindfoot_length    4111\nweight             3266\ndtype: int64\n\n\nOften it’s nice to visualise where your missing values are, to see if there are any patterns that are obvious. There are several packages in Python that can do this, of which missingno is one.\n\nimport missingno as msno\n\n\n# visual matrix\nmsno.matrix(surveys)",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#exercises",
    "href": "materials/da3-07-manipulating-rows.html#exercises",
    "title": "10  Manipulating rows",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\n\n10.7.1 Filtering data: infections\n\n\n\n\n\n\nExerciseExercise 1 - Filtering data\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/infections.csv.\nUsing these data, please answer the following questions:\n\nHow many patients had a bacterial infection, but were not admitted to the ICU (Intensive Care Unit)?\nIn patients older than 65, how many have CRP levels between 10-15 mg/L or 50-60 mg/L?\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nFirst we load the data.\n\nRPython\n\n\n\ninfections &lt;- read_csv(\"data/infections.csv\")\n\n\n\n\ninfections = pd.read_csv(\"data/infections.csv\")\n\n\n\n\n\n1. Bacterial and non-ICU\nTo get the data we want, we need to filter for rows where:\n\ninfection_type contains the text \"bacterial\"\nicu_admission is equal to TRUE (R) / True (Python)\n\n\nRPython\n\n\n\ninfections |&gt; \n  filter(infection_type == \"bacterial\" & icu_admission == FALSE) |&gt; \n  nrow()\n\n[1] 212\n\n\n\n\n\n(\n  infections\n  .query(\"infection_type == 'bacterial' and icu_admission == False\")\n  .shape[0]\n)\n\n212\n\n\n\n\n\n\n\n2. Patients in 65+ group with certain CRP levels\nHere we need to filter for three things:\n\nage_group values of \"65+\" (string/text)\ncrp_level &gt; 10 and crp_level &lt; 15\ncrp_level &gt; 50 and crp_level &lt; 60\n\n\nRPython\n\n\n\ninfections |&gt; \n  filter(age_group == \"65+\") |&gt; \n  filter(crp_level &gt; 10 & crp_level &lt; 15 |\n           crp_level &gt; 50 & crp_level &lt; 60) |&gt; \n  nrow()\n\n[1] 91\n\n\n\n\nUsing .query():\n\n(\n  infections\n  .query(\n    \"age_group == '65+' and ((10 &lt; crp_level &lt; 15) or (50 &lt; crp_level &lt; 60))\")\n    .shape[0]\n)\n\n91\n\n\nWe can also use a more explicit version using normal subsetting (which becomes quite convoluted, quite quickly!):\n\n\n91\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.7.2 Creating categories: risk_group\n\n\n\n\n\n\nExerciseExercise 2 - Creating categories\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll again be using the data from data/infections.csv.\nWe are interested in creating a new category, based on the original data. Patients that are over 65 years of age and have a relatively high systolic blood pressure (over 140 mmHg) are considered “high” risk.\nCreate a new column that contains the risk group (high, low) and count how many patients fall within each category.\nWe’ve not yet covered counting observations, so you’ll have to do a quick search!\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nMake sure you consider how to deal with missing data!\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe can do this in two steps:\n\nCreate a new column that contains the risk group\nCount the number of rows within each group\n\n\nRPython\n\n\n\ninfections |&gt;\n  mutate(risk_group = case_when(\n    # if age_group AND systolic_pressure are not missing\n    !is.na(age_group) & !is.na(systolic_pressure) &\n      # AND age_group is 65+ AND systolic_pressure &gt; 140\n      # then assign a risk group \"high\"\n      age_group == \"65+\" & systolic_pressure &gt; 140 ~ \"high\",\n    # if age_group AND systolic_pressure are not missing\n    # then assign a risk group \"low\"\n    !is.na(age_group) & !is.na(systolic_pressure) ~ \"low\",\n    # otherwise, assign the missing value character\n    TRUE ~ NA_character_)) |&gt;\n  count(risk_group)\n\n# A tibble: 3 × 2\n  risk_group     n\n  &lt;chr&gt;      &lt;int&gt;\n1 high          32\n2 low         1226\n3 &lt;NA&gt;         142\n\n\nA more intuitive way would have been to do something like this:\n\ninfections |&gt; \n  mutate(risk_group = ifelse(age_group == \"65+\" & systolic_pressure &gt; 140, \"high\", \"low\")) |&gt; \n  count(risk_group)\n\n# A tibble: 3 × 2\n  risk_group     n\n  &lt;chr&gt;      &lt;int&gt;\n1 high          32\n2 low         1335\n3 &lt;NA&gt;          33\n\n\nWhich would read something like “if age group is 65+ and systolic pressure is more than 140, assign”high”, otherwise assign “low”. Which, at first glance, should really result in the same assignments.\nHowever, the results here are different. Why? This is because of the way that missing values are being handled.\nIn the latter, simpler, syntax the condition where NA is returned is different. Let’s look a bit closer into how R handles these conditions. Consider the following:\n\nTRUE  & TRUE    # TRUE\nTRUE  & FALSE   # FALSE\nFALSE & FALSE   # FALSE\nNA    & TRUE    # NA\nNA    & FALSE   # FALSE   &lt;-- important\nNA    & NA      # NA\n\nWhenever the ifelse() statement results in TRUE, it returns \"high\". When it results in a FALSE, it returns \"low\". If it encounters NA, it returns NA.\nThis means that if either of the value of age_group or systolic_pressure is missing (NA) AND the other condition is FALSE (i.e. age_group is not \"65+\" or systolic_pressure is not &gt; 140) then the resulting output is FALSE. Then, \"low\" is assigned.\nThis doesn’t make sense in our context. Because if either of the two values are missing we shouldn’t even continue with the conditional statement, because a patient with high blood pressure - but no data on the age group - should not be classified as \"high\" in the risk group.\n\n\n\n# The default risk_group value is missing data\ninfections[\"risk_group\"] = pd.Series(pd.NA, dtype = \"object\")\n\n# Assign \"high\" when both conditions are satisfied\ninfections.loc[\n    (infections[\"age_group\"] == \"65+\") & (infections[\"systolic_pressure\"] &gt; 140),\n    \"risk_group\"\n] = \"high\"\n\n# Assign \"low\" when both values are known AND condition is not met\n# using ~(...), which negates / inverses the condition\ninfections.loc[\n    (infections[\"age_group\"].notna()) & \n    (infections[\"systolic_pressure\"].notna()) & \n    ~((infections[\"age_group\"] == \"65+\") & (infections[\"systolic_pressure\"] &gt; 140)),\n    \"risk_group\"\n] = \"low\"\n\n# Count occurrences\nrisk_counts = infections[\"risk_group\"].value_counts(dropna = False).reset_index()\nrisk_counts.columns = [\"risk_group\", \"n\"]\n\nrisk_counts\n\n  risk_group     n\n0        low  1226\n1        NaN   142\n2       high    32\n\n\nNote that when we define the initial risk_group column, we fill the column with missing values (as a default). We do this with:\ninfections[\"risk_group\"] = pd.Series(pd.NA, dtype = \"object\")\nWe can read this as “create a column called risk_group, in the infections data set, which contains a pandas Series, filled with missing data (pd.NA) and the type of column is text (dtype = \"object\").\nFollowing that, we then populate the column based on the conditions we check against. We have to explicitly check if we are dealing with missing data, or not.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-07-manipulating-rows.html#summary",
    "href": "materials/da3-07-manipulating-rows.html#summary",
    "title": "10  Manipulating rows",
    "section": "10.8 Summary",
    "text": "10.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nRPython\n\n\n\nWe use the arrange() function to order data, and can reverse the order using desc().\nUnique rows can be retained with distinct().\nWe use filter() to choose rows based on conditions.\nConditions can be set using several operators: &gt;, &gt;=, &lt;, &lt;=, ==, !=, %in%.\nConditions can be combined using & (AND) and | (OR).\nThe function is.na() can be used to identify missing values. It can be negated as !is.na() to find non-missing values.\nWe can visualise missing data using the vis_miss() function from the naniar package.\n\n\n\n\nWe can use the .sort_values() method to order data, specifying the ascending = argument as True or False to control the order.\nThe .drop_duplicates() method allows us to retain unique rows.\nWe use subsetting (e.g. surveys[surveys[\"hindfoot_length\"] &gt; 31]) together with conditions to filter data. – Conditions can be set using several operators: &gt;, &gt;=, &lt;, &lt;=, ==, !=, .isin([...]).\nConditions can be combined using & (AND) and | (OR).\nThe function .isna() can be used to identify missing values. It can be negated using ~ to find non-missing values (e.g. surveys[~surveys[\"weight\"].isna()].\nWe can visualise missing data using the msno.matrix function from the missingno package.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulating rows</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html",
    "href": "materials/da3-08-grouped-operations.html",
    "title": "11  Grouped operations",
    "section": "",
    "text": "11.1 Context\nWe’ve done different types of operations, all on the entire data set. Sometimes there is structure within the data, such as different groups (e.g. genotypes, patient cohorts, geographical areas etc). We might then want information on a group-by-group basis.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#setup_grouped_operations",
    "href": "materials/da3-08-grouped-operations.html#setup_grouped_operations",
    "title": "11  Grouped operations",
    "section": "11.2 Section setup",
    "text": "11.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 03_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\ninfections &lt;- read_csv(\"data/infections.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 03_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# A package to deal with arrays\nimport numpy as np\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\ninfections = pd.read_csv(\"data/infections.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#split-apply-combine",
    "href": "materials/da3-08-grouped-operations.html#split-apply-combine",
    "title": "11  Grouped operations",
    "section": "11.3 Split-apply-combine",
    "text": "11.3 Split-apply-combine\nConceptually, this kind of operation can be referred to as split-apply-combine, because we split the data, apply some function and then combine the outcome.\nLet’s illustrate this with an example. Figure 11.1 shows a hypothetical data set, where we have temperature and rainfall measurements for different cities.\n\n\n\n\n\n\nFigure 11.1: An example of a table with groups\n\n\n\nLet’s assume we were interested in the average temperature for each city. We would have to do the following:\n\nSplit the data by city\nCalculate the average temperature\nCombine the outcome together in a new table\n\nThis is visualised in Figure 11.2.\n\n\n\n\n\n\nFigure 11.2: Split-apply-combine",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#summary-operations",
    "href": "materials/da3-08-grouped-operations.html#summary-operations",
    "title": "11  Grouped operations",
    "section": "11.4 Summary operations",
    "text": "11.4 Summary operations\nLet’s put this into practice with our data set.\n\n11.4.1 Summarising data\nA common task in data analysis is to summarise variables to get the mean and the variation around it.\nFor example, let’s calculate what the mean and standard deviation are for weight.\n\nRPython\n\n\nWe can achieve this task using the summarise() function.\n\nsurveys |&gt; \n  summarise(weight_mean = mean(weight, na.rm  = TRUE),\n            weight_sd = sd(weight, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  weight_mean weight_sd\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        42.7      36.6\n\n\nA couple of things to notice:\nThe output of summarise is a new table, where each column is named according to the input to summarise().\nWithin summarise() we should use functions for which the output is a single value. Also notice that, above, we used the na.rm option within the summary functions, so that they ignored missing values when calculating the respective statistics.\n\n\nFor these kind of summary statistics we can use .agg() - the aggregate function in pandas. You can apply this to a DataFrame or Series. It works on standard summary functions, listed below.\n\nsurveys[\"weight\"].agg(\n    weight_mean = \"mean\",\n    weight_sd = \"std\"\n)\n\nweight_mean    42.672428\nweight_sd      36.631259\nName: weight, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\nTipSummary functions\n\n\n\n\nRPython\n\n\nThere are many functions whose input is a vector (or a column in a table) and the output is a single number. Here are several common ones:\n\n\n\n\n\n\n\n\nR\nExample\nDescription\n\n\n\n\nmean\nmean(x, na.rm = TRUE)\nArithmetic mean\n\n\nmedian\nmedian(x, na.rm = TRUE)\nMedian\n\n\nsd\nsd(x, na.rm = TRUE)\nStandard deviation\n\n\nvar\nvar(x, na.rm = TRUE)\nVariance\n\n\nmad\nmad(x, na.rm = TRUE)\nMedian absolute deviation\n\n\nmin\nmin(x, na.rm = TRUE)\nMinimum value\n\n\nmax\nmax(x, na.rm = TRUE)\nMaximum value\n\n\nsum\nsum(x, na.rm = TRUE)\nSum of all values\n\n\nn_distinct\nn_distinct(x)\nNumber of distinct (unique) values\n\n\n\nAll of these have the option na.rm, which tells the function remove missing values before doing the calculation.\n\n\n\n\n\n\n\n\n\n\nPython (pandas)\nExample (in .agg())\nDescription\n\n\n\n\n\"mean\"\ndf[\"x\"].agg(\"mean\")\nArithmetic mean\n\n\n\"median\"\ndf[\"x\"].agg(\"median\")\nMedian\n\n\n\"std\"\ndf[\"x\"].agg(\"std\")\nStandard deviation\n\n\n\"var\"\ndf[\"x\"].agg(\"var\")\nVariance\n\n\n\"mad\"\ndf[\"x\"].agg(\"mad\")\nMean absolute deviation\n\n\n\"min\"\ndf[\"x\"].agg(\"min\")\nMinimum value\n\n\n\"max\"\ndf[\"x\"].agg(\"max\")\nMaximum value\n\n\n\"sum\"\ndf[\"x\"].agg(\"sum\")\nSum of all values\n\n\n\"nunique\"\ndf[\"x\"].agg(\"nunique\")\nNumber of distinct (unique) values\n\n\n\n\n\n\n\n\n\n\n11.4.2 Grouped summaries\nIn most cases we want to calculate summary statistics across groups of our data.\n\nRPython\n\n\nWe can achieve this by combining summarise() with the group_by() function. For example, let’s modify the previous example to calculate the summary for each sex group:\n\nsurveys |&gt; \n  group_by(sex) |&gt; \n  summarise(weight_mean = mean(weight, na.rm  = TRUE),\n            weight_sd = sd(weight, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  sex   weight_mean weight_sd\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 F            42.2      36.8\n2 M            43.0      36.2\n3 &lt;NA&gt;         64.7      62.2\n\n\nThe table output now includes both the columns we defined within summarise() as well as the grouping columns defined within group_by().\n\n\n\nsurveys.groupby(\"sex\")[\"weight\"].agg(\n    weight_mean = \"mean\",\n    weight_sd = \"std\"\n).reset_index()\n\n  sex  weight_mean  weight_sd\n0   F    42.170555  36.847958\n1   M    42.995379  36.184981",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#counting-data",
    "href": "materials/da3-08-grouped-operations.html#counting-data",
    "title": "11  Grouped operations",
    "section": "11.5 Counting data",
    "text": "11.5 Counting data\nCounting or tallying data is an extremely useful way of getting to know your data better.\n\n11.5.1 Simple counting\n\nRPython\n\n\nWe can use the count() function from dplyr to count data. It always returns the number of rows it counts.\nFor example, this gives us the total number of observations (rows) in our data set:\n\ncount(surveys)\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1 35549\n\n\n\n\nWe can use the .shape attribute. In pandas, each DataFrame has a .shape attribute that returns a tuple in the format (rows, columns).\nSo, .shape[0] will return the number of rows, whereas .shape[1] returns the number of columns.\nFor our surveys DataFrame we then get the number of rows by:\n\nsurveys.shape[0]\n\n35549\n\n\n\n\n\nWe can also use that in combination with a conditional statement. For example, if we’re interested in all the observations from the year 1982.\n\nRPython\n\n\n\n# count the observations from the year 1982\nsurveys |&gt; \n  filter(year == 1982) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  1978\n\n\n\n\n\nsurveys[surveys[\"year\"] == 1982].shape[0]\n\n1978\n\n\nOr, slightly easier to read, with the .query() function:\n\nsurveys.query(\"year == 1982\").shape[0]\n\n1978\n\n\n\n\n\n\n\n11.5.2 Counting by group\nCounting really comes into its own when we’re combining this with some grouping. For example, we might be interested in the number of observations for each year.\n\nRPython\n\n\n\nsurveys |&gt; \n  count(year)\n\n# A tibble: 26 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  1977   503\n 2  1978  1048\n 3  1979   719\n 4  1980  1415\n 5  1981  1472\n 6  1982  1978\n 7  1983  1673\n 8  1984   981\n 9  1985  1438\n10  1986   942\n# ℹ 16 more rows\n\n\nWe can also easily visualise this (we can pipe straight into ggplot()). We use geom_col() to create a bar chart of the number of observations per year. We count by sex and use this variable to fill the colour of the bars.\n\nsurveys |&gt; \n  count(sex, year) |&gt; \n  ggplot(aes(x = year, y = n, fill = sex)) +\n  geom_col()\n\n\n\n\n\n\n\nFigure 11.3: Number of observations per year, by sex.\n\n\n\n\n\n\n\n\n# Count number of rows for each year\ncounts = surveys.groupby(\"year\").size().reset_index(name = \"n\")\n\n# Look at the first few rows\ncounts.head()\n\n   year     n\n0  1977   503\n1  1978  1048\n2  1979   719\n3  1980  1415\n4  1981  1472\n\n\nLet’s expand this example a bit, where we count by two variables: sex and year. We then also plot the results, just to illustrate how useful that can be.\n\n# Count observations by sex and year\ncounts = surveys.groupby([\"sex\", \"year\"]).size().reset_index(name = \"n\")\n\n\np = (ggplot(counts, aes(x = \"year\", y = \"n\", fill = \"sex\")) +\n     geom_col())\n\np.show()\n\n\n\n\n\n\n\nFigure 11.4: Number of observations per year, by sex.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\nThis is not an exact equivalent to the plot above, because sns.barplot() gives dodged (side-by-side) groupings by default.\nThere is no native stacked bars in Seaborn, so you’d have to use the underlying matplotlib. We’re not covering this here, but feel free to search or use ChatGPT.\n\nsns.barplot(\n    data = counts,\n    x = \"year\",\n    y = \"n\",\n    hue = \"sex\"   # fill = sex in ggplot\n)\n\nplt.title(\"Counts by year and sex\")\nplt.show()\n\n\n\n\n\n\n\nFigure 11.5: Number of observations per year, by sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantCounting within a summary pipeline\n\n\n\nOften we want to do counting when we’re creating summaries. Let’s illustrate this with an example where we take the observations from 1981 and 1982, then calculate the mean weight and count the number of observations.\n\nRPython\n\n\nThe count() function can’t be used within summarise(), but there is a special helper function called n(). Look at the following example, where we group by year, filter the data, create some summary statistic and also count the number of rows within each group.\n\nsurveys |&gt; \n  group_by(year) |&gt;                                   # group the data\n  filter(year %in% c(1981, 1982)) |&gt;                  # filter a subset of years\n  summarise(mean_weight = mean(weight, na.rm = TRUE), # calculate mean weight\n            n_obs = n()) |&gt;                           # number of rows\n  ungroup()                                           # drop the grouping\n\n# A tibble: 2 × 3\n   year mean_weight n_obs\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;\n1  1981        65.8  1472\n2  1982        53.8  1978\n\n\n\n\nWe need to do this in two steps:\n\nFilter out the relevant data\nCalculate the summary statistics\n\nHere we’re using (  ) around the pipeline, so we can break up the code into different lines. This aids with readability, but doesn’t change how the code works!\n\n# Group by year and summarise\nsummary = (\n     surveys\n    .query(\"year in [1981, 1982]\")         # filter data based on year\n    .groupby(\"year\")                       # group by year\n    .agg(                                  # create summary statistics\n        mean_weight = (\"weight\", \"mean\"),  # calculate mean weights\n        n_obs = (\"weight\", \"count\")        # count of non-NaN weights\n    )\n    .reset_index()                         # converts index into regular column\n)\n\nprint(summary)\n\n   year  mean_weight  n_obs\n0  1981    65.843888   1358\n1  1982    53.765888   1841\n\n\n\n\n\n\n\n\n\n11.5.3 Counting missing data\nOh, missing data! How we’ve missed you. For something that isn’t there, is has quite the presence. But, it is an important consideration in data analysis. We’ve already seen how we can remove missing data from and also explored ways to visualise them.\n\nRPython\n\n\nWe have seen how to use the summary() function to find missing values. Here we’ll see (even more) ways to tally them.\nWe can use the is.na() function to great effect, within a summarise() pipeline. We can negate with !is.na() to find non-missing values. Again, using the sum() function then enables us to tally how many missing / non-missing values there are.\n\nsurveys |&gt; \n  summarise(obs_present = sum(!is.na(species_id)),    # count non-missing data\n            obs_absent = sum(is.na(species_id)),      # count missing data\n            n_obs = n(),                              # total number of rows\n            percentage_absent = \n              (obs_absent / n_obs) * 100) |&gt;          # percentage of missing data\n  ungroup()\n\n# A tibble: 1 × 4\n  obs_present obs_absent n_obs percentage_absent\n        &lt;int&gt;      &lt;int&gt; &lt;int&gt;             &lt;dbl&gt;\n1       34786        763 35549              2.15\n\n\n\n\nWe can use the .isna() and .notna() functions to great effect. Again, we’re using the .sum() function to tally the numbers.\n\nsummary = pd.DataFrame([{\n    \"obs_present\": surveys[\"species_id\"].notna().sum(),\n    \"obs_absent\": surveys[\"species_id\"].isna().sum(),\n    \"n_obs\": len(surveys),\n    \"percentage_absent\": surveys[\"species_id\"].isna().mean() * 100\n}])\n\nprint(summary)\n\n   obs_present  obs_absent  n_obs  percentage_absent\n0        34786         763  35549           2.146333",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#grouped-operations",
    "href": "materials/da3-08-grouped-operations.html#grouped-operations",
    "title": "11  Grouped operations",
    "section": "11.6 Grouped operations",
    "text": "11.6 Grouped operations\n\n11.6.1 Grouped filters\nSometimes it can be really handy to filter data, by group. In our surveys data, for example, you might be interested to find out what the minimum weight value is for each year. We can do that as follows:\n\nRPython\n\n\n\nsurveys |&gt; \n  group_by(year) |&gt; \n  filter(weight == min(weight, na.rm = TRUE)) |&gt; \n  ungroup()\n\n# A tibble: 94 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1       218     9    13  1977       1 PF         M                  13      4\n 2      1161     8     5  1978       6 PF         F                  16      6\n 3      1230     9     3  1978      19 PF         M                  16      6\n 4      1265     9     4  1978      15 PF         F                  16      6\n 5      1282     9     4  1978       4 PF         F                  16      6\n 6      1343    10     7  1978      11 PF         F                  15      6\n 7      1351    10     8  1978       2 PF         M                  15      6\n 8      1380    10     8  1978       3 PF         F                  16      6\n 9      1400    11     4  1978      19 PF         M                  15      6\n10      1427    11     4  1978      21 PF         F                  15      6\n# ℹ 84 more rows\n\n\nYou can see that this outputs the minimum value, but if there are multiple entries for each year (such as in 1978), multiple rows returned. If we only wanted to get a single row per minimum value, per year, then we can use slice(1). This slices the first row of each group:\n\nsurveys |&gt; \n  group_by(year) |&gt; \n  filter(weight == min(weight, na.rm = TRUE)) |&gt; \n  slice(1) |&gt; \n  ungroup()\n\n# A tibble: 26 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1       218     9    13  1977       1 PF         M                  13      4\n 2      1161     8     5  1978       6 PF         F                  16      6\n 3      1923     7    25  1979      18 PF         F                  NA      6\n 4      2506     2    25  1980       3 PF         F                  15      5\n 5      4052     4     5  1981       3 PF         F                  15      4\n 6      5346     2    22  1982      21 PF         F                  14      4\n 7      8736    12     8  1983      19 RM         M                  17      4\n 8      8809     2     4  1984      16 RM         F                  14      7\n 9      9790     1    19  1985      16 RM         F                  16      4\n10     11299     3     9  1986       3 RM         F                  16      7\n# ℹ 16 more rows\n\n\n\n\n\nfirst_min_per_year = (\n    surveys.dropna(subset=[\"weight\"])                     # 1. remove rows with missing weight\n           .groupby(\"year\", group_keys = False)           # 2. split data into groups by year\n           .apply(lambda g: g.loc[g[\"weight\"].idxmin()])  # 3. in each group, pick row with min weight\n           .reset_index(drop = True)                      # 4. clean up index\n)\n\nfirst_min_per_year.head()\n\n   record_id  month  day  year  plot_id species_id sex  hindfoot_length  weight\n0        218      9   13  1977        1         PF   M             13.0     4.0\n1       1161      8    5  1978        6         PF   F             16.0     6.0\n2       1923      7   25  1979       18         PF   F              NaN     6.0\n3       2506      2   25  1980        3         PF   F             15.0     5.0\n4       4052      4    5  1981        3         PF   F             15.0     4.0\n\n\nHere we’re adding group_keys = False to the groupby() function, so we’re not adding the group labels to the index. We are using .idxmin() to find the minimum index id.\n\n\n\n\n\n11.6.2 Grouped changes\nSometimes you might need to add a new variable to our table, based on different groups. Let’s say we want to see how many female and male observations there are in our surveys data set for each year.\nWe’re also interested in the percentage of female observations out of the total number of observations where sex was recorded.\nWe have the following number of female / male observations:\n\nRPython\n\n\n\nsurveys |&gt; \n  group_by(year) |&gt; \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |&gt; \n  ungroup()\n\n# A tibble: 26 × 3\n    year n_obs_f n_obs_m\n   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n 1  1977     204     214\n 2  1978     503     433\n 3  1979     327     324\n 4  1980     605     727\n 5  1981     631     745\n 6  1982     823    1027\n 7  1983     771     797\n 8  1984     445     443\n 9  1985     636     716\n10  1986     414     455\n# ℹ 16 more rows\n\n\nNow, let’s say we’d be interested in the percentage of female observations out of the total of observations where it was scored. We’d have to add a new column. Adding new columns is, as we’ve seen before, a job for mutate().\n\nsurveys |&gt; \n  group_by(year) |&gt; \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  mutate(female_pct = n_obs_f / (n_obs_f + n_obs_m) * 100)\n\n# A tibble: 26 × 4\n    year n_obs_f n_obs_m female_pct\n   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1  1977     204     214       48.8\n 2  1978     503     433       53.7\n 3  1979     327     324       50.2\n 4  1980     605     727       45.4\n 5  1981     631     745       45.9\n 6  1982     823    1027       44.5\n 7  1983     771     797       49.2\n 8  1984     445     443       50.1\n 9  1985     636     716       47.0\n10  1986     414     455       47.6\n# ℹ 16 more rows\n\n\nThe nice thing about chaining all these commands is that we can quickly build up what we want. We could, for example, easily plot the outcome of this.\n\nsurveys |&gt; \n  group_by(year) |&gt; \n  summarise(n_obs_f = sum(sex == \"F\", na.rm = TRUE),\n            n_obs_m = sum(sex == \"M\", na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  mutate(female_pct = n_obs_f / (n_obs_f + n_obs_m) * 100) |&gt; \n  ggplot(aes(x = year, y = female_pct)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 11.6: Percentage of female observations across years.\n\n\n\n\n\n\n\n\nsummary = (\n    surveys\n    .groupby(\"year\")\n    .agg(n_obs_f = (\"sex\", lambda x: (x == \"F\").sum()),\n         n_obs_m = (\"sex\", lambda x: (x == \"M\").sum()))\n    .reset_index()\n)\n\nsummary.head()\n\n   year  n_obs_f  n_obs_m\n0  1977      204      214\n1  1978      503      433\n2  1979      327      324\n3  1980      605      727\n4  1981      631      745\n\n\n\n# Add female percentage column\nsummary[\"female_pct\"] = summary[\"n_obs_f\"] / (summary[\"n_obs_f\"] + summary[\"n_obs_m\"]) * 100\n\nsummary.head()\n\n   year  n_obs_f  n_obs_m  female_pct\n0  1977      204      214   48.803828\n1  1978      503      433   53.739316\n2  1979      327      324   50.230415\n3  1980      605      727   45.420420\n4  1981      631      745   45.857558\n\n\nNow we have the table, we can easily plot it to get a better sense of any trends.\n\np = (ggplot(summary, aes(x = \"year\", y = \"female_pct\")) +\n    geom_line())\n\np.show()\n\n\n\n\n\n\n\nFigure 11.7: Percentage of female observations across years.\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\n# seaborn lineplot\nsns.lineplot(\n    data = summary,\n    x = \"year\",\n    y = \"female_pct\"\n)\n\nplt.title(\"Female percentage by year\")\nplt.show()\n\n\n\n\n\n\n\nFigure 11.8: Percentage of female observations across years.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningTo ungroup or not ungroup (R-only)\n\n\n\nEach time you do a grouped operation, it’s good practice to remove the grouping afterwards. If you don’t, then you might unintentionally be doing operations within the groups later on.\n\nRPython\n\n\nLet’s illustrate this with an example. We’ll take out any missing values, to simplify things.\n\nobs_count &lt;- surveys |&gt; \n  drop_na() |&gt; \n  group_by(sex, year) |&gt; \n  summarise(n_obs = n())\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n\nobs_count\n\n# A tibble: 52 × 3\n# Groups:   sex [2]\n   sex    year n_obs\n   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n 1 F      1977   123\n 2 F      1978   430\n 3 F      1979   297\n 4 F      1980   442\n 5 F      1981   482\n 6 F      1982   672\n 7 F      1983   758\n 8 F      1984   436\n 9 F      1985   624\n10 F      1986   400\n# ℹ 42 more rows\n\n\nLet’s say we now wanted to transform the n_obs variable to a percentage of the total number of observations in the entire data set (which is 30676).\n\nobs_count &lt;- obs_count |&gt; \n  mutate(n_obs_pct = n_obs / sum(n_obs) * 100)\n\nWe’d expect these values in n_obs_pct to add up to 100%.\n\nsum(obs_count$n_obs_pct)\n\n[1] 200\n\n\nHowever, they add up to 200 instead! Why? That’s because the table was still grouped by sex and as such, the percentages were calculated by each sex group. There are two of them (F, M - we filtered out the missing values), so the percentages add up to 100% within each sex group.\nThe way to avoid this issue is to ensure we remove any groups from our table, which we can do with ungroup(). Here’s the full string of commands, with the ungrouping step added:\n\nobs_count &lt;- surveys |&gt; \n  drop_na() |&gt;                                  # remove all NAs\n  group_by(sex, year) |&gt;                        # group by sex, year\n  summarise(n_obs = n()) |&gt;                     # get number of rows\n  ungroup() |&gt;                                  # ungroup here\n  mutate(n_obs_pct = n_obs / sum(n_obs) * 100)  # calculate percentage\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\nWe can check the percentages again and see that all is well:\n\nsum(obs_count$n_obs_pct)\n\n[1] 100\n\n\n\n\nIn pandas, grouping only affects the aggregation step. Once you run .groupby().agg() or .groupby().sum(), the grouping is gone — the resulting DataFrame is no longer grouped.\nSo, things are easier in Python in this respect. Sometimes it’s nice to be smug.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#exercises",
    "href": "materials/da3-08-grouped-operations.html#exercises",
    "title": "11  Grouped operations",
    "section": "11.7 Exercises",
    "text": "11.7 Exercises\n\n11.7.1 Grouped summaries: infections\n\n\n\n\n\n\nExerciseExercise 1 - Grouped summaries\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/infections.csv.\nPlease find the following:\n\nThe average CRP level for each age group\nThe average CRP level for each age group by ICU admission status\nThe number of observations per hospital by quarter\nThe minimum, average and maximum number of symptoms per age group\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe’ll assume you’ve still got the data loaded. For all of the output we’ll just display the first few rows.\n\n1. The average CRP level for each age group\nIf we want to do this, we need to group our data by age_group and then calculate the average (mean) value for each of those groups.\n\nRPython\n\n\n\ninfections |&gt; \n  group_by(age_group) |&gt; \n  summarise(avg_crp_level = mean(crp_level, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  head()\n\n# A tibble: 4 × 2\n  age_group avg_crp_level\n  &lt;chr&gt;             &lt;dbl&gt;\n1 18 - 64            18.7\n2 65+                19.5\n3 &lt; 18               20.1\n4 &lt;NA&gt;               20.0\n\n\n\nWe use the summarise() function since we want a summary of the average.\nWe specify na.rm = TRUE inside the mean() function, to ensure that any missing values within crp_level are ignored.\nWe ungroup() at the end to remove the grouping.\n\n\n\n\n(\n  infections\n  .groupby(\"age_group\", as_index = False)\n  .agg(avg_crp_level = (\"crp_level\", \"mean\"))\n  .head()\n)\n\n  age_group  avg_crp_level\n0   18 - 64      18.698266\n1       65+      19.513784\n2      &lt; 18      20.076799\n\n\n\nWe use .groupby() to specify the age_group as the grouping variable.\nWe set as_index = False to ensure we keep it as a column, not an index.\nWe use .agg() to calculate the \"mean\" for \"crp_level\".\n\n\n\n\n\n\n\nNoteIncluding missing values or not?\n\n\n\n\n\nBy default, .groupby() skips missing values. We have missing values in our age_group variable, which you might still be interested in. If you want to include them, then you need to be explicit about this:\n\n(\n  infections\n  .groupby(\"age_group\", dropna = False, as_index = False)\n  .agg(avg_crp_level = (\"crp_level\", \"mean\"))\n  .head()\n)\n\n  age_group  avg_crp_level\n0   18 - 64      18.698266\n1       65+      19.513784\n2      &lt; 18      20.076799\n3       NaN      19.958197\n\n\n\n\n\n\n\n\n\n\n2. The average CRP level for each age group by ICU admission status\nHere we can just add to our previous example, but instead of grouping by only age_group, we also group by icu_admission.\n\nRPython\n\n\n\ninfections |&gt; \n  group_by(age_group, icu_admission) |&gt; \n  summarise(avg_crp_level = mean(crp_level, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  head()\n\n`summarise()` has grouped output by 'age_group'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n  age_group icu_admission avg_crp_level\n  &lt;chr&gt;     &lt;lgl&gt;                 &lt;dbl&gt;\n1 18 - 64   FALSE                  18.7\n2 18 - 64   TRUE                   19.2\n3 18 - 64   NA                     16.6\n4 65+       FALSE                  19.1\n5 65+       TRUE                   20.1\n6 65+       NA                     17.2\n\n\n\n\nWe can do this by passing a list with the column names to .groupby():\n\n(\n  infections\n  .groupby([\"age_group\", \"icu_admission\"], dropna = False, as_index = False)\n  .agg(avg_crp_level = (\"crp_level\", \"mean\"))\n  .head()\n)\n\n  age_group icu_admission  avg_crp_level\n0   18 - 64         False      18.664167\n1   18 - 64          True      19.202897\n2   18 - 64           NaN      16.615909\n3       65+         False      19.095684\n4       65+          True      20.093705\n\n\n\n\n\n\n\n3. The number of observations per hospital by quarter\nTo get these values, we need to tally the number of rows (each row is an observation). We’re asked to do this per hospital (encoded in the hospital column) and for each quarter.\n\nRPython\n\n\n\ninfections |&gt; \n  count(hospital, quarter) |&gt; \n  head()\n\n# A tibble: 6 × 3\n  hospital   quarter     n\n  &lt;chr&gt;      &lt;chr&gt;   &lt;int&gt;\n1 hospital_1 Q1         82\n2 hospital_1 Q2         96\n3 hospital_1 Q3         83\n4 hospital_1 &lt;NA&gt;       16\n5 hospital_2 Q1         73\n6 hospital_2 Q2         84\n\n\n\n\n\n(\n  infections\n  .groupby([\"hospital\", \"quarter\"], as_index = False)\n  .size()\n  .rename(columns = {\"size\": \"n\"})\n  .head()\n)\n\n     hospital quarter   n\n0  hospital_1      Q1  82\n1  hospital_1      Q2  96\n2  hospital_1      Q3  83\n3  hospital_2      Q1  73\n4  hospital_2      Q2  84\n\n\n\n\n\n\n\n4. The minimum, average and maximum number of symptoms per age group\nHere we again need to aggregate/summarise our data, because we’re being asked to calculate some summary statistics. We need to group our data by age_group and then determine the minimum (min), average (mean) and maximum (max).\n\nRPython\n\n\n\ninfections |&gt; \n  group_by(age_group) |&gt; \n  summarise(min_symptoms = min(symptoms_count, na.rm = TRUE),\n            mean_symptoms = mean(symptoms_count, na.rm = TRUE),\n            max_symptoms = max(symptoms_count, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  head()\n\n# A tibble: 4 × 4\n  age_group min_symptoms mean_symptoms max_symptoms\n  &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 18 - 64              0          8.44           21\n2 65+                  0          8.50           19\n3 &lt; 18                 0          8.73           19\n4 &lt;NA&gt;                 2          8.41           19\n\n\n\n\n\n(\n  infections\n  .groupby(\"age_group\", dropna = False, as_index=False)\n  .agg(\n      min_symptoms = (\"symptoms_count\", \"min\"),\n      mean_symptoms = (\"symptoms_count\", \"mean\"),\n      max_symptoms = (\"symptoms_count\", \"max\")\n      )\n  .head()\n)\n\n  age_group  min_symptoms  mean_symptoms  max_symptoms\n0   18 - 64           0.0       8.443325          21.0\n1       65+           0.0       8.502174          19.0\n2      &lt; 18           0.0       8.729730          19.0\n3       NaN           2.0       8.405797          19.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.7.2 Grouped operations: infections\n\n\n\n\n\n\nExerciseExercise 2 - Grouped operations\n\n\n\n\n\n\nLevel: \nWe’ll keep using the infections data set.\nTo add a bit of fun, we’ll combine the filtering with making some changes to our data and plotting these. These are the kind of operations you’ll be doing a lot when exploring and analysis your data, so it’s good to practise!\nHave a look at the plot below and try to recreate it as accurately as possible:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHintHint\n\n\n\n\n\n\nFollows this logic:\n\nWhat data is being displayed?\nWhich variable(s) do I need to calculate?\n\n\ntop 10 &gt; think about arranging your data and subsetting it\nlog values can be calculated, use search engine\n\n\nIs any missing data present/absent?\nWhat kind of plot am I looking at (search for: lollipop plot in ggplot for R and lollipop plot in plotnine for Python)\n\n\n\n\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nWe’ll assume you still have the data loaded.\n\nRPython\n\n\n\ninfections |&gt; \n  filter(!is.na(hospital)) |&gt;\n  group_by(hospital) |&gt; \n  arrange(desc(crp_level)) |&gt; \n  slice(1:10) |&gt; \n  summarise(mean_crp = mean(crp_level, na.rm = TRUE),\n            log_crp = log(mean_crp)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = hospital, y = log_crp)) +\n  geom_point() +\n  geom_segment(aes(x = hospital, xend = hospital, y = 3.8, yend = log_crp)) +\n  labs(title = \"Average log values for CRP\",\n       subtitle = \"calculated on top 10 highest values within each hospital\",\n       x = \"\",\n       y = \"CRP level (log values)\")\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\ninfections_result = (\n    infections\n    .dropna(subset=[\"hospital\"])\n    .sort_values(\"crp_level\", ascending = False)        # arrange data\n    .groupby(\"hospital\", as_index = False)              # group data         \n    .head(10)                                           # get first 10 rows\n    .groupby(\"hospital\", as_index = False)              # group again\n    .agg(mean_crp = (\"crp_level\", \"mean\"))              # mean values\n    .assign(log_crp = lambda d: np.log(d[\"mean_crp\"]))  # calculate log\n)\n\n\np = (\n    ggplot(infections_result, aes(x = \"hospital\", y = \"log_crp\")) +\n    geom_point() +\n    geom_segment(aes(x = \"hospital\", xend = \"hospital\", y = 3.8, yend = \"log_crp\")) +\n    labs(\n        title = \"Average log values for CRP\",\n        subtitle = \"calculated on top 10 highest values within each hospital\",\n        x = \"\",\n        y = \"CRP level (log values)\"\n    )\n)\n\np.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da3-08-grouped-operations.html#summary",
    "href": "materials/da3-08-grouped-operations.html#summary",
    "title": "11  Grouped operations",
    "section": "11.8 Summary",
    "text": "11.8 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nRPython\n\n\n\nWe can split our data into groups and apply operations to each group.\nWe can then combine the outcomes in a new table.\nWe use summarise() to calculate summary statistics (e.g. mean, median, maximum, etc).\nUsing pipes with groups (e.g. group_by() |&gt; summarise()) we can calculate those summaries across groups.\nWe can also filter (group_by() |&gt;  filter()) or create new columns (group_by() |&gt; mutate()).\nIt is good practice to remove grouping (with ungroup()) from tables after group_by() operations, to avoid issues with retained groupings.\n\n\n\n\nWe can split our data into groups and apply operations to each group.\nWe can then combine the outcomes in a new table.\nWe use .agg() to calculate summary statistics (e.g. mean, median, maximum, etc).\nWe can use .groupby() to group by variables in our data.\nUsing grouped filters we can find values for each group within our data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA3: Manipulating data",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Grouped operations</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html",
    "href": "materials/da4-09-reshaping-data.html",
    "title": "12  Reshaping data",
    "section": "",
    "text": "12.1 Context\nSo far, we have provided data in the most convenient format. In real life, this is of course not always the case, because people collect data in a format that works best for them - not the computer. So, sometimes we need to change the shape of our data, so we can calculate or visualise the data we’d like.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#setup_reshaping_data",
    "href": "materials/da4-09-reshaping-data.html#setup_reshaping_data",
    "title": "12  Reshaping data",
    "section": "12.2 Section setup",
    "text": "12.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 04_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys_join &lt;- read_csv(\"data/surveys_join.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 04_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsurveys_join = pd.read_csv(\"data/surveys_join.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#data-reshaping",
    "href": "materials/da4-09-reshaping-data.html#data-reshaping",
    "title": "12  Reshaping data",
    "section": "12.3 Data reshaping",
    "text": "12.3 Data reshaping\nLet’s look at a hypothetical data set, based on the type of variables we’ve come across in the surveys data.\n\n\n\n\n\n\nFigure 12.1: Wide and long data formats contain the same information\n\n\n\nThe data that is present in both tables is the same - it’s just encoded slightly differently.\n\nThe “long” format is where we have a column for each of the types of things we measured or recorded in our data. In other words, each variable has its own column.\nThe “wide” format occurs when we have data relating to the same measured thing in different columns. In this case, we have values related to our metric spread across multiple columns (a column each for a year).\n\n\n\n\n\n\n\nNoteWide or long?\n\n\n\nNeither of these formats is necessarily more correct than the other: it will depend on what analysis you intend on doing. However, it is worth mentioning that the “long” format is often preferred, as it is clearer how many distinct types of variables we have in the data.\nTo figure out which format you might need, it may help to think of which visualisations you may want to build with ggplot() (or other packages, for that example). Taking the above example:\n\nIf you were interested in looking at the change of weight across years for each individual, then the long format is more suitable to define each aesthetic of such a graph: aes(x = year, y = weight, colour = record_id).\nIf you were interested in the correlation of this metric between 2021 and 2022, then the wide format would be more suitable: aes(x = yr_2021, y = yr_2022, colour = record_id).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#sec-long_to_wide",
    "href": "materials/da4-09-reshaping-data.html#sec-long_to_wide",
    "title": "12  Reshaping data",
    "section": "12.4 From long to wide",
    "text": "12.4 From long to wide\nLet’s illustrate that with a dummy data set, called surveys_join. In this synthetic data set we have weight measurements for individuals across four years: 2021 - 2024. This means that there are four measurements for each record_id.\n\nRPython\n\n\n\n# Read in the data\nsurveys_join &lt;- read_csv(\"data/surveys_join.csv\")\n\n# Look at the first few rows\nhead(surveys_join)\n\n# A tibble: 6 × 3\n  record_id  year weight\n      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       166  2021   195.\n2       166  2022   190.\n3       166  2023   184.\n4       166  2024   182 \n5       228  2021   192.\n6       228  2022   189 \n\n\nWe can reshape our data from long to wide as follows, where I do not overwrite the existing data, but instead just pipe it through to the head() function, so we can see what the pivot_wider() function is doing:\n\nsurveys_join |&gt; \n  pivot_wider(names_from = \"year\",\n              values_from = \"weight\",\n              names_prefix = \"yr_\") |&gt;\n  head()\n\n# A tibble: 6 × 5\n  record_id yr_2021 yr_2022 yr_2023 yr_2024\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1       166    195.    190.    184.    182 \n2       228    192.    189     188     179 \n3       337    181.    176.    177     168.\n4       330    172.    170.    165.    157.\n5       205    177.    176.    167.    166.\n6       878    170.    172     167.    161.\n\n\nLet’s unpack that a bit.\nThe pivot_wider() function needs at least the first two arguments:\n\nnames_from = the column who’s values we want to use for our new column names (year)\nvalues_from = the column who’s values we want to use to populate these new columns (weight)\nnames_prefix = a prefix for our column names (optional)\n\nHere we also add names_prefix = \"yr_\", otherwise the column names would contain only numbers and that’s not very good programming habit.\nLet’s assign it to a new object and then visualise some of the data.\n\nsurveys_wide &lt;- surveys_join |&gt; \n  pivot_wider(names_from = \"year\",\n              values_from = \"weight\",\n              names_prefix = \"yr_\")\n\n\n\n\n# Read in the data\nsurveys_join = pd.read_csv(\"data/surveys_join.csv\")\n\n# Look at the first few rows\nsurveys_join.head()\n\n   record_id  year  weight\n0        166  2021   195.4\n1        166  2022   189.5\n2        166  2023   183.6\n3        166  2024   182.0\n4        228  2021   191.9\n\n\n\nsurveys_wide = surveys_join.pivot(\n    index = \"record_id\",\n    columns = \"year\",\n    values = \"weight\"\n)\n\n# Add 'yr_' prefix to year columns\nsurveys_wide.columns = [f\"yr_{col}\" for col in surveys_wide.columns]\n\n# Reset index to make 'record_id' a column again\nsurveys_wide = surveys_wide.reset_index()\n\nsurveys_wide.head()\n\n   record_id  yr_2021  yr_2022  yr_2023  yr_2024\n0        118    199.5    192.8    192.6    187.6\n1        160    192.5    189.0    184.3    181.4\n2        166    195.4    189.5    183.6    182.0\n3        178    190.1    188.4    182.2    177.5\n4        184    195.1    186.7    193.5    187.3\n\n\n\n\n\n\n\n\nNoteMore on adding the prefix\n\n\n\nWe explain the f notation in more detail in Section 14.3.1. Briefly, in the example above we take all the column names (surveys_wide.columns and for each column (for col in) we add the yr_ prefix.\n\n\n\n\n\nWe can then use this to visualise possible relationships between the different years:\n\nRPython\n\n\n\nggplot(surveys_wide, aes(x = yr_2021, y = yr_2022)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 12.2: Scatterplot of weight for 2021 and 2022\n\n\n\n\n\nIf you’d be interested in comparisons across all years, you’d have to use the original, long format because there isn’t a single column in the wide table that contains all of the year information.\n\nggplot(surveys_join, aes(x = factor(year), y =  weight)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nFigure 12.3: Boxplot of weight for 2021 - 2024\n\n\n\n\n\n\n\n\np = (ggplot(surveys_wide, aes(x = \"yr_2021\", y = \"yr_2022\")) +\n    geom_point())\n    \np.show()\n\n\n\n\n\n\n\nFigure 12.4: Scatterplot of weight for 2021 and 2022\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.scatterplot(\n    data = surveys_wide,\n    x = \"yr_2021\",\n    y = \"yr_2022\"\n)\n\nplt.title(\"Scatterplot of yr_2021 vs yr_2022\")\nplt.show()\n\n\n\n\n\n\n\nFigure 12.5: Scatterplot of weight for 2021 and 2022\n\n\n\n\n\n\n\n\n\np = (ggplot(surveys_join, aes(x = \"factor(year)\", y = \"weight\")) +\n    geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\nFigure 12.6: Boxplot of weight for 2021 - 2024\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.boxplot(\n    data = surveys_join,\n    x = \"year\", # seaborn treats numeric as categorical automatically if discrete\n    y = \"weight\"\n)\n\nplt.title(\"Weight by year (boxplot)\")\nplt.show()\n\n\n\n\n\n\n\nFigure 12.7: Boxplot of weight for 2021 - 2024",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#from-wide-to-long",
    "href": "materials/da4-09-reshaping-data.html#from-wide-to-long",
    "title": "12  Reshaping data",
    "section": "12.5 From wide to long",
    "text": "12.5 From wide to long\nWe can reshape our data from wide to long. This is more or less the inverse of what we did above.\n\nRPython\n\n\n\nsurveys_wide |&gt; \n  pivot_longer(cols = -record_id,\n               names_to = \"year\",\n               values_to = \"weight\",\n               names_prefix = \"yr_\")\n\n# A tibble: 200 × 3\n   record_id year  weight\n       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1       166 2021    195.\n 2       166 2022    190.\n 3       166 2023    184.\n 4       166 2024    182 \n 5       228 2021    192.\n 6       228 2022    189 \n 7       228 2023    188 \n 8       228 2024    179 \n 9       337 2021    181.\n10       337 2022    176.\n# ℹ 190 more rows\n\n\nHere we use the following arguments:\n\ncols = this tells pivot_longer() which columns to pivot - here we want to use all but the record_id column\nnames_to = the name of the column that gets to hold the column names (e.g. yr_2021, yr_2022 …)\nvalues_to = the name of the column that will contain the measured values (here those are the weight measurements)\nnames_prefix = here we tell it that all column names have a prefix yr_, which then gets removed prior to populating the column\n\n\n\nTo go back to the long format, we use the .melt() function.\n\nsurveys_long = surveys_wide.melt(\n    id_vars = \"record_id\",            # columns to keep fixed\n    var_name = \"year\",                # name of the new 'year' column\n    value_name = \"weight\"             # name of the new 'weight' column\n)\n\nsurveys_long.head()\n\n   record_id     year  weight\n0        118  yr_2021   199.5\n1        160  yr_2021   192.5\n2        166  yr_2021   195.4\n3        178  yr_2021   190.1\n4        184  yr_2021   195.1\n\n\nThis uses the following arguments:\n\nid_vars = this tells it what the id column is - record_id in our case, which does not get pivoted.\nvar_name = the name of the column that gets to hold the column names (e.g. yr_2021, yr_2022 …)\nvalue_name = the name of the column that will contain the measured values (here those are the weight measurements)\n\nThis then creates a column year that contains the values yr_2021, yr_2022, ..., since we added the prefix. If we want to remove the prefix we can do the following:\n\n# Remove 'yr_' prefix and convert year to integer\nsurveys_long[\"year\"] = surveys_long[\"year\"].str.replace(\"yr_\", \"\").astype(int)\n\nsurveys_long.head()\n\n   record_id  year  weight\n0        118  2021   199.5\n1        160  2021   192.5\n2        166  2021   195.4\n3        178  2021   190.1\n4        184  2021   195.1",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#exercises",
    "href": "materials/da4-09-reshaping-data.html#exercises",
    "title": "12  Reshaping data",
    "section": "12.6 Exercises",
    "text": "12.6 Exercises\n\n12.6.1 Reshaping: auxin\n\n\n\n\n\n\nExerciseExercise 1 - Reshaping data\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/auxin.csv.\nThese are synthetic data that measure the plant height of Arabidopsis thaliana, in different genotypes (variants of the same species, here: control and mutant). It measures the effect the plant hormone auxin has on the growth of these plants.\nThis is done across different auxin concentrations (none, low, high).\nPlease do the following:\n\nCheck the data structure.\nCreate a “wide” table where, for each replicate_id / genotype pair, we have a column for each auxin concentration category. The data in these columns should be the plant_height measurements.\nUse this wide format to calculate the average plant height for each row.\nChange the data back to “long” format & check if you have your original data back.\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\nFirst, let’s read in the data.\n\nRPython\n\n\n\nauxin &lt;- read_csv(\"data/auxin.csv\")\n\n\n\n\nauxin = pd.read_csv(\"data/auxin.csv\")\n\n\n\n\n\n1. Data structure\nBefore we make any changes, it’s important to get a sense of how the data is currently shaped.\n\nRPython\n\n\n\nhead(auxin)\n\n# A tibble: 6 × 4\n  replicate_id genotype concentration plant_height\n         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1            1 control  high                  31.7\n2            2 control  high                  33.6\n3            3 control  high                  31.3\n4            4 control  high                  30.5\n5            5 control  high                  30.4\n6            6 control  high                  30.1\n\n\n\n\n\nauxin.head()\n\n   replicate_id genotype concentration  plant_height\n0             1  control          high          31.7\n1             2  control          high          33.6\n2             3  control          high          31.3\n3             4  control          high          30.5\n4             5  control          high          30.4\n\n\n\n\n\nWe have 56 distinct replicate_id values, with at least 1 and up to 6 observations.\nThere are also three distinct concentration categories: high, low, none\n\n\n\n\n\n\nImportant\n\n\n\nCheck this yourself!\n\n\n\n\n2. From long to wide\nSo what we want to do is get a table where for each replicate_id and genotype combination we have a separate column for each concentration category. That would allow us, for example, to calculate the average plant height for each measurement across the different concentration types.\nThe data should look something like:\n\n\n\nreplicate_id\ngenotype\nhigh\nlow\nnone\n\n\n\n\n1\ncontrol\n31.7\n38.5\n42.7\n\n\n2\ncontrol\n33.6\n35.9\n46.8\n\n\n3\ncontrol\n31.3\n38.4\n42.8\n\n\n\nSo, let’s do that.\n\nRPython\n\n\n\nauxin_wide &lt;- pivot_wider(auxin,\n            names_from = concentration,\n            values_from = plant_height)\n\nhead(auxin_wide)\n\n# A tibble: 6 × 5\n  replicate_id genotype  high   low  none\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1            1 control   31.7  38.5  42.7\n2            2 control   33.6  35.9  46.8\n3            3 control   31.3  38.4  42.8\n4            4 control   30.5  42.3  46.5\n5            5 control   30.4  32.8  46.5\n6            6 control   30.1  37.4  45.4\n\n\n\n\n\nauxin_wide = auxin.pivot(\n    index = [\"replicate_id\", \"genotype\"],\n    columns = \"concentration\",\n    values = \"plant_height\"\n).reset_index()\n\nauxin_wide.head()\n\nconcentration  replicate_id genotype  high   low  none\n0                         1  control  31.7  38.5  42.7\n1                         1   mutant  33.4  41.5  37.6\n2                         2  control  33.6  35.9  46.8\n3                         2   mutant  30.4  35.7  40.3\n4                         3  control  31.3  38.4  42.8\n\n\n\n\n\n\n\n3. Calculating average plant_height\nHaving the data in this format allows us to do:\n\nRPython\n\n\n\nauxin_wide |&gt; \n  mutate(avg_height = round((high + low + none) / 3, 1)) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  replicate_id genotype  high   low  none avg_height\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1            1 control   31.7  38.5  42.7       37.6\n2            2 control   33.6  35.9  46.8       38.8\n3            3 control   31.3  38.4  42.8       37.5\n4            4 control   30.5  42.3  46.5       39.8\n5            5 control   30.4  32.8  46.5       36.6\n6            6 control   30.1  37.4  45.4       37.6\n\n\n\n\n\nauxin_wide.assign(\n    avg_height = auxin_wide[[\"high\", \"low\", \"none\"]] # select columns\n    .mean(axis = 1)                                  # calculate mean\n    .round(1)                                        # round to 1 decimal\n).head()                                             # display first few rows\n\nconcentration  replicate_id genotype  high   low  none  avg_height\n0                         1  control  31.7  38.5  42.7        37.6\n1                         1   mutant  33.4  41.5  37.6        37.5\n2                         2  control  33.6  35.9  46.8        38.8\n3                         2   mutant  30.4  35.7  40.3        35.5\n4                         3  control  31.3  38.4  42.8        37.5\n\n\n\n\n\n\n\n4. From wide to long\nWe can also revert back to our original “long” format data. The data then has its original shape back, which follows:\n\n\n\nreplicate_id\ngenotype\nconcentration\nplant_height\n\n\n\n\n1\ncontrol\nhigh\n31.7\n\n\n2\ncontrol\nhigh\n33.6\n\n\n3\ncontrol\nhigh\n31.3\n\n\n\nSo, let’s do that.\n\nRPython\n\n\n\nauxin_long &lt;- pivot_longer(auxin_wide,\n             cols = c(\"high\", \"low\", \"none\"),\n             names_to = \"concentration\",\n             values_to = \"plant_height\")\n\n\n\n\nauxin_long = auxin_wide.melt(\n    id_vars = [\"replicate_id\", \"genotype\"],  # columns to keep as identifiers\n    value_vars = [\"high\", \"low\", \"none\"],    # columns to unpivot\n    var_name = \"concentration\",              # new column for old column names\n    value_name = \"plant_height\"              # new column for values\n)\n\n\n\n\nHowever, the eagle-eyed among you might have noticed that there are more rows in our data than we started with:\n\nRPython\n\n\n\nnrow(auxin)\n\n[1] 275\n\nnrow(auxin_long)\n\n[1] 324\n\n\n\n\n\nauxin.shape[0]      # original data\n\n275\n\nauxin_long.shape[0] # wide-and-back\n\n324\n\n\n\n\n\nThis is because in some of the replicate_id / genotype combinations there were no measured values for all three concentration types. This introduced missing values, which are then propagated when going back to a long format.\nSo, to deal with this, we can simply remove the values where the plant_height value is missing:\n\nRPython\n\n\n\nauxin_long &lt;- pivot_longer(auxin_wide,\n             cols = c(\"high\", \"low\", \"none\"),\n             names_to = \"concentration\",\n             values_to = \"plant_height\",\n             values_drop_na = TRUE)\n\nnrow(auxin_long)\n\n[1] 275\n\n\n\n\n\nauxin_long = (auxin_wide.melt(\n    id_vars = [\"replicate_id\", \"genotype\"],  # columns to keep as identifiers\n    value_vars = [\"high\", \"low\", \"none\"],    # columns to unpivot\n    var_name = \"concentration\",              # new column for old column names\n    value_name = \"plant_height\"              # new column for values\n)\n.dropna(subset = [\"plant_height\"])\n)\n\nauxin_long.shape[0]\n\n275\n\n\n\n\n\nSuccess!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-09-reshaping-data.html#summary",
    "href": "materials/da4-09-reshaping-data.html#summary",
    "title": "12  Reshaping data",
    "section": "12.7 Summary",
    "text": "12.7 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nRPython\n\n\n\nWe can reshape data, going from long to wide (and back).\nWhich format you use depends on the aim: consider how you’d plot data.\nWe can use pivot_wider() to create a wide-format data set.\nWe can use pivot_longer() to create a long-format data set.\n\n\n\n\nWe can reshape data, going from long to wide (and back).\nWhich format you use depends on the aim: consider how you’d plot data.\nWe can use .pivot() to create a wide-format data set.\nWe can use .melt() to create a long-format data set.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reshaping data</span>"
    ]
  },
  {
    "objectID": "materials/da4-10-combining-data.html",
    "href": "materials/da4-10-combining-data.html",
    "title": "13  Combining data",
    "section": "",
    "text": "13.1 Context\nData is often split over multiple tables. We saw this in the previous section. Sometimes we need to combine information from multiple sources.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Combining data</span>"
    ]
  },
  {
    "objectID": "materials/da4-10-combining-data.html#setup_grouped_operations",
    "href": "materials/da4-10-combining-data.html#setup_grouped_operations",
    "title": "13  Combining data",
    "section": "13.2 Section setup",
    "text": "13.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 04_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\nplot_types &lt;- read_csv(\"data/plots.csv\")\nspecies &lt;- read_csv(\"data/species.csv\")\n\n\n\nWe’ll continue this section with the Notebook named 04_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\nplot_types = pd.read_csv(\"data/plots.csv\")\nspecies = pd.read_csv(\"data/species.csv\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Combining data</span>"
    ]
  },
  {
    "objectID": "materials/da4-10-combining-data.html#joining-tables",
    "href": "materials/da4-10-combining-data.html#joining-tables",
    "title": "13  Combining data",
    "section": "13.3 Joining tables",
    "text": "13.3 Joining tables\nWe’ve already seen that some of our data sets have an identifier, for example the record_id in the surveys data set.\nWhen it comes to joining data, these identifiers become extra important. After all, we need to be able to tell the computer how to join the data. We do this through a common key or variable between two data sets. We’ll illustrate how this works below.\n\n13.3.1 Setting up your joins\nThere are different ways you can join tables, depending on which data you’d like to retain. The way these joins are named often depend on the direction in which you are joining. Let’s look at this in more detail, using examples.\n\nRPython\n\n\n\nplot_types &lt;- read_csv(\"data/plots.csv\")\n\nRows: 5 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): plot_type\ndbl (1): plot_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s look at the data. We can see that there are five distinct plot types encoded in these data, using plot_id as a key.\n\nplot_types\n\n# A tibble: 5 × 2\n  plot_id plot_type                \n    &lt;dbl&gt; &lt;chr&gt;                    \n1       1 Spectab exclosure        \n2       2 Control                  \n3       3 Long-term Krat Exclosure \n4       4 Rodent Exclosure         \n5       5 Short-term Krat Exclosure\n\n\nNow let’s look at our surveys data set. We know we have a plot_id column there, too. Let’s check how many times the different plot_id values occur in our data. For this, we can use the count() function:\n\nsurveys |&gt; count(plot_id)\n\n# A tibble: 24 × 2\n   plot_id     n\n     &lt;dbl&gt; &lt;int&gt;\n 1       1  1995\n 2       2  2194\n 3       3  1828\n 4       4  1969\n 5       5  1194\n 6       6  1582\n 7       7   816\n 8       8  1891\n 9       9  1936\n10      10   469\n# ℹ 14 more rows\n\n\nThis shows that there are 24 plot_id values (there are 24 rows in the output). So, the plot_types data set won’t contain information on all of these, since it only contains 5 distinct plots.\n\n\n\nplot_types = pd.read_csv(\"data/plots.csv\")\n\nLet’s look at the data. We can see that there are five distinct plot types encoded in these data, using plot_id as a key.\n\nplot_types\n\n   plot_id                  plot_type\n0        1          Spectab exclosure\n1        2                    Control\n2        3   Long-term Krat Exclosure\n3        4           Rodent Exclosure\n4        5  Short-term Krat Exclosure\n\n\nNow let’s look at our surveys data set. We know we have a plot_id column there, too. Let’s check how many times the different plot_id values occur in our data.\n\nsurveys.groupby('plot_id').size().reset_index(name='n')\n\n    plot_id     n\n0         1  1995\n1         2  2194\n2         3  1828\n3         4  1969\n4         5  1194\n5         6  1582\n6         7   816\n7         8  1891\n8         9  1936\n9        10   469\n10       11  1918\n11       12  2365\n12       13  1538\n13       14  1885\n14       15  1069\n15       16   646\n16       17  2039\n17       18  1445\n18       19  1189\n19       20  1390\n20       21  1173\n21       22  1399\n22       23   571\n23       24  1048\n\n\nThis shows that there are 24 plot_id values (there are 24 rows in the output). So, the plot_types data set won’t contain information on all of these, since it only contains 5 distinct plots.\n\n\n\n\n\n13.3.2 Left joins\nWe’ll be adding the data from plot_types to the existing surveys data. This uses the following principle:\n\n\n\n\n\n\nFigure 13.1: Left join: table b to a\n\n\n\nThis means that any plot_id that appears in surveys, but isn’t present in plot_types will be empty or missing for its plot_type value.\nFor an animated representation of the underlying principle of joining, have a look at the garrickadenbuie website.\n\nRPython\n\n\n\nleft_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 35,549 × 10\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         6     7    16  1977       1 PF         M                  14     NA\n 7         7     7    16  1977       2 PE         F                  NA     NA\n 8         8     7    16  1977       1 DM         M                  37     NA\n 9         9     7    16  1977       1 DM         F                  34     NA\n10        10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n# ℹ 1 more variable: plot_type &lt;chr&gt;\n\n\nLet’s assign that output to an object called surveys_left.\n\nsurveys_left &lt;- left_join(surveys, plot_types, by = \"plot_id\")\n\n\n\n\nsurveys_left = pd.merge(surveys, plot_types, how = \"left\", on = \"plot_id\")\n\n\nsurveys_left\n\n       record_id  month  ...  weight                  plot_type\n0              1      7  ...     NaN                    Control\n1              2      7  ...     NaN   Long-term Krat Exclosure\n2              3      7  ...     NaN                    Control\n3              4      7  ...     NaN                        NaN\n4              5      7  ...     NaN   Long-term Krat Exclosure\n...          ...    ...  ...     ...                        ...\n35544      35545     12  ...     NaN                        NaN\n35545      35546     12  ...     NaN                        NaN\n35546      35547     12  ...    14.0                        NaN\n35547      35548     12  ...    51.0                        NaN\n35548      35549     12  ...     NaN  Short-term Krat Exclosure\n\n[35549 rows x 10 columns]\n\n\n\n\n\nHaving this information now allows us to plot the data by plot_type in a much more meaningful way than if we would have used plot_id. For example, let’s look at the hindfoot_length for each plot_type.\n\nRPython\n\n\n\nggplot(surveys_left, aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot()\n\nWarning: Removed 4111 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nFigure 13.2: Boxplot of hindfoot length for each plot type\n\n\n\n\n\n\n\n\np = (ggplot(surveys_left, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n  geom_boxplot())\n  \np.show()\n\n\n\n\n\n\n\nFigure 13.3: Boxplot of hindfoot length for each plot type\n\n\n\n\n\n\n\n\n\n\n\nNoteSeaborn equivalent\n\n\n\n\n\n\nsns.boxplot(\n    data = surveys_left,\n    x = \"plot_type\",\n    y = \"hindfoot_length\"\n)\n\nplt.title(\"Hindfoot length by plot type\")\nplt.show()\n\n\n\n\n\n\n\nFigure 13.4: Boxplot of hindfoot length for each plot type\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 Right joins\nNow we’re doing a join in the opposite direction: we’re joining the surveys data set to the plot_types data set. Conceptually, this looks like so:\n\n\n\n\n\n\nFigure 13.5: Right join: table a to b\n\n\n\nThis means that for all the rows where there isn’t a match for plot_id in the surveys data set, the rows are dropped.\n\nRPython\n\n\n\nright_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 9,180 × 10\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         5     7    16  1977       3 DM         M                  35     NA\n 5         6     7    16  1977       1 PF         M                  14     NA\n 6         7     7    16  1977       2 PE         F                  NA     NA\n 7         8     7    16  1977       1 DM         M                  37     NA\n 8         9     7    16  1977       1 DM         F                  34     NA\n 9        11     7    16  1977       5 DS         F                  53     NA\n10        13     7    16  1977       3 DM         M                  35     NA\n# ℹ 9,170 more rows\n# ℹ 1 more variable: plot_type &lt;chr&gt;\n\n\n\n\n\nsurveys_right = pd.merge(surveys, plot_types, how = \"right\", on = \"plot_id\")\n\nLet’s see how many rows we’ve retained.\n\nlen(surveys_right)\n\n9180\n\n\n\n\n\nWe can see that we have far fewer rows left (9180) than in the full data set (35549). This is because all the rows where there isn’t a match for plot_id in the surveys data set are dropped.\n\n\n13.3.4 Inner joins\n\n\n\n\n\n\nFigure 13.6: Inner join: retain values present in both tables\n\n\n\n\nRPython\n\n\n\ninner_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 9,180 × 10\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         5     7    16  1977       3 DM         M                  35     NA\n 5         6     7    16  1977       1 PF         M                  14     NA\n 6         7     7    16  1977       2 PE         F                  NA     NA\n 7         8     7    16  1977       1 DM         M                  37     NA\n 8         9     7    16  1977       1 DM         F                  34     NA\n 9        11     7    16  1977       5 DS         F                  53     NA\n10        13     7    16  1977       3 DM         M                  35     NA\n# ℹ 9,170 more rows\n# ℹ 1 more variable: plot_type &lt;chr&gt;\n\n\n\n\n\nsurveys_inner = pd.merge(surveys, plot_types, how = \"inner\", on = \"plot_id\")\n\nLet’s see how many rows we’ve retained. We can either use .shape[0] or len().\n\nlen(surveys_inner)\n\n9180\n\n\n\n\n\nThis actually gives the same result as with the right join (9180 rows), which is because there aren’t any rows in plot_type that don’t have a match in surveys.\n\n\n13.3.5 Full joins\n\n\n\n\n\n\nFigure 13.7: Full join: retain all values\n\n\n\n\nRPython\n\n\n\nfull_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 35,549 × 10\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         4     7    16  1977       7 DM         M                  36     NA\n 5         5     7    16  1977       3 DM         M                  35     NA\n 6         6     7    16  1977       1 PF         M                  14     NA\n 7         7     7    16  1977       2 PE         F                  NA     NA\n 8         8     7    16  1977       1 DM         M                  37     NA\n 9         9     7    16  1977       1 DM         F                  34     NA\n10        10     7    16  1977       6 PF         F                  20     NA\n# ℹ 35,539 more rows\n# ℹ 1 more variable: plot_type &lt;chr&gt;\n\n\n\n\nThe “full” join in pandas’ merge() function is referred to as \"outer\".\n\nsurveys_full = pd.merge(surveys, plot_types, how = \"outer\", on = \"plot_id\")\n\nLet’s see how many rows we’ve retained.\n\nlen(surveys_full)\n\n35549\n\n\n\n\n\nThis again gives us our entire surveys data set, including the additional plot_type information where available. The reason why this is not different to the left join is because there are no rows in plot_types that do not have a match in surveys.\n\n\n13.3.6 Filtering joins\nThere is one last set of joins we haven’t discussed yet: filtering joins. These can be really helpful if you’re comparing two tables and want to specifically extract the rows that are either present (semi-join) or absent (anti-join) in the other table.\n\nRPython\n\n\nSemi-join:\n\nsemi_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 9,180 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         1     7    16  1977       2 NL         M                  32     NA\n 2         2     7    16  1977       3 NL         M                  33     NA\n 3         3     7    16  1977       2 DM         F                  37     NA\n 4         5     7    16  1977       3 DM         M                  35     NA\n 5         6     7    16  1977       1 PF         M                  14     NA\n 6         7     7    16  1977       2 PE         F                  NA     NA\n 7         8     7    16  1977       1 DM         M                  37     NA\n 8         9     7    16  1977       1 DM         F                  34     NA\n 9        11     7    16  1977       5 DS         F                  53     NA\n10        13     7    16  1977       3 DM         M                  35     NA\n# ℹ 9,170 more rows\n\n\nAnti-join:\n\nanti_join(surveys, plot_types, by = \"plot_id\")\n\n# A tibble: 26,369 × 9\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1         4     7    16  1977       7 DM         M                  36     NA\n 2        10     7    16  1977       6 PF         F                  20     NA\n 3        12     7    16  1977       7 DM         M                  38     NA\n 4        14     7    16  1977       8 DM         &lt;NA&gt;               NA     NA\n 5        15     7    16  1977       6 DM         F                  36     NA\n 6        20     7    17  1977      11 DS         F                  48     NA\n 7        21     7    17  1977      14 DM         F                  34     NA\n 8        22     7    17  1977      15 NL         F                  31     NA\n 9        23     7    17  1977      13 DM         M                  36     NA\n10        24     7    17  1977      13 SH         M                  21     NA\n# ℹ 26,359 more rows\n\n\n\n\nSemi-join:\n\nsurveys[surveys[\"plot_id\"].isin(plot_types[\"plot_id\"])]\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n0              1      7   16  1977  ...          NL    M            32.0     NaN\n1              2      7   16  1977  ...          NL    M            33.0     NaN\n2              3      7   16  1977  ...          DM    F            37.0     NaN\n4              5      7   16  1977  ...          DM    M            35.0     NaN\n5              6      7   16  1977  ...          PF    M            14.0     NaN\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n35498      35499     12   31  2002  ...          PB    F            27.0    28.0\n35499      35500     12   31  2002  ...          PB    F            25.0    28.0\n35500      35501     12   31  2002  ...          DO    M            33.0    48.0\n35501      35502     12   31  2002  ...          PP    M            21.0    16.0\n35548      35549     12   31  2002  ...         NaN  NaN             NaN     NaN\n\n[9180 rows x 9 columns]\n\n\nAnti-join:\n\nsurveys[~surveys[\"plot_id\"].isin(plot_types[\"plot_id\"])]\n\n       record_id  month  day  year  ...  species_id  sex hindfoot_length  weight\n3              4      7   16  1977  ...          DM    M            36.0     NaN\n9             10      7   16  1977  ...          PF    F            20.0     NaN\n11            12      7   16  1977  ...          DM    M            38.0     NaN\n13            14      7   16  1977  ...          DM  NaN             NaN     NaN\n14            15      7   16  1977  ...          DM    F            36.0     NaN\n...          ...    ...  ...   ...  ...         ...  ...             ...     ...\n35543      35544     12   31  2002  ...          US  NaN             NaN     NaN\n35544      35545     12   31  2002  ...          AH  NaN             NaN     NaN\n35545      35546     12   31  2002  ...          AH  NaN             NaN     NaN\n35546      35547     12   31  2002  ...          RM    F            15.0    14.0\n35547      35548     12   31  2002  ...          DO    M            36.0    51.0\n\n[26369 rows x 9 columns]",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Combining data</span>"
    ]
  },
  {
    "objectID": "materials/da4-10-combining-data.html#exercises",
    "href": "materials/da4-10-combining-data.html#exercises",
    "title": "13  Combining data",
    "section": "13.4 Exercises",
    "text": "13.4 Exercises\n\n13.4.1 Simple joins: surveys\n\n\n\n\n\n\nExerciseExercise 1 - Simple joins\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/surveys.csv and data/species.csv.\nWe’ve of course seen the surveys data set more times than we can remember, but it’s still got more to give. In fact, we are going to add to it. We currently have a rather uninformative species_id column.\nFor this exercise, we’re going to link that with the data from species, which has more useful descriptions and additional data.\nAs an aside, this is not a bad data strategy at all - splitting your data across multiple tables. That way you don’t end up with gigantic tables that contain lots of data you might only need for very specific purposes. Or data that you might not want to share freely (e.g. patient or customer data).\nWithout further ado, please do the following:\n\nLoad the data\nJoin the two tables (consider how!) & inspect the output\n\n\n\n\n\n\n\nAnswerAnswer\n\n\n\n\n\n\n\n1. Loading the data\nFirst, we read in the data and have a look at the structure of species.\n\nRPython\n\n\n\nspecies &lt;- read_csv(\"data/species.csv\")\n\n\n\n\nspecies = pd.read_csv(\"data/species.csv\")\n\n\n\n\nLet’s look at what we’ve got.\n\nRPython\n\n\n\nhead(species)\n\n# A tibble: 6 × 4\n  species_id genus            species         taxa  \n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt; \n1 AB         Amphispiza       bilineata       Bird  \n2 AH         Ammospermophilus harrisi         Rodent\n3 AS         Ammodramus       savannarum      Bird  \n4 BA         Baiomys          taylori         Rodent\n5 CB         Campylorhynchus  brunneicapillus Bird  \n6 CM         Calamospiza      melanocorys     Bird  \n\n\n\n\n\nspecies.head()\n\n  species_id             genus          species    taxa\n0         AB        Amphispiza        bilineata    Bird\n1         AH  Ammospermophilus          harrisi  Rodent\n2         AS        Ammodramus       savannarum    Bird\n3         BA           Baiomys          taylori  Rodent\n4         CB   Campylorhynchus  brunneicapillus    Bird\n\n\n\n\n\nSo, the column we have in common between species and surveys is the species_id column. That will be our key column that we’ll use to join the data.\n\n\n2. Joining the tables and inspecting the output\nWe’ll be joining the species table to the surveys table, so we’ll join left: surveys &lt;&lt;&lt; species.\n\nRPython\n\n\n\nsurveys_species &lt;- left_join(surveys, species, by = \"species_id\")\n\nhead(surveys_species)\n\n# A tibble: 6 × 12\n  record_id month   day  year plot_id species_id sex   hindfoot_length weight\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n1         1     7    16  1977       2 NL         M                  32     NA\n2         2     7    16  1977       3 NL         M                  33     NA\n3         3     7    16  1977       2 DM         F                  37     NA\n4         4     7    16  1977       7 DM         M                  36     NA\n5         5     7    16  1977       3 DM         M                  35     NA\n6         6     7    16  1977       1 PF         M                  14     NA\n# ℹ 3 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;\n\n\n\n\n\nsurveys_species = surveys.merge(species, on = \"species_id\", how = \"left\")\n\nsurveys_species.head()\n\n   record_id  month  day  year  ...  weight      genus   species    taxa\n0          1      7   16  1977  ...     NaN    Neotoma  albigula  Rodent\n1          2      7   16  1977  ...     NaN    Neotoma  albigula  Rodent\n2          3      7   16  1977  ...     NaN  Dipodomys  merriami  Rodent\n3          4      7   16  1977  ...     NaN  Dipodomys  merriami  Rodent\n4          5      7   16  1977  ...     NaN  Dipodomys  merriami  Rodent\n\n[5 rows x 12 columns]\n\n\n\n\n\nWe can see that the data from species has been joined with surveys, since we now have genus, species and taxa columns in our data set.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Combining data</span>"
    ]
  },
  {
    "objectID": "materials/da4-10-combining-data.html#summary",
    "href": "materials/da4-10-combining-data.html#summary",
    "title": "13  Combining data",
    "section": "13.5 Summary",
    "text": "13.5 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nWe can use left, right, inner and outer/full joins to merge two tables together.\nThere needs to be at least one matching column between the two tables that can be used as a key to link them.\nWe can also use filter joins to identify rows in a table that are present/absent in the other",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "DA4: Organise and combine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Combining data</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html",
    "href": "materials/da4-11-cleaning-data.html",
    "title": "14  Clean, style & arrange",
    "section": "",
    "text": "14.1 Context\nOften data is in a messy state before you can work with it. So, it is useful to know when and how to make changes to your data.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#setup_grouped_operations",
    "href": "materials/da4-11-cleaning-data.html#setup_grouped_operations",
    "title": "14  Clean, style & arrange",
    "section": "14.2 Section setup",
    "text": "14.2 Section setup\n\nRPython\n\n\nWe’ll continue this section with the script named 05_session. If needed, add the following code to the top of your script and run it.\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# A package for cleaning up data\nlibrary(janitor)\n\n# A package for creating composite plots\nlibrary(patchwork)\n\nmessy_data &lt;- read_csv(\"data/messy_data.csv\")\nsurveys &lt;- read_csv(\"data/surveys.csv\")\nplot_types &lt;- read_csv(\"data/plots.csv\")\nsurveys_left &lt;- left_join(surveys, plot_types, by = \"plot_id\")\n\n\n\nWe’ll continue this section with the Notebook named 05_session. Add the following code to the first cell and run it.\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# A package for cleaning up data\nimport janitor\n\n# A package for text wrapping\nimport textwrap\n\nmessy_data = pd.read_csv(\"data/messy_data.csv\")\nsurveys = pd.read_csv(\"data/surveys.csv\")\nplot_types = pd.read_csv(\"data/plots.csv\")\nsurveys_left = pd.merge(surveys, plot_types, how = \"left\", on = \"plot_id\")",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#cleaning-data",
    "href": "materials/da4-11-cleaning-data.html#cleaning-data",
    "title": "14  Clean, style & arrange",
    "section": "14.3 Cleaning data",
    "text": "14.3 Cleaning data\nPerhaps it’s not the most glamorous part of data analysis, but it is a very important one: cleaning data. If you need motivation for it, just think of the amount of time you can save yourself by recording your data consistently and correctly in the first place.\nIn the next few sections we’ll go through some (very) common messy data issues you’re likely to come across. We will revisit some of our previous data sets, but will mostly illustrate things using the messy data set. This data set has been synthesised for this exact purpose, so it’s over-the-top bad. If you ever come across a real data set that looks like this, then a stern word with that researcher is in order!\nWe’ll read in the data and take it from there:\n\nRPython\n\n\n\nmessy_data &lt;- read_csv(\"data/messy_data.csv\")\n\nRows: 100 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): ID, Age, Gender, Score, country, employed.or.not, notes\ndbl (1): Income.in.GBP\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nmessy_data = pd.read_csv(\"data/messy_data.csv\")\n\n\n\n\n\n14.3.1 Variable naming\nThe messy data set has many different issues with it. One of the issues is that the column headers are not consistent. This is an issue that you’ll come across on a regular basis, since people have their own style (usually an inconsistent one!).\nIn both R and Python we have access to a “janitor” package (janitor in R and pyjanitor in Python). These are fantastic packages that can save you tons of time. Here we’ll just use one of the functions: clean_names().\nFirst, let’s see what we’re dealing with.\n\nRPython\n\n\n\nmessy_data |&gt; \n  colnames()\n\n[1] \"ID\"              \"Age\"             \"Gender\"          \"Score\"          \n[5] \"Income.in.GBP\"   \"country\"         \"employed.or.not\" \"notes\"          \n\n\n\n\n\nmessy_data.columns.tolist()\n\n['ID', 'Age', 'Gender', 'Score', 'Income.in.GBP', 'country', 'employed.or.not', 'notes']\n\n\n\n\n\nThat’s not ideal. There is inconsistency in the use of capitalisation and there are full stops in the column names.\nNow, let’s see what the clean_names() function makes of all of this (without updating the names just yet).\n\nRPython\n\n\n\nmessy_data |&gt; \n  clean_names() |&gt; \n  colnames()\n\n[1] \"id\"              \"age\"             \"gender\"          \"score\"          \n[5] \"income_in_gbp\"   \"country\"         \"employed_or_not\" \"notes\"          \n\n\n\n\n\n(messy_data           # data\n  .clean_names()      # clean column names\n  .columns.tolist())  # put column names in list\n\n['id', 'age', 'gender', 'score', 'income_in_gbp', 'country', 'employed_or_not', 'notes']\n\n\n\n\n\nWe can see that the column names are now consistently lowercase and that the full stop . in the names have been replaced with _ underscores.\nI’m happy with that, so I’ll assign those new names to the data.\n\nRPython\n\n\n\nmessy_data &lt;- messy_data |&gt;\n  clean_names()\n\n\n\n\nmessy_data = messy_data.clean_names()\n\nSometimes we also want to include generic prefixes to our column names. We can do this with f-strings.\n\n\n\n\n\n\nNoteFormatted string literals (f-strings)\n\n\n\nThese let you insert variables or expressions directly into a string by prefixing it with f or F.\nHave a look at this example:\n\ncourse = \"Data analysis in\"\nlanguage = \"Python\"\n\nmessage = f\"This is the {course} in {language} course.\"\n\nprint(message)\n\nThis is the Data analysis in in Python course.\n\n\nWe already used f-strings earlier in Section 12.4. There we created a prefix yr_ for each column.\n\nsurveys_wide.columns = [f\"yr_{col}\" for col in surveys_wide.columns]\n\nThis example uses list comprehension: [f\"yr_{col}\" for col in surveys_wide.columns]. It loops through each col in surveys_wide.columns and for each column it creates an f-string f\"yr_{col}. The value of col gets taken from whatever the column name is, in our case a number representing the year.\n\n\n\n\n\n\n\n14.3.2 Adjusting numbers\nIn the joining section we saw that it wasn’t great practice to just use numbers to indicate plot_id, since they obviously have no numerical value, but instead define a category. This is something that often occurs, so it’s useful to know how to be able to format them differently.\nFor example, we could encode them in the format plot_xxx where xxx is a number with leading zeros (so that it sorts nicely).\nWe can do that as follows:\n\nRPython\n\n\n\nplot_types |&gt; \n  mutate(plot_id = paste0(\"plot_\", sprintf(\"%03d\", plot_id)))\n\n# A tibble: 5 × 2\n  plot_id  plot_type                \n  &lt;chr&gt;    &lt;chr&gt;                    \n1 plot_001 Spectab exclosure        \n2 plot_002 Control                  \n3 plot_003 Long-term Krat Exclosure \n4 plot_004 Rodent Exclosure         \n5 plot_005 Short-term Krat Exclosure\n\n\n\n\n\n\n\n\nTipAlternative: using the stringr package\n\n\n\n\n\nThe stringr package is part of tidyverse and has a whole range of functions that allow you to change strings (text). The equivalent of the code above would be:\n\nplot_types |&gt; \n  mutate(plot_id = str_c(\"plot_\", str_pad(plot_id, width = 3, pad = \"0\")))\n\n# A tibble: 5 × 2\n  plot_id  plot_type                \n  &lt;chr&gt;    &lt;chr&gt;                    \n1 plot_001 Spectab exclosure        \n2 plot_002 Control                  \n3 plot_003 Long-term Krat Exclosure \n4 plot_004 Rodent Exclosure         \n5 plot_005 Short-term Krat Exclosure\n\n\nYou can read this as: “Use mutate() to update the plot_id column, where (=) the contents of plot_id is a combined string (str_c) containing the text \"plot_ and we pad out (str_pad) plot_id so it’s 3 digits wide (width = 3) and we pad out with \"0\" (pad = \"0\").”\n\n\n\nIf you wanted to update the column, just add plot_types &lt;- in front of this!\n\n\n\nplot_types[\"plot_id\"].apply(\n    lambda x: f\"plot_{x:03d}\"\n)\n\n0    plot_001\n1    plot_002\n2    plot_003\n3    plot_004\n4    plot_005\nName: plot_id, dtype: object\n\n\n\n\n\n\n\n\nTipAlternative: using .zfill()\n\n\n\n\n\nIf you’re not keen on this method using the lambda x: approach, you can use .zfill:\n\n\"plot_\" + plot_types[\"plot_id\"].astype(str).str.zfill(3)\n\n0    plot_001\n1    plot_002\n2    plot_003\n3    plot_004\n4    plot_005\nName: plot_id, dtype: object\n\n\nYou can read this as: “Take the plot_id column from plot_types (plot_types[\"plot_id\"]) and update it (=) with a combination of the text plot_ (\"plot_\") and the value of plot_types[\"plot_id\"], which needs to be converted to a string (.astype(str)) before we can fill it with zeros up to a maximum of 3 digits (.str.zfill(3)).”\n\n\n\nIf you wanted to update the column, just add plot_types[\"plot_id\"] = in front of this!\n\n\n\nNote: this means that you would also have to change the plot_id column values in the surveys data set, if you wanted to combine the data from these tables!\n\n\n14.3.3 Encoding issues\nIn the messy_data data set it’s not just the column names that are an issue. There are quite a few different encoding issues. We will address several of them now, but some of these you’ll investigate later in the exercises.\n\n\n\n\n\n\n\nVariable\nProblem(s)\n\n\n\n\nid\nClean — serves as a unique identifier\n\n\nage\n(to investigate later)\n\n\ngender\n(to investigate later)\n\n\nscore\nNumeric-like variable contains text entries: \"five\", \"high\", NA\n\n\nincome_gbp\n(to investigate later)\n\n\ncountry\nInconsistent naming for UK: \"UK\", \"U.K.\", \"United Kingdom\", \"United kingdom\", NA\n\n\nemployed_or_not\nInconsistent boolean values: \"yes\", \"y\", \"TRUE\", \"n\", NA, etc.\n\n\nnotes\nFree-text notes with mixed missing indicators: \"N/A\", \"\", \"none\", NA\n\n\n\nLet’s focus on the country column, which is a classical case of “different people have different ways of encoding the same thing”.\nFirst, we check what entries we have in this column. Apart from just showing the different types of entries, we’re also looking at how many times they occur.\n\nRPython\n\n\n\nmessy_data |&gt; \n  count(country)\n\n# A tibble: 5 × 2\n  country            n\n  &lt;chr&gt;          &lt;int&gt;\n1 U.K.              18\n2 UK                24\n3 United Kingdom    19\n4 United kingdom    20\n5 &lt;NA&gt;              19\n\n\n\n\n\ncounts = (messy_data\n          .groupby(\"country\")\n          .size()\n          .reset_index(name = \"n\"))\n\ncounts\n\n          country   n\n0            U.K.  18\n1              UK  24\n2  United Kingdom  19\n3  United kingdom  20\n\n\n\n\n\nWe’ve got quite some variation going on here. A convenient way of addressing this is to create a list of substitutions and then apply that to the data. This makes it easier in the future if you get more variations that you need to update.\n\nRPython\n\n\nIn R we have the case_when() function, which can help you do exactly that. It’s more general though: it allows you to re-encode values based on conditions. So, before we apply it to our data, let’s go through a simplified example.\n\n\n\n\n\n\nTipUsing case_when() for recoding values\n\n\n\nIt works in this way:\n\n# 1. create some dummy data\ndf &lt;- tibble(score = c(45, 67, 82, 90, 55))\n\n# 2. recode the values\ndf &lt;- df |&gt; \n  mutate(performance = case_when(\n    score &lt; 60               ~ \"fail\",\n    score &gt;= 60 & score &lt; 80 ~ \"pass\",\n    TRUE                     ~ \"excellent\"\n  ))\n\n# 3. show the result\ndf\n\n# A tibble: 5 × 2\n  score performance\n  &lt;dbl&gt; &lt;chr&gt;      \n1    45 fail       \n2    67 pass       \n3    82 excellent  \n4    90 excellent  \n5    55 fail       \n\n\nIn the mini-example above we create a simple data set with one column: score. There are 5 values in this column. We now want to assign a performance value to it, which will differ based on the score.\nThe case_when() function goes through each possible defined comparison (e.g. score &lt; 60, ...) and then assigns the value based on that (~  \"fail\"). We put the whole thing in a mutate() because we are creating a new column: performance.\nAt the end of the case_when() we have TRUE ~ \"excellent. The TRUE ~ designation means: “for everything else do…”.\n\n\nLet’s apply this to our messy_data.\n\nmessy_data &lt;- messy_data |&gt; \n  mutate(country = case_when(\n    country %in% c(\"United kingdom\", \"U.K.\", \"UK\") ~ \"United Kingdom\",\n    TRUE ~ country\n  ))\n\n\ncountry =indicates that we are updating our country column.\ncountry %in% c(\"United kingdom\", \"U.K.\", \"UK\") says, for each value in country, compare it to the values within c()\n~ \"United Kingdom\" if it finds it, replace it with `“United Kingdom”\nTRUE ~ country otherwise, leave it unchanged (including the missing values)\n\n\n\nLet’s apply this to our messy_data.\nFirst we create a “translation” table: which values do we want to re-encode and how?\n\n# Define standard mapping for inconsistent names\ncountry_map = {\n    \"United kingdom\": \"United Kingdom\",\n    \"U.K.\": \"United Kingdom\",\n    \"UK\": \"United Kingdom\"\n    # Add more variations if needed\n}\n\nNext, we apply this to our data:\n\n# Update the country names\nmessy_data[\"country\"] = (\n    messy_data[\"country\"]\n    .replace(country_map))\n\n\n\n\nWe can view the end result:\n\nRPython\n\n\n\nmessy_data |&gt; \n  count(country)\n\n# A tibble: 2 × 2\n  country            n\n  &lt;chr&gt;          &lt;int&gt;\n1 United Kingdom    81\n2 &lt;NA&gt;              19\n\n\n\n\n\ncounts = (messy_data\n         .groupby(\"country\")\n         .size()\n         .reset_index(name = \"n\"))\n\ncounts\n\n          country   n\n0  United Kingdom  81\n\n\n\n\n\n\n\n14.3.4 Boolean values\nWe have a column employed_or_not, which contains information on the employment status of each person. This is encoded inconsistently, as \"yes, \"y\", \"TRUE\", …\nApart from being consistent, it’s useful to keep columns that can only have 1 of 3 value (true, false, missing) as a boolean. This makes it easier to filter and tally contents than if we’d encode it as text.\nLet’s tackle this issue in a similar approach as above. First, let’s see what we’re dealing with here, by looking up the column types and the contents of employed_or_not.\n\nRPython\n\n\n\nclass(messy_data$employed_or_not)\n\n[1] \"character\"\n\n\n\nmessy_data |&gt;\n  count(employed_or_not)\n\n# A tibble: 7 × 2\n  employed_or_not     n\n  &lt;chr&gt;           &lt;int&gt;\n1 FALSE               9\n2 TRUE               15\n3 n                  14\n4 no                 17\n5 y                  15\n6 yes                12\n7 &lt;NA&gt;               18\n\n\nWe can see that the column is viewed as a \"chr\" or character column.\n\n\n\nmessy_data[\"employed_or_not\"].dtype\n\ndtype('O')\n\n\nWe can see that the column is viewed as an object type ('O'). This type usually holds text, but can also be used for a mixture of Python objects.\nLet’s see what kind of values we have in our column:\n\ncounts = (messy_data\n          .groupby(\"employed_or_not\", dropna = False)\n          .size()\n          .reset_index(name = \"n\"))\n\ncounts\n\n  employed_or_not   n\n0           FALSE   9\n1            TRUE  15\n2               n  14\n3              no  17\n4               y  15\n5             yes  12\n6             NaN  18\n\n\n\n\n\nThe reason why we end up with a generic data type is because there is a mixture of data in the column. Let’s fixed that.\n\nRPython\n\n\n\nmessy_data &lt;- messy_data |&gt; \n  mutate(employed_or_not = case_when(\n    employed_or_not %in% c(\"no\", \"n\") ~ \"FALSE\",\n    employed_or_not %in% c(\"yes\", \"y\") ~ \"TRUE\",\n    TRUE ~ employed_or_not\n  ))\n\nLet’s count again:\n\nmessy_data |&gt; \n  count(employed_or_not)\n\n# A tibble: 3 × 2\n  employed_or_not     n\n  &lt;chr&gt;           &lt;int&gt;\n1 FALSE              40\n2 TRUE               42\n3 &lt;NA&gt;               18\n\n\nBut the employed_or_not column is still viewed as character:\n\nclass(messy_data$employed_or_not)\n\n[1] \"character\"\n\n\nSo, we need to force it to view it as such.\nTo make it a boolean / logical column, we use the as.logical() function.\n\nmessy_data &lt;- messy_data |&gt; \n  mutate(employed_or_not = as.logical(employed_or_not))\n\nWe check one last time:\n\nclass(messy_data$employed_or_not)\n\n[1] \"logical\"\n\n\n\n\n\n# Define standard mapping for inconsistent names\nemployment_map = {\n    \"no\": \"False\",\n    \"n\": \"False\",\n    \"yes\": \"True\",\n    \"y\": \"True\"\n    # Add more variations if needed\n}\n\nNext, we apply this to our data:\n\n# Update the country names\nmessy_data[\"employed_or_not\"] = (\n    messy_data[\"employed_or_not\"]\n    .replace(employment_map))\n\nWe count again to check what has happened:\n\ncounts = (\n    messy_data\n    .groupby(\"employed_or_not\", dropna = False)\n    .size()\n    .reset_index(name = \"n\")\n)\n\ncounts\n\n  employed_or_not   n\n0           FALSE   9\n1           False  31\n2            TRUE  15\n3            True  27\n4             NaN  18\n\n\nThis still shows a difference between FALSE and False, and TRUE and True. That’s because the all-caps words are interpreted as text. In fact, all of them are viewed as text, because we used \" \" in our mapping.\nPython’s boolean phrases are True and False, so we need to adjust that.\nTo do this, we first convert all the values to lower case with .str.lower(), and then convert those values to boolean (without \" \"). Lastly, we convert the column to \"boolean\". We need to do that last step, because we have missing data. If we didn’t, then Python would keep all the contents as text.\n\nmessy_data[\"employed_or_not\"] = (\n    messy_data[\"employed_or_not\"]\n    .str.lower()\n    .map({\"true\": True, \"false\": False})\n    .astype(\"boolean\")     # convert to boolean type\n)\n\nWe can see that we only have True, False or &lt;NA&gt;.\n\ncounts = (\n    messy_data\n    .groupby(\"employed_or_not\", dropna = False)\n    .size()\n    .reset_index(name = \"n\")\n)\n\ncounts\n\n   employed_or_not   n\n0            False  40\n1             True  42\n2             &lt;NA&gt;  18\n\n\nAnd our data type is now correct.\n\nmessy_data[\"employed_or_not\"].dtype\n\nBooleanDtype\n\n\n\n\n\nSuccess!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#cleaning-plots",
    "href": "materials/da4-11-cleaning-data.html#cleaning-plots",
    "title": "14  Clean, style & arrange",
    "section": "14.4 Cleaning plots",
    "text": "14.4 Cleaning plots\nOften we need to put a bit of work into making plots more presentable. Perhaps default colours are not quite right, or labels need adjusting. Below we cover some common changes.\nLet’s use the surveys_left data set for the next few sections.If you haven’t done so already, please load the following:\n\nRPython\n\n\n\nsurveys &lt;- read_csv(\"data/surveys.csv\")\nplot_types &lt;- read_csv(\"data/plots.csv\")\nsurveys_left &lt;- left_join(surveys, plot_types, by = \"plot_id\")\n\n\n\n\nsurveys = pd.read_csv(\"data/surveys.csv\")\nplot_types = pd.read_csv(\"data/plots.csv\")\nsurveys_left = pd.merge(surveys, plot_types, how = \"left\", on = \"plot_id\")\n\n\n\n\n\n14.4.1 Adding titles and axis labels\nFirst we visualise some data. Here we are creating a violin plot. Partly because we can, but mostly because they are useful. They can tell you a lot about how your data are distributed.\n\nRPython\n\n\n\nggplot(surveys_left, aes(x = plot_type, y = hindfoot_length)) +\n         geom_violin()\n\nWarning: Removed 4111 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\nFigure 14.1: Violin plot of hindfoot_length for each plot type\n\n\n\n\n\n\n\n\np = (ggplot(surveys_left, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n         geom_violin())\n\np.show()\n\n\n\n\n\n\n\nFigure 14.2: Violin plot of hindfoot_length for each plot type\n\n\n\n\n\n\n\n\nIt’d be nice to have plot title, as well as some clearer (and nicer) axis labels. We can do this as follows:\n\nRPython\n\n\n\nggplot(surveys_left, aes(x = plot_type, y = hindfoot_length)) +\n         geom_violin() +\n  labs(title = \"Violin plot for hindfoot length by plot type\",\n       subtitle = \"1977 - 2022\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\")\n\n\n\n\n\n\n\nFigure 14.3: Violin plot of hindfoot_length for each plot type\n\n\n\n\n\n\n\n\np = (ggplot(surveys_left, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n         geom_violin() +\n       labs(title = \"Violin plot for hindfoot length by plot type\",\n            subtitle = \"1977 - 2022\",\n            x = \"Plot type\",\n            y = \"Hindfoot length (mm)\"))\n\np.show()\n\n\n\n\n\n\n\nFigure 14.4: Violin plot of hindfoot_length for each plot type\n\n\n\n\n\n\n\n\n\n\n14.4.2 Text axis labels: rotate\nAnother common issue is that axis text labels are poorly aligned. In the plots above the plot_type values are too long and the text overlaps. Let’s fix this. We’ll do this in two ways:\n\nRotating the axis text\nWrapping / inserting a line break in the axis text\n\n\nRPython\n\n\n\nggplot(surveys_left, aes(x = plot_type, y = hindfoot_length)) +\n  geom_violin() +\n  labs(title = \"Violin plot for hindfoot length by plot type\",\n       subtitle = \"1977 - 2022\",\n       x = \"Plot type\",\n       y = \"Hindfoot length (mm)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nFigure 14.5: Rotating axis text labels.\n\n\n\n\n\n\n\n\np = (ggplot(surveys_left, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n         geom_violin() +\n       labs(title = \"Violin plot for hindfoot length by plot type\",\n            subtitle = \"1977 - 2022\",\n            x = \"Plot type\",\n            y = \"Hindfoot length (mm)\") +\n        theme(axis_text_x = element_text(rotation = 45, ha = \"right\")))\n\np.show()\n\n\n\n\n\n\n\nFigure 14.6: Rotating axis text labels.\n\n\n\n\n\n\n\n\n\n\n14.4.3 Text axis labels: wrap\nRotating the text labels can work in a lot of cases, but doesn’t get round the issue of long strings. The plot_type values aren’t exactly concise. So, it might be better to have some text wrapping instead.\n\nRPython\n\n\n\nggplot(surveys_left, aes(x = plot_type, y = hindfoot_length)) +\n  geom_violin() +\n  labs(\n    title = \"Violin plot for hindfoot length by plot type\",\n    subtitle = \"1977 - 2022\",\n    x = \"Plot type\",\n    y = \"Hindfoot length (mm)\") +\n  scale_x_discrete(labels = function(x) str_wrap(x, width = 10))\n\n\n\n\n\n\n\nFigure 14.7: Wrapping axis text labels.\n\n\n\n\n\n\n\nPython has a packages called textwrap that can help here.\n\nimport textwrap\n\nAlthough the code is a bit convoluted (it’s a case of copy/paste in future plots, really) it does result in a much cleaner plot:\n\np = (ggplot(surveys_left, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n         geom_violin() +\n       labs(title = \"Violin plot for hindfoot length by plot type\",\n            subtitle = \"1977 - 2022\",\n            x = \"Plot type\",\n            y = \"Hindfoot length (mm)\") +\n       scale_x_discrete(labels = lambda labels: [\n                          textwrap.fill(str(l), 10) if l is not None else \"\"\n                          for l in labels]))\n\np.show()\n\n\n\n\n\n\n\nFigure 14.8: Wrapping axis text labels.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#changing-colours",
    "href": "materials/da4-11-cleaning-data.html#changing-colours",
    "title": "14  Clean, style & arrange",
    "section": "14.5 Changing colours",
    "text": "14.5 Changing colours\nOften we want to use colours to emphasise parts of our plot or to highlight different groups within our data. Default options might not always be the most appropriate ones, so it’s good to know how to control the aesthetics of your plot. We’ll cover that below. We’ll use a small subset of the data so we can highlight the differences better.\n\nRPython\n\n\n\nsurveys_y2k &lt;- surveys_left |&gt; \n  filter(plot_type %in% c(\"Control\", \"Rodent Exclosure\")) |&gt; \n  filter(year %in% c(2000, 2001, 2002)) |&gt; \n  drop_na()\n\n\n\n\nsurveys_y2k = (\n    surveys_left[\n        surveys_left[\"plot_type\"].isin([\"Control\", \"Rodent Exclosure\"])\n        & surveys_left[\"year\"].isin([2000, 2001, 2002])]\n    .dropna())\n\n\n\n\n\n14.5.1 Colours: default\nThe default here is various shades of grey.\n\nRPython\n\n\n\nggplot(surveys_y2k, aes(x = plot_type, y = hindfoot_length)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1)\n\n\n\n\n\n\n\nFigure 14.9: Boxplot and jittered points, default.\n\n\n\n\n\n\n\n\np = (ggplot(surveys_y2k, aes(x = \"plot_type\", y = \"hindfoot_length\")) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1))\n  \np.show()\n\n\n\n\n\n\n\nFigure 14.10: Boxplot and jittered points, default.\n\n\n\n\n\n\n\n\nLet’s colour some of our data using the information from the sex variable.\n\nRPython\n\n\n\nggplot(surveys_y2k, aes(x = plot_type, y = hindfoot_length, fill = sex)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1)\n\n\n\n\n\n\n\nFigure 14.11: Boxplot and jittered points, default colours.\n\n\n\n\n\n\n\n\np = (ggplot(surveys_y2k,\n            aes(x = \"plot_type\", y = \"hindfoot_length\", fill = \"sex\")) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, fill = \"black\"))\n  \np.show()\n\n\n\n\n\n\n\nFigure 14.12: Boxplot and jittered points, default colours.\n\n\n\n\n\n\n\n\nWe can see that some default colours get applied. Let’s update this with some other default colour scheme.\n\n\n\n\n\n\nNoteColour inheritance in ggplot2 vs plotnine\n\n\n\n\n\nYou might have noticed that we’ve specifically given the instruction to geom_jitter() to use black: geom_jitter(width = 0.1, fill = \"black\")). This is because colour inheritance in plotnine works subtly different from ggplot2.\nIn ggplot2, geom_jitter() by default uses the colour aesthetic (outline color) and does not inherit fill (which is used for boxplots and polygons). This is why points appear black in R. However, plotnine is less consistent and therefor you need to specify the colour yourself, otherwise the geom_jitter() data would also get coloured by sex in the example above.\n\n\n\n\n\n14.5.2 Colours: standard palettes\nA popular colour scheme is the so-called Brewer palette. Although originally developed for R, the colour combinations are appropriate in many contexts. It has colour palettes that are appropriate for data continuous data or ordered data, where colour signifies a difference in magnitude. It also contains colour palettes for qualitative data, where the emphasis is on creating visual separation between different groups.\n\nRPython\n\n\nThe brewer set of colours can be called using a set of functions in ggplot2. We’ll mostly use two of these:\n\nscale_fill_brewer() for objects in a plot that have a surface area (such as boxplots)\nscale_colour_brewer() for objects that do not have a surface area (such as points)\n\nWe simply define the one we want, then add the type of palette with palette =. Here we use \"Dark2\", which is particularly suited for clear separation between categories.\n\nggplot(surveys_y2k, aes(x = plot_type, y = hindfoot_length, fill = sex)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1) +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nFigure 14.13: Boxplot and jittered points, Brewer (Dark2) colours.\n\n\n\n\n\n\n\nThe brewer set of colours can be called using a set of functions in plotnine. We’ll mostly use two of these:\n\nscale_fill_brewer() for objects in a plot that have a surface area (such as boxplots)\nscale_colour_brewer() for objects that do not have a surface area (such as points)\n\nWe simply define the one we want, then add the type of palette with palette =. Here we use \"Dark2\", which is particularly suited for clear separation between categories. We also need to define that the data are qualitative, using type = qual.\n\np = (ggplot(surveys_y2k,\n            aes(x = \"plot_type\", y = \"hindfoot_length\", fill = \"sex\")) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, fill = \"black\") +\n  scale_fill_brewer(type = \"qual\", palette = \"Dark2\"))\n  \np.show()\n\n\n\n\n\n\n\nFigure 14.14: Boxplot and jittered points, Brewer (Dark2) colours.\n\n\n\n\n\n\n\n\n\n\n14.5.3 Colours: colour-blind friendly\nColour blindness is a reasonably common occurrence, with around 4.5% of the U.K. population having some form, according to Colour Blind Awareness (2025).\nThe brewer palettes often have a good visual separation, but might not always be the most optimal ones. There is a set of colour-blind friendly colours developed by Okabe and Ito (2008).\n\n\n\n\n\n\n\n\nFigure 14.15: Colour-blind friendly colour palette by Okabe and Ito (2008).\n\n\n\n\n\nIf we want to apply these custom colours, we’ll have to do that manually. The easiest way is to assign them to a manual colour palette, and then use that when plotting.\n\nRPython\n\n\n\ncb_palette   &lt;- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                  \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\n\nggplot(surveys_y2k, aes(x = plot_type, y = hindfoot_length, fill = sex)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1) +\n  scale_fill_manual(values = cb_palette)\n\n\n\n\n\n\n\nFigure 14.16: Boxplot and jittered points, Okabe and Ito colours.\n\n\n\n\n\n\n\n\ncb_palette = [\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n              \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"]\n\n\np = (ggplot(surveys_y2k,\n            aes(x = \"plot_type\", y = \"hindfoot_length\", fill = \"sex\")) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1, fill = \"black\") +\n  scale_fill_manual(values = cb_palette))\n  \np.show()\n\n\n\n\n\n\n\nFigure 14.17: Boxplot and jittered points, Okabe and Ito colours.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#arranging-plots",
    "href": "materials/da4-11-cleaning-data.html#arranging-plots",
    "title": "14  Clean, style & arrange",
    "section": "14.6 Arranging plots",
    "text": "14.6 Arranging plots\nWe’ve already seen that we can use facetting to divide plots into sub-plots. Sometimes it can be very useful to combine completely separate plots into a single figure. We’ll show how to do this below.\nLet’s keep using our surveys_y2k data set. We’ll first create three plots (randomly chosen) to play around with. Just copy/paste the code below.\n\nRPython\n\n\n\np1 &lt;- ggplot(surveys_y2k, aes(x = plot_type, y = weight)) +\n  geom_boxplot() +\n  labs(title = \"plot 1\")\n\np2 &lt;- ggplot(surveys_y2k, aes(x = weight, y = hindfoot_length)) +\n  geom_point() +\n  labs(title = \"plot 2\")\n\np3 &lt;- ggplot(surveys_y2k, aes(x = factor(month), y = weight)) +\n  geom_boxplot() +\n  labs(title = \"plot 3\")\n\n\n\n\np1 = (ggplot(surveys_y2k, aes(x='plot_type', y='weight')) +\n      geom_boxplot() +\n      labs(title=\"plot 1\"))\n\np2 = (ggplot(surveys_y2k, aes(x='weight', y='hindfoot_length')) +\n      geom_point() +\n      labs(title=\"plot 2\"))\n\np3 = (ggplot(surveys_y2k, aes(x='factor(month)', y='weight')) +\n      geom_boxplot() +\n      labs(title=\"plot 3\"))\n\n\n\n\n\nRPython\n\n\nIn R we can use the patchwork package. Install it, if needed, with:\n\ninstall.packages(\"patchwork\")\n\nAnd then load it:\n\nlibrary(patchwork)\n\n\n\n\n\n\n\nNote\n\n\n\nWe’re not covering the functionality of patchwork extensively here, since there is a very good vignette doing that. So, for more details, please refer to that link!\n\n\n\n\nIn Python there is a patchworklib that tries to emulate what R’s patchwork is doing. However, it seems to run into issues quite quickly when plotnine gets updated.\nSo, we don’t provide code for this for the time being. We’ll revisit this once the package is more stable.\n\n\n\nWe can easily combine the 3 plots we created into a single panel.\n\nRPython\n\n\n\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOften it’s helpful to add labels, either to the figure itself and/or to the subpanels.\n\nRPython\n\n\nWe can add a title using the title = and add tags using tag_levels = arguments. These get added to the plot, using plot_annotation():\n\n(p1 + p2) / p3 +\n  plot_annotation(title = \"Fig. 1: Trialling composite plots\",\n                  tag_levels = \"a\")\n\n\n\n\n\n\n\n\nYou can change the tag_levels = value to \"A\" for uppercase tags, or \"1\" for numbers (which wouldn’t be very clear).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/da4-11-cleaning-data.html#summary",
    "href": "materials/da4-11-cleaning-data.html#summary",
    "title": "14  Clean, style & arrange",
    "section": "14.7 Summary",
    "text": "14.7 Summary\n\n\n\n\n\n\nTipKey points\n\n\n\n\nRPython\n\n\n\nWe can use the clean_names() function from the janitor package to clean up column names, avoiding spaces, inconsistent case use etc.\nTry to avoid ID columns with numbers only, but instead use a prefix, so the data are not viewed as numerical.\nCommon encoding issues include: inconsistent (upper) case use (e.g. Belgium vs belgium); poorly defined boolean values (y / yes instead of TRUE or FALSE).\nIt is good practice to include a plot title and well-defined x and y axis labels.\nEnsure axes labels are clear by wrapping text or, less ideal, rotating them.\nUse colours wisely, ensuring separation between groups.\nConsider colour blindness in your audience and use colour-blind safe colour schemes.\nArrange plots using patchwork\n\n\n\n\nWe can use the clean_names() function from the pyjanitor package to clean up column names, avoiding spaces, inconsistent case use etc.\nTry to avoid ID columns with numbers only, but instead use a prefix, so the data are not viewed as numerical.\nCommon encoding issues include: inconsistent (upper) case use (e.g. Belgium vs belgium); poorly defined boolean values (y / yes instead of True or False).\nIt is good practice to include a plot title and well-defined x and y axis labels.\nEnsure axes labels are clear by wrapping text or, less ideal, rotating them.\nUse colours wisely, ensuring separation between groups.\nConsider colour blindness in your audience and use colour-blind safe colour schemes.\n\n\n\n\n\n\n\n\n\n\nColour Blind Awareness. 2025. “Colour Blindness.” https://www.colourblindawareness.org/colour-blindness/.\n\n\nOkabe, Masatake, and Kei Ito. 2008. “Color Universal Design (CUD) - How to Make Figures and Presentations That Are Friendly to Colorblind People -.” https://jfly.uni-koeln.de/color/.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Clean, style & arrange</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html",
    "href": "materials/trainers-live-demo.html",
    "title": "15  Live demo exercises",
    "section": "",
    "text": "15.1 Section setup",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#section-setup",
    "href": "materials/trainers-live-demo.html#section-setup",
    "title": "15  Live demo exercises",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(tidyverse)\n\n\n\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# If using seaborn for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da1-getting-started",
    "href": "materials/trainers-live-demo.html#da1-getting-started",
    "title": "15  Live demo exercises",
    "section": "15.2 DA1: getting started",
    "text": "15.2 DA1: getting started\nThis is a hands-on demo.\n\nRPython\n\n\nLive demo RStudio:\n\nopen RStudio\nhighlight panels\nchange default settings (.Rdata and not save workspace)\nset up new RProject data-analysis\ncreate the subfolders (data, images, scripts)\ncreate a script\n\n\n\nLive demo JupyterLab:\n\nopen JupyterLab\nhighlight Launcher\nshow how to check working directory, change if needed (data-analysis)\ncreate the subfolders (data, images, scripts)\ncreate a Notebook",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da1-data-types",
    "href": "materials/trainers-live-demo.html#da1-data-types",
    "title": "15  Live demo exercises",
    "section": "15.3 DA1: data types",
    "text": "15.3 DA1: data types\nImportant to realise that there often is some order to how data types are interpreted. For example, look at the following:\n\nRPython\n\n\n\nexample &lt;- c(22, 87, NA, 32)\n\n\nclass(example)\n\n[1] \"numeric\"\n\n\n\nexample &lt;- c(22, 87, NA, \"unsure\")\n\n\nclass(example)\n\n[1] \"character\"",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da1-indexing",
    "href": "materials/trainers-live-demo.html#da1-indexing",
    "title": "15  Live demo exercises",
    "section": "15.4 DA1: indexing",
    "text": "15.4 DA1: indexing\nA bit of a technical detail, but an important one, particularly if you’re using both R and Python. The main focus of the example is not on the subsetting, it’s on the fact that R has 1-based indexing and Python has 0-based indexing.\nFurthermore, that R’s indexing is inclusive, and Python’s exclusive (comparing 1:3 in R vs Python). That said, it’s to create awareness of the fact that indexing has to start somewhere and that it can be different between programming languages.\n\nRPython\n\n\n\nwinnings &lt;- c(\"first\", \"second\", \"third\", \"fourth\")\n\nThe positional index is as follows:\n\nseq_along(winnings)\n\n[1] 1 2 3 4\n\n\nSo, if we wanted to select the first and third values, we’d do:\n\nwinnings[c(1, 3)]\n\n[1] \"first\" \"third\"\n\n\n\n\n\nwinnings = [\"first\", \"second\", \"third\", \"fourth\"]\n\nThe positional index is as follows:\n\nlist(range(len(winnings)))\n\n[0, 1, 2, 3]\n\n\nSo, if we wanted to select the first and third values, we’d do:\n\nwinnings[0]\n\n'first'\n\nwinnings[2]\n\n'third'",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da2-data-exploration",
    "href": "materials/trainers-live-demo.html#da2-data-exploration",
    "title": "15  Live demo exercises",
    "section": "15.5 DA2: data exploration",
    "text": "15.5 DA2: data exploration\n\nRPython\n\n\nRead in the data:\n\ninfections &lt;- read_csv(\"data/infections.csv\")\n\nAnd have a look:\n\nhead(infections)\n\n# A tibble: 6 × 11\n  patient_id hospital   quarter infection_type vaccination_status age_group\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;    \n1 ID_0001    hospital_3 Q2      none           &lt;NA&gt;               65+      \n2 ID_0002    hospital_3 Q2      viral          &lt;NA&gt;               18 - 64  \n3 ID_0003    hospital_2 Q2      none           unknown            65+      \n4 ID_0004    hospital_2 Q3      fungal         unvaccinated       &lt; 18     \n5 ID_0005    hospital_3 Q2      fungal         vaccinated         65+      \n6 ID_0006    hospital_5 Q3      none           vaccinated         65+      \n# ℹ 5 more variables: icu_admission &lt;lgl&gt;, symptoms_count &lt;dbl&gt;,\n#   systolic_pressure &lt;dbl&gt;, body_temperature &lt;dbl&gt;, crp_level &lt;dbl&gt;\n\n\n\n\nRead in the data:\n\ninfections = pd.read_csv(\"data/infections.csv\")\n\nAnd have a look:\n\ninfections.head()\n\n  patient_id    hospital  ... body_temperature crp_level\n0    ID_0001  hospital_3  ...             37.8     12.05\n1    ID_0002  hospital_3  ...             39.1      8.11\n2    ID_0003  hospital_2  ...             38.5      5.24\n3    ID_0004  hospital_2  ...             39.4     41.73\n4    ID_0005  hospital_3  ...             36.9     10.51\n\n[5 rows x 11 columns]\n\n\n\n\n\n\n15.5.1 Data structure\nNumber of rows & columns:\n\nRPython\n\n\n\nnrow(infections)\n\n[1] 1400\n\n\n\nncol(infections)\n\n[1] 11\n\n\n\n\n\ninfections.shape[0]\n\n1400\n\n\n\ninfections.shape[1]\n\n11\n\n\n\n\n\nIt’s good to look at the column attributes: what type of columns are we dealing with and is it what we expect?\n\nRPython\n\n\n\nsummary(infections)\n\n  patient_id          hospital           quarter          infection_type    \n Length:1400        Length:1400        Length:1400        Length:1400       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n vaccination_status  age_group         icu_admission   symptoms_count  \n Length:1400        Length:1400        Mode :logical   Min.   : 0.000  \n Class :character   Class :character   FALSE:814       1st Qu.: 6.000  \n Mode  :character   Mode  :character   TRUE :513       Median : 9.000  \n                                       NA's :73        Mean   : 8.549  \n                                                       3rd Qu.:11.000  \n                                                       Max.   :21.000  \n                                                       NA's   :67      \n systolic_pressure body_temperature   crp_level     \n Min.   : 87.0     Min.   :36.30    Min.   : 1.000  \n 1st Qu.:118.0     1st Qu.:38.20    1st Qu.: 9.117  \n Median :125.0     Median :38.80    Median :16.215  \n Mean   :125.1     Mean   :38.75    Mean   :19.465  \n 3rd Qu.:132.0     3rd Qu.:39.40    3rd Qu.:26.433  \n Max.   :163.0     Max.   :41.50    Max.   :58.860  \n NA's   :74        NA's   :67       NA's   :156     \n\n\n\nstr(infections)\n\nspc_tbl_ [1,400 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ patient_id        : chr [1:1400] \"ID_0001\" \"ID_0002\" \"ID_0003\" \"ID_0004\" ...\n $ hospital          : chr [1:1400] \"hospital_3\" \"hospital_3\" \"hospital_2\" \"hospital_2\" ...\n $ quarter           : chr [1:1400] \"Q2\" \"Q2\" \"Q2\" \"Q3\" ...\n $ infection_type    : chr [1:1400] \"none\" \"viral\" \"none\" \"fungal\" ...\n $ vaccination_status: chr [1:1400] NA NA \"unknown\" \"unvaccinated\" ...\n $ age_group         : chr [1:1400] \"65+\" \"18 - 64\" \"65+\" \"&lt; 18\" ...\n $ icu_admission     : logi [1:1400] FALSE FALSE TRUE TRUE TRUE FALSE ...\n $ symptoms_count    : num [1:1400] 1 6 3 7 7 5 10 12 13 7 ...\n $ systolic_pressure : num [1:1400] 117 115 120 129 114 124 133 120 124 127 ...\n $ body_temperature  : num [1:1400] 37.8 39.1 38.5 39.4 36.9 36.8 39.4 39.3 39.6 39.1 ...\n $ crp_level         : num [1:1400] 12.05 8.11 5.24 41.73 10.51 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   patient_id = col_character(),\n  ..   hospital = col_character(),\n  ..   quarter = col_character(),\n  ..   infection_type = col_character(),\n  ..   vaccination_status = col_character(),\n  ..   age_group = col_character(),\n  ..   icu_admission = col_logical(),\n  ..   symptoms_count = col_double(),\n  ..   systolic_pressure = col_double(),\n  ..   body_temperature = col_double(),\n  ..   crp_level = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\ninfections.describe()\n\n       symptoms_count  systolic_pressure  body_temperature    crp_level\ncount     1333.000000        1326.000000       1333.000000  1244.000000\nmean         8.549137         125.070136         38.751988    19.464751\nstd          3.825004          10.346761          0.883689    13.083907\nmin          0.000000          87.000000         36.300000     1.000000\n25%          6.000000         118.000000         38.200000     9.117500\n50%          9.000000         125.000000         38.800000    16.215000\n75%         11.000000         132.000000         39.400000    26.432500\nmax         21.000000         163.000000         41.500000    58.860000\n\n\n\n\n\n\n\n15.5.2 Quality control checks\nIt’s good to do some basic sanity / quality control checks. For example, if there are different categories in a column, do all the categories we expect show up or are there missing ones / misspelled etc.?\nFor example, we can check the unique values in a column:\n\nRPython\n\n\n\nunique(infections$infection_type)\n\n[1] \"none\"      \"viral\"     \"fungal\"    NA          \"bacterial\"\n\n\n\n\n\ninfections[\"infection_type\"].unique()\n\narray(['none', 'viral', 'fungal', nan, 'bacterial'], dtype=object)\n\n\n\n\n\nWe can count the number of missing values in the column infection_type.\n\nRPython\n\n\nYou read the code “inside-out”:\n\nsum(is.na(infections$infection_type))\n\n[1] 74\n\n\n\n\n\ninfections[\"infection_type\"].isna().sum()\n\n74",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da2-subsetting-tables",
    "href": "materials/trainers-live-demo.html#da2-subsetting-tables",
    "title": "15  Live demo exercises",
    "section": "15.6 DA2: subsetting tables",
    "text": "15.6 DA2: subsetting tables\nLet’s select patient_id:\n\nRPython\n\n\n\ninfections$patient_id\n\n\n\n\ninfections.patient_id\n\n0       ID_0001\n1       ID_0002\n2       ID_0003\n3       ID_0004\n4       ID_0005\n         ...   \n1395    ID_1396\n1396    ID_1397\n1397    ID_1398\n1398    ID_1399\n1399    ID_1400\nName: patient_id, Length: 1400, dtype: object\n\n\n\n\n\nOr more than 1 column, by column name:\n\nRPython\n\n\n\ninfections[, c(\"patient_id\", \"systolic_pressure\")]\n\n# A tibble: 1,400 × 2\n   patient_id systolic_pressure\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 ID_0001                  117\n 2 ID_0002                  115\n 3 ID_0003                  120\n 4 ID_0004                  129\n 5 ID_0005                  114\n 6 ID_0006                  124\n 7 ID_0007                  133\n 8 ID_0008                  120\n 9 ID_0009                  124\n10 ID_0010                  127\n# ℹ 1,390 more rows\n\n\n\n\n\ninfections[[\"patient_id\", \"systolic_pressure\"]]\n\n     patient_id  systolic_pressure\n0       ID_0001              117.0\n1       ID_0002              115.0\n2       ID_0003              120.0\n3       ID_0004              129.0\n4       ID_0005              114.0\n...         ...                ...\n1395    ID_1396              118.0\n1396    ID_1397              117.0\n1397    ID_1398              137.0\n1398    ID_1399              135.0\n1399    ID_1400              129.0\n\n[1400 rows x 2 columns]\n\n\n\n\n\nCombine this with selecting only a subset of rows, let’s say the first three rows.\n\nRPython\n\n\n\ninfections[1:3, c(\"patient_id\", \"systolic_pressure\")]\n\n# A tibble: 3 × 2\n  patient_id systolic_pressure\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 ID_0001                  117\n2 ID_0002                  115\n3 ID_0003                  120\n\n\n\n\nWe need to be aware of the zero-based indexing, also noting that the value after the : is not included:\n\ninfections[[\"patient_id\", \"systolic_pressure\"]].iloc[0:3]\n\n  patient_id  systolic_pressure\n0    ID_0001              117.0\n1    ID_0002              115.0\n2    ID_0003              120.0",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da2-simple-plots",
    "href": "materials/trainers-live-demo.html#da2-simple-plots",
    "title": "15  Live demo exercises",
    "section": "15.7 DA2: simple plots",
    "text": "15.7 DA2: simple plots\nLet’s start with a simple scatterplot, where we plot body_temperature on the x-axis and crp_level on the y-axis.\n\nRPython\n\n\n\nggplot(infections, aes(x = body_temperature, y = crp_level)) +\n  geom_point()\n\nWarning: Removed 215 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(infections, aes(x = \"body_temperature\", y = \"crp_level\")) +\n  geom_point())\n  \np.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can explore this a bit further. For example, we can colour the points based on the hospital variable, to see if there are any patterns across the different hospitals:\n\nRPython\n\n\n\nggplot(infections, aes(x = body_temperature, y = crp_level,\n                       colour = hospital)) +\n  geom_point()\n\nWarning: Removed 215 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(infections, aes(x = \"body_temperature\", y = \"crp_level\",\n                            colour = \"hospital\")) +\n  geom_point())\n  \np.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da2-facetting-data",
    "href": "materials/trainers-live-demo.html#da2-facetting-data",
    "title": "15  Live demo exercises",
    "section": "15.8 DA2: facetting data",
    "text": "15.8 DA2: facetting data\nWhen we plotted body_temperature and crp_level against each other and coloured the data based on hospital, we ended up with a rather unclear plot. This is probably because there are no clear differences between the hospitals. However, we can separate these data a bit more clearly by using facets.\n\nRPython\n\n\n\nggplot(infections, aes(x = body_temperature, y = crp_level,\n                       colour = hospital)) +\n  geom_point() +\n  facet_wrap(facets = vars(hospital))\n\nWarning: Removed 215 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n  ggplot(infections, aes(x = body_temperature, y = crp_level,\n                         colour = hospital)) +\n    geom_point() +\n    facet_grid(rows = vars(icu_admission), cols = vars(hospital))\n\nWarning: Removed 215 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\np = (ggplot(infections, aes(x = \"body_temperature\", y = \"crp_level\",\n                            colour = \"hospital\")) +\n  geom_point() +\n  facet_wrap(\"~ hospital\"))\n  \np.show()",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da3-selecting-columns",
    "href": "materials/trainers-live-demo.html#da3-selecting-columns",
    "title": "15  Live demo exercises",
    "section": "15.9 DA3: selecting columns",
    "text": "15.9 DA3: selecting columns\nWe’re practising two things:\n\nselecting columns\ncreating columns (and highlight the use by plotting)\n\nLet’s see which columns we have:\n\nRPython\n\n\n\ncolnames(infections)\n\n [1] \"patient_id\"         \"hospital\"           \"quarter\"           \n [4] \"infection_type\"     \"vaccination_status\" \"age_group\"         \n [7] \"icu_admission\"      \"symptoms_count\"     \"systolic_pressure\" \n[10] \"body_temperature\"   \"crp_level\"         \n\n\n\n\n\n\n\n\nLet’s say we only wanted to select some of these.\n\nRPython\n\n\n\nselect(infections, patient_id, body_temperature)\n\n# A tibble: 1,400 × 2\n   patient_id body_temperature\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 ID_0001                37.8\n 2 ID_0002                39.1\n 3 ID_0003                38.5\n 4 ID_0004                39.4\n 5 ID_0005                36.9\n 6 ID_0006                36.8\n 7 ID_0007                39.4\n 8 ID_0008                39.3\n 9 ID_0009                39.6\n10 ID_0010                39.1\n# ℹ 1,390 more rows\n\n\n\n\n\n\n\n\nOr select by data type:\n\nRPython\n\n\n\nselect(infections, where(is.numeric))\n\n# A tibble: 1,400 × 4\n   symptoms_count systolic_pressure body_temperature crp_level\n            &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1              1               117             37.8     12.0 \n 2              6               115             39.1      8.11\n 3              3               120             38.5      5.24\n 4              7               129             39.4     41.7 \n 5              7               114             36.9     10.5 \n 6              5               124             36.8      6.57\n 7             10               133             39.4     53.1 \n 8             12               120             39.3     NA   \n 9             13               124             39.6     50.3 \n10              7               127             39.1     13.0 \n# ℹ 1,390 more rows\n\n\n\n\n\n\n\n\nOr based on a certain phrase within the column heading:\n\nRPython\n\n\n\nselect(infections, contains(\"_id\"))\n\n# A tibble: 1,400 × 1\n   patient_id\n   &lt;chr&gt;     \n 1 ID_0001   \n 2 ID_0002   \n 3 ID_0003   \n 4 ID_0004   \n 5 ID_0005   \n 6 ID_0006   \n 7 ID_0007   \n 8 ID_0008   \n 9 ID_0009   \n10 ID_0010   \n# ℹ 1,390 more rows",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da3-creating-columns",
    "href": "materials/trainers-live-demo.html#da3-creating-columns",
    "title": "15  Live demo exercises",
    "section": "15.10 DA3: creating columns",
    "text": "15.10 DA3: creating columns\nThere aren’t too many variables in our data set that we can transform. However, the CRP levels are a good example. The CRP (C-reactive protein) is a continuous biomarker, often used clinically to indicate inflammation or infection severity. It can be skewed at times, so scaling it could be useful if you’re making comparisons across groups.\n\n\n\n\n\n\nNoteScaling vs log-transforming\n\n\n\nThis falls well outside the scope of this course, but it’s possible that the question comes up. We could also log-transform our data if there is an issue with skewing etc. Very simply & briefly put:\n\nuse log() when the distribution shape is the issue (e.g. skew, tails)\nuse scale() when the unit or range is the issue (often when comparing across different variables with different units)\nusing both can also be helpful, where you log-transform first, then scale the data to use in modelling.\n\n\n\nWe can scale the data where, for each value, we subtract the mean and divide by the standard deviation.\n\nRPython\n\n\nIn R, we can use the scale() function to do this.\n\nmutate(infections, crp_scaled = scale(crp_level))\n\n# A tibble: 1,400 × 12\n   patient_id hospital   quarter infection_type vaccination_status age_group\n   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;    \n 1 ID_0001    hospital_3 Q2      none           &lt;NA&gt;               65+      \n 2 ID_0002    hospital_3 Q2      viral          &lt;NA&gt;               18 - 64  \n 3 ID_0003    hospital_2 Q2      none           unknown            65+      \n 4 ID_0004    hospital_2 Q3      fungal         unvaccinated       &lt; 18     \n 5 ID_0005    hospital_3 Q2      fungal         vaccinated         65+      \n 6 ID_0006    hospital_5 Q3      none           vaccinated         65+      \n 7 ID_0007    hospital_4 Q1      fungal         unvaccinated       18 - 64  \n 8 ID_0008    hospital_1 Q1      &lt;NA&gt;           unvaccinated       18 - 64  \n 9 ID_0009    hospital_2 Q1      viral          &lt;NA&gt;               65+      \n10 ID_0010    hospital_3 Q3      none           unvaccinated       &lt;NA&gt;     \n# ℹ 1,390 more rows\n# ℹ 6 more variables: icu_admission &lt;lgl&gt;, symptoms_count &lt;dbl&gt;,\n#   systolic_pressure &lt;dbl&gt;, body_temperature &lt;dbl&gt;, crp_level &lt;dbl&gt;,\n#   crp_scaled &lt;dbl[,1]&gt;\n\n\nLet’s store this in a temporary object, so we can visualise it.\n\nexample &lt;- mutate(infections, crp_scaled = scale(crp_level))\n\nWe can then plot it, adding a reference line at y = 0.\n\nggplot(example, aes(x = quarter, y = crp_scaled)) +\n  geom_jitter(width = 0.1) +\n  geom_hline(yintercept = 0, colour = \"blue\", linewidth = 1)\n\nWarning: Removed 156 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da3-creating-columns-conditional",
    "href": "materials/trainers-live-demo.html#da3-creating-columns-conditional",
    "title": "15  Live demo exercises",
    "section": "15.11 DA3: creating columns (conditional)",
    "text": "15.11 DA3: creating columns (conditional)\nSometimes we need to create new columns based on certain conditions. Let’s say we wanted to flag if patients not vaccinated and have been admitted to ICU we consider them as high risk.\nWe want to encode this in a column risk_factor, where we label them as \"high\". This would then allow us to add other values, such as \"low\" or \"medium\" at a later time.\n\nRPython\n\n\n\nmutate(infections,\n       risk_factor = ifelse(vaccination_status == \"unvaccinated\" & icu_admission == TRUE, \"high\", NA))\n\n# A tibble: 1,400 × 12\n   patient_id hospital   quarter infection_type vaccination_status age_group\n   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;    \n 1 ID_0001    hospital_3 Q2      none           &lt;NA&gt;               65+      \n 2 ID_0002    hospital_3 Q2      viral          &lt;NA&gt;               18 - 64  \n 3 ID_0003    hospital_2 Q2      none           unknown            65+      \n 4 ID_0004    hospital_2 Q3      fungal         unvaccinated       &lt; 18     \n 5 ID_0005    hospital_3 Q2      fungal         vaccinated         65+      \n 6 ID_0006    hospital_5 Q3      none           vaccinated         65+      \n 7 ID_0007    hospital_4 Q1      fungal         unvaccinated       18 - 64  \n 8 ID_0008    hospital_1 Q1      &lt;NA&gt;           unvaccinated       18 - 64  \n 9 ID_0009    hospital_2 Q1      viral          &lt;NA&gt;               65+      \n10 ID_0010    hospital_3 Q3      none           unvaccinated       &lt;NA&gt;     \n# ℹ 1,390 more rows\n# ℹ 6 more variables: icu_admission &lt;lgl&gt;, symptoms_count &lt;dbl&gt;,\n#   systolic_pressure &lt;dbl&gt;, body_temperature &lt;dbl&gt;, crp_level &lt;dbl&gt;,\n#   risk_factor &lt;chr&gt;",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da4-reshaping-data",
    "href": "materials/trainers-live-demo.html#da4-reshaping-data",
    "title": "15  Live demo exercises",
    "section": "15.12 DA4: reshaping data",
    "text": "15.12 DA4: reshaping data\nTo illustrate reshaping data, we’re going to simplify our data set a little bit. First, we’ll calculate the average CRP levels for each hospital, quarter, infection type, vaccination status and age group.\nWe’ll use that summary table to highlight some use cases for reshaping data from a long to a wide format - and back.\n\n15.12.1 Creating a summary table for CRP\n\nRPython\n\n\n\ninfections_summary &lt;- infections |&gt; \n  drop_na() |&gt; \n  group_by(hospital, quarter, infection_type, vaccination_status, age_group) |&gt; \n  summarise(mean_crp = round(mean(crp_level, na.rm = TRUE), 2)) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'hospital', 'quarter', 'infection_type',\n'vaccination_status'. You can override using the `.groups` argument.\n\n\nLet’s look at the output. We now have 6 columns, one for each of our grouping variables and at the end the mean_crp column containing the average CRP levels (rounded to two digits).\n\nhead(infections_summary)\n\n# A tibble: 6 × 6\n  hospital   quarter infection_type vaccination_status age_group mean_crp\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;        &lt;dbl&gt;\n1 hospital_1 Q1      bacterial      unknown            65+          23.7 \n2 hospital_1 Q1      bacterial      unknown            &lt; 18         25.8 \n3 hospital_1 Q1      bacterial      unvaccinated       18 - 64       8.35\n4 hospital_1 Q1      bacterial      unvaccinated       65+          23.2 \n5 hospital_1 Q1      bacterial      unvaccinated       &lt; 18         26.8 \n6 hospital_1 Q1      bacterial      vaccinated         18 - 64      22.4 \n\n\n\n\n\n\n\n\n\n\n15.12.2 Wide table by quarter\nLet’s say we’d be interested how the (average) CRP levels change over time across the different groupings. We’d be interested in having the mean_crp values for each quarter next to each other. That way we’d be able to plot quarterly average CRP levels against each other.\nWe can do this as follows:\n\nRPython\n\n\n\ninfections_wide &lt;- infections_summary |&gt; \n  pivot_wider(names_from = \"quarter\",\n              values_from = \"mean_crp\")\n\n\nhead(infections_wide)\n\n# A tibble: 6 × 7\n  hospital   infection_type vaccination_status age_group    Q1    Q2    Q3\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 hospital_1 bacterial      unknown            65+       23.7   NA    26.8\n2 hospital_1 bacterial      unknown            &lt; 18      25.8   54.5  NA  \n3 hospital_1 bacterial      unvaccinated       18 - 64    8.35  NA    NA  \n4 hospital_1 bacterial      unvaccinated       65+       23.2   28.6  NA  \n5 hospital_1 bacterial      unvaccinated       &lt; 18      26.8   25.0  17.1\n6 hospital_1 bacterial      vaccinated         18 - 64   22.4   12.7  NA  \n\n\n\n\n\n\n\n\nThis then allows us to plot the data, for example:\n\nRPython\n\n\n\nggplot(infections_wide, aes(x = Q1, y = Q2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.12.3 Back to long format\nWe can revert back to a long format with the following:\n\nRPython\n\n\n\npivot_longer(infections_wide,\n             cols = Q1:Q3,           # columns to pivot\n             names_to = \"quarter\",   # column for headings\n             values_to = \"mean_crp\") # column for values\n\n# A tibble: 534 × 6\n   hospital   infection_type vaccination_status age_group quarter mean_crp\n   &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n 1 hospital_1 bacterial      unknown            65+       Q1         23.7 \n 2 hospital_1 bacterial      unknown            65+       Q2         NA   \n 3 hospital_1 bacterial      unknown            65+       Q3         26.8 \n 4 hospital_1 bacterial      unknown            &lt; 18      Q1         25.8 \n 5 hospital_1 bacterial      unknown            &lt; 18      Q2         54.5 \n 6 hospital_1 bacterial      unknown            &lt; 18      Q3         NA   \n 7 hospital_1 bacterial      unvaccinated       18 - 64   Q1          8.35\n 8 hospital_1 bacterial      unvaccinated       18 - 64   Q2         NA   \n 9 hospital_1 bacterial      unvaccinated       18 - 64   Q3         NA   \n10 hospital_1 bacterial      unvaccinated       65+       Q1         23.2 \n# ℹ 524 more rows\n\n\n\n\n\n\n\n\nAnd all is well with the world again.",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  },
  {
    "objectID": "materials/trainers-live-demo.html#da4-joining-data",
    "href": "materials/trainers-live-demo.html#da4-joining-data",
    "title": "15  Live demo exercises",
    "section": "15.13 DA4: joining data",
    "text": "15.13 DA4: joining data\nIn the infections data set we have a variable called hospital. This contains the following unique entries:\n\nRPython\n\n\n\ninfections |&gt; count(hospital)\n\n# A tibble: 6 × 2\n  hospital       n\n  &lt;chr&gt;      &lt;int&gt;\n1 hospital_1   277\n2 hospital_2   259\n3 hospital_3   280\n4 hospital_4   245\n5 hospital_5   271\n6 &lt;NA&gt;          68\n\n\n\n\n\n\n\n\nNote that for some of the hospital entries there are missing data. This is relevant later on, when we’re joining.\nWe’re now going to add information on the hospitals, which are stored in hospital_info.csv. Let’s read in the data:\n\nRPython\n\n\n\nhospital_info &lt;- read_csv(\"data/hospital_info.csv\")\n\n\n\n\n\n\n\nLook at the data. There are 6 distinct hospital entries, with additional information for each hospital. We have the following variables:\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\nhospital\ncharacter (id)\nUnique hospital identifier (hospital_1 … hospital_6).\n\n\nhospital_name\ncharacter\nOfficial hospital name (e.g., Royal London Hospital).\n\n\nlocation\ncharacter\nCity where the hospital is located (e.g., London, Manchester).\n\n\nbed_capacity\ninteger\nApproximate number of inpatient beds available at the hospital.\n\n\nteaching_hospital\nlogical\nIndicates if the hospital is a teaching hospital (TRUE / FALSE).\n\n\n\n\n15.13.1 Left join\nFirst, we’ll add the hospital_info data to the infections data set. We do this with a left-join. We expect hospital info data to be added to our main infections table, if the hospital value in infections matches with the one in hospital_info.\n\nRPython\n\n\n\ninfections_left &lt;- left_join(infections, hospital_info, by = \"hospital\")\n\nLet’s just select the ID column from infections, together with all the columns that have been added.\n\ninfections_left |&gt; \n  select(patient_id, hospital, hospital_name, location, bed_capacity, teaching_hospital) |&gt; \n  head()\n\n# A tibble: 6 × 6\n  patient_id hospital   hospital_name    location bed_capacity teaching_hospital\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt; &lt;lgl&gt;            \n1 ID_0001    hospital_3 Bristol Royal I… Bristol           768 TRUE             \n2 ID_0002    hospital_3 Bristol Royal I… Bristol           768 TRUE             \n3 ID_0003    hospital_2 Manchester Gene… Manches…          849 FALSE            \n4 ID_0004    hospital_2 Manchester Gene… Manches…          849 FALSE            \n5 ID_0005    hospital_3 Bristol Royal I… Bristol           768 TRUE             \n6 ID_0006    hospital_5 Cardiff Univers… Cardiff           582 TRUE             \n\n\nWe can see that the hospital information is now added to the data. We don’t expect any data to have dropped, so the number of observations/rows in infections_left should match the original 1400 from infections.\n\nnrow(infections_left)\n\n[1] 1400\n\n\n\n\n\n\n\n\n\n\n15.13.2 Right join\nSo, how does a right join then differ from a left join? Well, here we’d be adding the infections data to the hospital_info data (so, in the other direction). That means that for each hospital value that exists in hospital_info it will try and find the values that match the hospital column in infections.\n\nRPython\n\n\n\ninfections_right &lt;- right_join(infections, hospital_info, by = \"hospital\")\n\nAgain, the resulting table contains the data from both infections and hospital_info:\n\nhead(infections_right)\n\n# A tibble: 6 × 15\n  patient_id hospital   quarter infection_type vaccination_status age_group\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;chr&gt;    \n1 ID_0001    hospital_3 Q2      none           &lt;NA&gt;               65+      \n2 ID_0002    hospital_3 Q2      viral          &lt;NA&gt;               18 - 64  \n3 ID_0003    hospital_2 Q2      none           unknown            65+      \n4 ID_0004    hospital_2 Q3      fungal         unvaccinated       &lt; 18     \n5 ID_0005    hospital_3 Q2      fungal         vaccinated         65+      \n6 ID_0006    hospital_5 Q3      none           vaccinated         65+      \n# ℹ 9 more variables: icu_admission &lt;lgl&gt;, symptoms_count &lt;dbl&gt;,\n#   systolic_pressure &lt;dbl&gt;, body_temperature &lt;dbl&gt;, crp_level &lt;dbl&gt;,\n#   hospital_name &lt;chr&gt;, location &lt;chr&gt;, bed_capacity &lt;dbl&gt;,\n#   teaching_hospital &lt;lgl&gt;\n\n\nHow many observations do we expect? Let’s have a look at how many we’ve got.\n\nnrow(infections_right)\n\n[1] 1333\n\n\n\n\n\n\n\n\nHere we see that we have fewer than the original 1400 observations, because we are joining in the other direction. Remember, there are missing values in our infections data set (68 in total). When we’re joining infections to hospital_info, these entries get dropped, because there is no missing value entry in the hospital column of hospital_info.\nFor the eagle-eyed amongst you: we have 1333 observations/rows in infections_right, whereas there are 68 missing values. We might have expected there to be 1332 rows (the difference between the number of rows in infections and the number of missing values), but there are 1333.\nWhy? Well, there is one entry in hospital_info that does not appear in infections: hospital_6. When we right join, this value gets retained, so this adds one additional row to the final output.\n\nRPython\n\n\n\ninfections_right |&gt; count(hospital)\n\n# A tibble: 6 × 2\n  hospital       n\n  &lt;chr&gt;      &lt;int&gt;\n1 hospital_1   277\n2 hospital_2   259\n3 hospital_3   280\n4 hospital_4   245\n5 hospital_5   271\n6 hospital_6     1\n\n\n\n\n\n\n\n\nIf we wouldn’t want that, we’d use inner join.\n\n\n15.13.3 Inner join\nInner join will join two tables and only retain values (based on the joining ID) that exist in both tables.\n\nRPython\n\n\n\ninfections_inner &lt;- inner_join(infections, hospital_info, by = \"hospital\")\n\n\nnrow(infections_inner)\n\n[1] 1332\n\n\nOnly the 5 hospitals that appear in the infections data set are retained:\n\ninfections_inner |&gt; count(hospital)\n\n# A tibble: 5 × 2\n  hospital       n\n  &lt;chr&gt;      &lt;int&gt;\n1 hospital_1   277\n2 hospital_2   259\n3 hospital_3   280\n4 hospital_4   245\n5 hospital_5   271\n\n\n\n\n\n\n\n\nThis is entirely expected, since we don’t have missing data in hospital_info (so all the missing data from infections are dropped), nor do we have a hospital_6 entry in infections (so that is dropped too).\n\n\n15.13.4 Full join\nIf we wanted to retain all observations, regardless of which table they’re from, we’d use a full join.\n\nRPython\n\n\n\ninfections_full &lt;- full_join(infections, hospital_info, by = \"hospital\")\n\nHow many rows do we expect? Well, the 1400 from infections plus the one extra (hospital_6 entry) from hospital_info:\n\nnrow(infections_full)\n\n[1] 1401\n\n\nWe can check that we’ve retained all values, by counting the number of observations for each hospital value:\n\ninfections_full |&gt; count(hospital)\n\n# A tibble: 7 × 2\n  hospital       n\n  &lt;chr&gt;      &lt;int&gt;\n1 hospital_1   277\n2 hospital_2   259\n3 hospital_3   280\n4 hospital_4   245\n5 hospital_5   271\n6 hospital_6     1\n7 &lt;NA&gt;          68\n\n\n\n\n\n\n\n\nWe see that all hospital values are retained, including the missing values and the one hospital_6 value coming from the hospital_info data set. Success!",
    "crumbs": [
      "{{< iconify fa6-brands github size=lg title='CRIT GitHub' >}}",
      "Additional resources",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Live demo exercises</span>"
    ]
  }
]