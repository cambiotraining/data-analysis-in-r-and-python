---
title: Manipulating rows
---

::: {.callout-tip}
#### Learning objectives

- Learn to order and arrange rows.
- Be able to find and retain unique rows.
- Understand how logical operators are used.
- Implement conditional statements to filter specific data.
- Be able to identify and decide how to deal with missing data.

:::


## Context

Data sets can contain large quantities of observations. Often we are only interested in part of the data at a given time. We can deal with this by manipulating rows.

## Section setup {#setup_manipulating_rows}

::: {.panel-tabset group="language"}
## R

We'll continue this section with the script named `03_session`. If needed, add the following code to the top of your script and run it.

In this section we will use a new package, called `naniar`, to visualise where we have missing data. Install it as follows:

```{r}
#| eval: false
# install if needed
install.packages("naniar")
```

```{r}
#| message: false
# A collection of R packages designed for data science
library(tidyverse)

# Package to visualise missing data
library(naniar)

surveys <- read_csv("data/surveys.csv")
```

## Python

We'll continue this section with the Notebook named `03_session`. Add the following code to the first cell and run it.

We'll use a new package, so install if needed from the terminal:

```{bash}
#| eval: false
mamba install conda-forge::missingno
```

Then, if needed, add the following code to the top of your script and run it.

```{python}
# A Python data analysis and manipulation tool
import pandas as pd
import numpy as np

# Python equivalent of `ggplot2`
from plotnine import *

# If using seaborn for plotting
import seaborn as sns
import matplotlib.pyplot as plt

# Load missingno
import missingno as msno

surveys = pd.read_csv("data/surveys.csv")
```

:::


## Manipulation of observations

### Ordering rows
We often want to order data in a certain way, for example ordering by date or in alphabetically. The example below illustrates how we would order data based on `weight`:

::: {.carousel data-caption="Ordering by weight in ascending order (click to toggle)."}

![](images/manipulation-order_main.png)
![](images/manipulation-order_weight.png)
:::

Let's illustrate this with the `surveys` data set, arranging the data based on `year`.

::: {.panel-tabset group="language"}
## R

```{r}
surveys |> 
  arrange(year)
```

If we'd want to arrange the data in *descending* order (most recent to oldest), we would employ the `desc()` helper function:

```{r}
surveys |> 
  arrange(desc(year))
```

We can read that bit of code as "take the `surveys` data set, send it to the `arrange()` function and ask it to arrange the data in descending order (using `desc()`) based on the `year` column".

We can also combine this approach with multiple variables, for example arranging data based on descending year *and* (ascending) hindfoot length:

```{r}
surveys |> 
  arrange(desc(year), hindfoot_length)
```

## Python

```{python}
surveys.sort_values(by = "year")
```

If we'd want to arrange the data in *descending* order (most recent to oldest), we would specify this with the `ascending = False` argument:

```{python}
surveys.sort_values(by = "year", ascending = False)
```

We can also combine this approach with multiple variables, for example arranging data based on descending year *and* (ascending) hindfoot length:

```{python}
surveys.sort_values(
    by = ["year", "hindfoot_length"],
    ascending = [False, True]
)
```

:::

### Finding unique values

Sometimes it is useful to retain rows with unique combinations of some of our variables (i.e. remove any duplicated rows). 

::: {.panel-tabset group="language"}
## R

This can be done with the `distinct()` function.

```{r}
surveys |> 
  distinct(species_id, year)
```


## Python

We can do this by specifying which column we'd like to get the unique values from (here, we're using `species_id` and `year` as an example). We then use `.drop_duplicates()` to remove all the duplicate values:

```{python}
surveys[["species_id", "year"]].drop_duplicates()
```

:::

### Filtering by condition
Often we want to filter our data based on specific conditions / properties in the data. For example, in our data set you might want to filter for certain years, a specific weight range or only get all the observations for the first 100 record IDs.

Before we delve into this, it is important to understand that when we set a condition like above, the output 
is a _logical vector_. Let's see an example using a small vector.

::: {.panel-tabset group="language"}
## R

```{r}
some_years <- c(1985, 1990, 1999, 1995, 2010, 2000)
some_years < 2000
```

## Python

For this example we'll keep using `pandas`, for consistency. Since we're only dealing with a simple, one-dimensional bunch of data, we create a `Series`:

```{python}
some_years = pd.Series([1985, 1990, 1999, 1995, 2010, 2000])
result = some_years < 2000

result
```

:::

It is possible to combine several conditions by using the _logical operators_ 
`&` (AND) and `|` (OR). For example, if we wanted the years between 1990 and 2000:

::: {.panel-tabset group="language"}
## R

```{r}
# both conditions have to be true
some_years > 1990 & some_years < 2000
```

And if we wanted the years below 1990 or above 2000, then:

```{r}
# only one or the other of the conditions has to be true
some_years < 1990 | some_years > 2000
```

## Python

```{python}
result = (some_years > 1990) & (some_years < 2000)

result
```

And if we wanted the years below 1990 or above 2000, then:

```{python}
result = (some_years < 1990) | (some_years > 2000)

result
```

:::

This concept is also applied to tables. We could filter across all rows in the `surveys` data set, for a `hindfoot_length` of larger than 31 mm:

![The logic behind filtering: for each row the condition is checked (here: `hindfoot_length > 31`). If the outcome is `TRUE` then the row is returned.](images/manipulation-filter_rows.png){#fig-manipulation_filter_rows}

We could then do this as follows:

::: {.panel-tabset group="language"}
## R

```{r}
surveys |> 
  filter(hindfoot_length > 31)
```

## Python

```{python}
surveys[surveys["hindfoot_length"] > 31]
```

Another option would be to use the `.query()` function. This is functionally equivalent, but a bit easier to read.

```{python}
surveys.query("hindfoot_length > 31")
```

:::

This then only keeps the observations where `hindfoot_length > 31`, in this case 
`r surveys |> filter(hindfoot_length > 31) |> nrow()` observations.

::: {.callout-important}
## Conditional  operators

::: {.panel-tabset group="language"}
## R

To set filtering conditions, use the following _relational operators_:
 
- `>` is greater than
- `>=` is greater than or equal to
- `<` is less than
- `<=` is less than or equal to
- `==` is equal to
- `!=` is different from
- `%in%` is contained in

To combine conditions, use the following _logical operators_:

- `&` AND
- `|` OR
 
Some functions return logical results and can be used in filtering operations:
 
- `is.na(x)` returns _TRUE_ if a value in _x_ is missing

The `!` can be used to negate a logical condition:

- `!is.na(x)` returns _TRUE_ if a value in _x_ is NOT missing
- `!(x %in% y)` returns _TRUE_ if a value in _x_ is NOT present in _y_

## Python

To set filtering conditions, use the following _relational operators_:
 
- `>` is greater than
- `>=` is greater than or equal to
- `<` is less than
- `<=` is less than or equal to
- `==` is equal to
- `!=` is different from
- `.isin([...])` is contained in

To combine conditions, use the following _logical operators_:

- `&` AND
- `|` OR
 
Some functions return logical results and can be used in filtering operations:
 
- `df["x"].isna()` returns _True_ if a value in _x_ is missing

The `~` (bitwise NOT) can be used to negate a logical condition:

- `~df["x"].isna()` returns _True_ if a value in _x_ is NOT missing
- `~df["x"].isin(["y"])` returns _True_ if a value in _x_ is NOT present in `"y"`
:::

:::

### Missing data revisited {#missingdata-revisited}

It's important to carefully consider how to deal with missing data, as we
[have previously seen](#missing-data). It's easy enough to filter out all rows that contain missing data, however this is rarely the best course of action, because you might accidentally throw out useful data in columns that you'll need later.

Furthermore, it's often a good idea to see if there is any **structure** in your missing data. Maybe certain variables are consistently absent, which could tell you something about your data.

::: {.panel-tabset group="language"}

## R

We could filter out all the missing `weight` values as follows:

```{r}
surveys |> 
  filter(!is.na(weight))
```

## Python

```{python}
surveys.dropna(subset=["weight"])
```

:::

::: {.panel-tabset group="language"}

## R

We can combine this for multiple columns:

```{r}
surveys |> 
  filter(!is.na(weight) & !is.na(hindfoot_length))
```

## Python

```{python}
surveys.dropna(subset = ["weight", "hindfoot_length"])
```

:::

We can also combine that with other filters.

::: {.panel-tabset group="language"}

## R

```{r}
surveys |> 
  filter(!is.na(weight) & !is.na(hindfoot_length)) |> 
  filter(hindfoot_length > 40)
```

## Python

To do this, we might prefer to use a slightly syntax: the `.notna()`. This allows us to chain operations a bit cleaner, making our code easier to read:

```{python}
surveys[
    (surveys["weight"].notna()) &
    (surveys["hindfoot_length"].notna()) &
    (surveys["hindfoot_length"] > 40)
]

```
:::

Often it's not that easy to get a sense of how missing data are distributed in the data set. We can use summary statistics and visualisations to get a better sense.

::: {.panel-tabset group="language"}

## R

The easiest way of getting some numbers on the missing data is by using the `summary()` function, which will report the number of `NA`'s for each column (for example: see the `hindfoot_length` column):

```{r}
summary(surveys)
```

Often it's nice to visualise where your missing values are, to see if there are any patterns that are obvious. There are several packages in R that can do this, of which `naniar` is one.

```{r}
#| eval: false
# install if needed
install.packages("naniar")

# load the library
library(naniar)
```

```{r}
# visualise missing data
vis_miss(surveys)
```

## Python

The easiest way of counting the number of missing values in Python is by combining `.isna()` and `.sum()`:

```{python}
# count missing values
surveys.isna().sum()
```

Often it's nice to visualise where your missing values are, to see if there are any patterns that are obvious. There are several packages in Python that can do this, of which `missingno` is one.

```{python}
#| eval: false
import missingno as msno
```

```{python}
# visual matrix
msno.matrix(surveys)
```

:::

## Exercises

### Filtering data: `infections` {#sec-exr_filter}

::::: {#ex-title .callout-exercise}
#### Filtering data

{{< level 2 >}}

For this exercise we'll be using the data from `data/infections.csv`.

Using these data, please answer the following questions:

1. How many patients had a bacterial infection, but were not admitted to the ICU (Intensive Care Unit)?
2. In patients older than 65, how many have CRP levels between 10-15 mg/L or 50-60 mg/L?

:::: {.callout-answer collapse="true"}

First we load the data.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
infections <- read_csv("data/infections.csv")
```


## Python

```{python}
infections = pd.read_csv("data/infections.csv")
```

:::

#### 1. Bacterial and non-ICU

To get the data we want, we need to filter for rows where:

1. `infection_type` contains the text `"bacterial"`
2. `icu_admission` is equal to `TRUE` (R) / `True` (Python)

::: {.panel-tabset group="language"}
## R

```{r}
infections |> 
  filter(infection_type == "bacterial" & icu_admission == FALSE) |> 
  nrow()
```

## Python

```{python}
(
  infections
  .query("infection_type == 'bacterial' and icu_admission == False")
  .shape[0]
)
```

:::

#### 2. Patients in 65+ group with certain CRP levels

Here we need to filter for three things:

1. `age_group` values of `"65+"` (string/text)
2. `crp_level > 10` and `crp_level < 15`
3. `crp_level > 50` and `crp_level < 60`

::: {.panel-tabset group="language"}
## R

```{r}
infections |> 
  filter(age_group == "65+") |> 
  filter(crp_level > 10 & crp_level < 15 |
           crp_level > 50 & crp_level < 60) |> 
  nrow()
```


## Python

Using `.query()`:

```{python}
(
  infections
  .query(
    "age_group == '65+' and ((10 < crp_level < 15) or (50 < crp_level < 60))")
    .shape[0]
)
```

We can also use a more explicit version using normal subsetting (which becomes quite convoluted, quite quickly!):

```{python}
#| echo: false
(
  infections
  .loc[
    (infections["age_group"] == "65+") &
    (
      ((infections["crp_level"] > 10) & (infections["crp_level"] < 15)) |
      ((infections["crp_level"] > 50) & (infections["crp_level"] < 60))
      )
    ]
    .shape[0]
)

```

:::

::::
:::::

### Creating categories: `risk_group` {#sec-creating_categories}

::::: {.callout-exercise #arranging_data}
#### Creating categories

{{< level 3 >}}

For this exercise we'll again be using the data from `data/infections.csv`.

We are interested in creating a new category, based on the original data. Patients that are over 65 years of age and have a relatively high systolic blood pressure (over 140 mmHg) are considered "high" risk.

Create a new column that contains the risk group (high, low) and count how many patients fall within each category.

We've not yet covered counting observations, so you'll have to do a quick search!

:::{.callout-hint}
Make sure you consider how to deal with missing data!
:::

:::: {.callout-answer collapse="true"}

We can do this in two steps:

1. Create a new column that contains the risk group
2. Count the number of rows within each group

::: {.panel-tabset group="language"}
## R

```{r}
infections |>
  mutate(risk_group = case_when(
    # if age_group AND systolic_pressure are not missing
    !is.na(age_group) & !is.na(systolic_pressure) &
      # AND age_group is 65+ AND systolic_pressure > 140
      # then assign a risk group "high"
      age_group == "65+" & systolic_pressure > 140 ~ "high",
    # if age_group AND systolic_pressure are not missing
    # then assign a risk group "low"
    !is.na(age_group) & !is.na(systolic_pressure) ~ "low",
    # otherwise, assign the missing value character
    TRUE ~ NA_character_)) |>
  count(risk_group)
```

A more intuitive way would have been to do something like this:

```{r}
infections |> 
  mutate(risk_group = ifelse(age_group == "65+" & systolic_pressure > 140, "high", "low")) |> 
  count(risk_group)
```

Which would read something like "if age group is 65+ and systolic pressure is more than 140, assign "high", otherwise assign "low". Which, at first glance, should really result in the same assignments.

However, the results here are different. Why? This is because of the way that missing values are being handled.

In the latter, simpler, syntax the condition where `NA` is returned is different. Let's look a bit closer into how R handles these conditions. Consider the following:

```{r}
#| eval: false
TRUE  & TRUE    # TRUE
TRUE  & FALSE   # FALSE
FALSE & FALSE   # FALSE
NA    & TRUE    # NA
NA    & FALSE   # FALSE   <-- important
NA    & NA      # NA
```

Whenever the `ifelse()` statement results in `TRUE`, it returns `"high"`. When it results in a `FALSE`, it returns `"low"`. If it encounters `NA`, it returns `NA`.

This means that if either of the value of `age_group` or `systolic_pressure` is missing (`NA`) AND the other condition is `FALSE` (i.e. `age_group` is *not* `"65+"` or `systolic_pressure` is *not* `> 140`) then the resulting output is `FALSE`. Then, `"low"` is assigned.

This doesn't make sense in our context. Because if either of the two values are missing we shouldn't even continue with the conditional statement, because a patient with high blood pressure - but no data on the age group - should not be classified as `"high"` in the risk group.

## Python

```{python}
# The default risk_group value is missing data
infections["risk_group"] = pd.Series(pd.NA, dtype = "object")

# Assign "high" when both conditions are satisfied
infections.loc[
    (infections["age_group"] == "65+") & (infections["systolic_pressure"] > 140),
    "risk_group"
] = "high"

# Assign "low" when both values are known AND condition is not met
# using ~(...), which negates / inverses the condition
infections.loc[
    (infections["age_group"].notna()) & 
    (infections["systolic_pressure"].notna()) & 
    ~((infections["age_group"] == "65+") & (infections["systolic_pressure"] > 140)),
    "risk_group"
] = "low"

# Count occurrences
risk_counts = infections["risk_group"].value_counts(dropna = False).reset_index()
risk_counts.columns = ["risk_group", "n"]

risk_counts
```

Note that when we define the initial `risk_group` column, we fill the column with missing values (as a default). We do this with:

`infections["risk_group"] = pd.Series(pd.NA, dtype = "object")`

We can read this as "create a column called `risk_group`, in the `infections` data set, which contains a pandas Series, filled with missing data (`pd.NA`) and the type of column is text (`dtype = "object"`). 

Following that, we then populate the column based on the conditions we check against. We have to explicitly check if we are dealing with missing data, or not.
:::
::::
:::::

## Summary

::: {.callout-tip}
#### Key points

::: {.panel-tabset group="language"}

## R

- We use the `arrange()` function to order data, and can reverse the order using `desc()`.
- Unique rows can be retained with `distinct()`.
- We use `filter()` to choose rows based on conditions.
- Conditions can be set using several operators: `>`, `>=`, `<`, `<=`, `==`, `!=`, `%in%`.
- Conditions can be combined using `&` (AND) and `|` (OR).
- The function `is.na()` can be used to identify missing values. It can be negated as `!is.na()` to find non-missing values.
- We can visualise missing data using the `vis_miss()` function from the `naniar` package.

## Python

- We can use the `.sort_values()` method to order data, specifying the `ascending =` argument as `True` or `False` to control the order.
- The `.drop_duplicates()` method allows us to retain unique rows.
- We use subsetting (e.g. `surveys[surveys["hindfoot_length"] > 31]`) together with conditions to filter data.
-- Conditions can be set using several operators: `>`, `>=`, `<`, `<=`, `==`, `!=`, `.isin([...])`.
- Conditions can be combined using `&` (AND) and `|` (OR).
- The function `.isna()` can be used to identify missing values. It can be negated using `~` to find non-missing values (e.g. `surveys[~surveys["weight"].isna()]`.
- We can visualise missing data using the `msno.matrix` function from the `missingno` package.

:::

:::
